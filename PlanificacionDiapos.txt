Este será el punteo de las diapos, vamos a considerar que las clases tienen una duración de 1 hora aproximadamente.

Clase 1: Introducción

1- Presentación, aclaraciones sobre el curso y sistemas de evaluación (20 a 30 mins)
	- Primera diapo se presenta al profesor.
	- Segunda diapo se presenta a auxiliares y ayudantes 
	- Tercera diapo se presenta el sistema de evaluaciones, ponderaciones y fechas para entregas de tareas. (En general todo el ámbito administrativo)

2 - Introducción y Orígenes del aprendizaje de máquinas(30 mins)
	- Analizar en función de lo que se considera importante mencionar.

Clase 2: Regresión Lineal

1- Regresión Lineal, Introducción (15 mins)
	- Presentación del problema, gráfica con una regresión lineal y por qué es el primer caso que estudiamos  

2 - Mínimos cuadrados, derivar el theta óptimo (25 mins)
	- Presentación del criterio, notación y cambios de variable.
	- Derivar el theta óptimo (Esto podría realizarse escribiendo sobre el beamer) 

3 - Interpretación Geométrica (20 mins) 
	- Por qué mínimos cuadrados, justificación conceptual y práctica.
	- Justificación geométrica (Presentar el diagrama)
	- Problemas del criterio de mínimos cuadrados (gráfica con/sin outliers)

Clase 3: Regularización

1 - Regularización, Introducción(20 mins)
	- Motivación para utilizar regularizadores, por qué a pesar de aumentar el valor del MSE, sigue siendo útil ()
	- Derivar el theta óptimo para p=2 (Solución cerrada)

2 - Equivalencia con un problema de optimización 
	- Comparar los regularizadores, mostrar curvas de nivel y hablar del efecto selección de variables (Gráfica con los histogramas)
	
Clase 4: Máxima Verosimilitud vs Mínimos Cuadrados

1 - Máxima Verosimilitud (60 mins)
	- Notación, función de verosimilitud, divergencia KL  
	- Divergencia KL a max verosimilitud (Ojo con MonteCarlo, esto debería ser un apéndice del apunte)
	- Estimador de max verosimilitud caso lineal es el que minimiza el error cuadrático  

Comentario: El ejemplo de la verosimilitud del caso Gaussiano se presenta en una hoja del beamer sin mucho detalle (pura matraca)

Comentario: La divergencia KL tiene una motivación de la teoría de la información que se podría mencionar explicitamente, podría mencionarse también que existen otras métricas que si son distancias entre distribuciones de probabilidad)
Revisar: http://blog.thegrandlocus.com/2019/06/focus-on-the-kullback-leibler-divergence que tiene un gran discurso al respecto.
Podría agregarse de apéndice también. 

Clase 5: Inferencia Bayesiana 

1 - Introducción (20 mins)
	- Problema de maximizar solo la verosimilitud, la necesidad de agregar conocimiento previo al problema.
	- Fórmula de Bayes, theta pasa a ser una cantidad aleatoria un poco de historia
2 - Elección de Prior: Conjugación (25 mins)
	- Def prior conjugado y ejemplo del modelo gaussiano mu y sigma conocido
	- Ejemplo para la regresión Lineal
3 - Máximo a posteriori (15 mins)
	- Definición y caso lineal equivale a utilizar regularizador p=1 y p=2

Comentario: Podría dejarse el modelo binomial como lectura propia tanto en prior conjugado como en MAP

Comentario: ¿mu y sigma^2 desconocido seguirá siendo tarea? (fue tarea el año pasado)

Comentario: La normal multivariada no se ve en muchos de los cursos de probabilidades y estadística no dim, por lo que debería ser agregado como apéndice

Clase 6: Predicciones y Regresión no Lineal 

1 - Predicciones (30 mins)
	- Definiciones, variables latentes
	- Concepto de desintegración
	- Mostrar gráficas para el caso lineal 

2 - Regresión no Lineal (30 mins)
	- Definiciones, seleccion de características y modelo lineal en los parámetros
	- Ejemplos de transformaciones y ejemplo de predicción pasajeros aerolinea

Clase 7: Clasificación (Año pasado era Clase 10)

1 - Introducción (20 mins)
	- Definiciones y clasificación lineal para 2 clases (parámetros a y b con respecto a la región de decisión)
	- Caso multiclase, one-versus-one , one-versus-rest y construcción por argmax (Tarea del año pasado)

2 - Ajuste por mínimos cuadrados (20 mins)
	- One hot encoding
	- Deducción de fórmulas, notación y problemáticas de este enfoque (En particular la presencia de outliers y que la estimación no es 'mas' o 'menos' correcta, aquí mostrar las gráficas)

3 - Discriminante Lineal de Fisher (20 mins)
	- Motivaciones, y reducción de dimensionalidad.
	- Deducción de las fórmulas

Comentario: Trabajar en la notación de esta sección pues está confusa. 

Comentario: Se puede quitar el discriminante lineal de fisher, introducirlo en otra clase y partir la clase 7 con las k-vecinos más cercanos. Esperaremos a que esté trabajado con mayor detalle. 

Comentario: Podría trabajarse en una gráfica con mayor detalle de cómo funciona el discriminante lineal de fisher. (En particular la gráfica que tiene la proyección en la recta)

Comentario: La deducción de fórmulas podría ser un problema de tarea, aunque más bien parece un problema de lineal jajaja


Clase 8: Clasificación no Lineal

1 - Clasificación no Lineal: El Perceptrón (20 mins)
	- Problemas de los modelos anteríores, introducción del perceptrón 
	- Extracción de características y función de enlace
	- Criterio del perceptrón, función objetivo y SGD

Comentario: Se debe agregar un apéndice acerca del descenso de gradiente estocástico.  

2 - Clasificación Probabilística: Modelo Generativo (30 mins)
	- Interpretación probabilística y Bayes 
	- Función y propiedades Sigmoide y Softmax 
	- Modelamieto, deducción de fórmulas para el caso gaussiano y de 2 clases. 

3 - Regresión vs Modelo Generativo (10 mins)
	- Optimizar directamente la verosimilitud, calcular el gradiente y método del descenso de gradiente.
	- Similitud y diferencia con lo realizado para el perceptrón. 

Comentario: Eventualmente esta clase 8 podría ser dividida en 2 clases si es demasiado larga

Clase 8.5: Selección de Modelos y validación cruzada

Clase 9: Support Vector Machines

1 - Motivación e introducción a las SVM (15 mins) 
	- Idea general del modelo y  definiciones
	- Problemáticas que resuelve

2 - Formulación del Problema (30 mins)
	- Deducción de fórmulas y el problema de optimización
	- Problema dual y resolución 
	- Propiedades y definiciones de ser necesarias (por ejemplo de ser definida positiva o lagrangiano, etc...)
	- Observación fundamental con respecto a cómo optimizar el modelo (Sequential minimal optimization) y observación con respecto a la reducción de operaciones cuando la dimensión M de la entrada es grande. 

3 - Soft Margin (15 mins)
	- Problemas con el modelo anterior, comparación de gráficas en presencia de outliers 
	- Utilización de variables de holgura ("Regularizador") 
	- Lagrangiano asociado e interpretación.

Clase 10: Support Vector Machines 2

1 - Recuerdo de clase anterior (15 mins)

2 - Método del Kernel (30 mins)
	- Qué sucede cuando los datos no son linealmente separables ni están cerca de serlos 
	- Ejemplo de extracción de características para "XOR"
	- Basta calcular el producto entre características de los datos, hablar sobre la maldición de la dimensionalidad. 
	- Definiciones y teorema de Mercer. Truco del Kernel

3 - Distintos tipos de Kernel (15 mins)
	- Kernel Polinomial, RBF, periódico 
	- Kernel Ridge Regression
	- Kernel SVM y conclusiones al respecto. 

Clase 11.5: Aprendizaje no Supervisado 

1 -  Definiciones 
	- Reducción de dimensionalidad
	- Maldición de la dimensionalidad
2 - Análisis de componentes principales
	- Motivación y Desarrollo PCA
	- Kernel PCA 
	- Probabilistic PCA 
	- Discriminante lineal de fisher (Agregar una introducción adhoc)

Comentario: En general pienso que esta sección no está desarrollada completamente y no deberíamos más que mencionarla, lo agregaré como clase 11.5 

Comentario: Esperaré a que esté más trabajado, quedan cosas por agregar. 


Clase 11: Clustering 

1- Algunos Clustering (30 mins)
	- Hierarchical clustering y las distintos criterios de "similitud"
	- k-means, formulación y algoritmo de optimización	

2 - Mixture Models (30 mins)
	- 

Comentario: Se arregló la formulación de k-means



