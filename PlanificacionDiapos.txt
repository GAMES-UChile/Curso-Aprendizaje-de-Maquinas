Este será el punteo de las diapos, vamos a considerar que las clases tienen una duración de 1 hora aproximadamente.

Clase 1: Introducción

1- Presentación, aclaraciones sobre el curso y sistemas de evaluación (20 a 30 mins)
	- Primera diapo se presenta al profesor.
	- Segunda diapo se presenta a auxiliares y ayudantes 
	- Tercera diapo se presenta el sistema de evaluaciones, ponderaciones y fechas para entregas de tareas. (En general todo el ámbito administrativo)

2 - Introducción y Orígenes del aprendizaje de máquinas(30 mins)
	- Analizar en función de lo que se considera importante mencionar.

Clase 2: Regresión Lineal

1- Regresión Lineal, Introducción (15 mins)
	- Presentación del problema, gráfica con una regresión lineal y por qué es el primer caso que estudiamos  

2 - Mínimos cuadrados, derivar el theta óptimo (25 mins)
	- Presentación del criterio, notación y cambios de variable.
	- Derivar el theta óptimo (Esto podría realizarse escribiendo sobre el beamer) 

3 - Interpretación Geométrica (20 mins) 
	- Por qué mínimos cuadrados, justificación conceptual y práctica.
	- Justificación geométrica (Presentar el diagrama)
	- Problemas del criterio de mínimos cuadrados (gráfica con/sin outliers)

Clase 3: Regularización

1 - Regularización, Introducción(20 mins)
	- Motivación para utilizar regularizadores, por qué a pesar de aumentar el valor del MSE, sigue siendo útil ()
	- Derivar el theta óptimo para p=2 (Solución cerrada)

2 - Equivalencia con un problema de optimización 
	- Comparar los regularizadores, mostrar curvas de nivel y hablar del efecto selección de variables (Gráfica con los histogramas)
	
Clase 4: Máxima Verosimilitud vs Mínimos Cuadrados

1 - Máxima Verosimilitud (60 mins)
	- Notación, función de verosimilitud, divergencia KL  
	- Divergencia KL a max verosimilitud (Ojo con MonteCarlo, esto debería ser un apéndice del apunte)
	- Estimador de max verosimilitud caso lineal es el que minimiza el error cuadrático  

Comentario: El ejemplo de la verosimilitud del caso Gaussiano se presenta en una hoja del beamer sin mucho detalle (pura matraca)

Comentario: La divergencia KL tiene una motivación de la teoría de la información que se podría mencionar explicitamente, podría mencionarse también que existen otras métricas que si son distancias entre distribuciones de probabilidad)
Revisar: http://blog.thegrandlocus.com/2019/06/focus-on-the-kullback-leibler-divergence que tiene un gran discurso al respecto.
Podría agregarse de apéndice también. 

Clase 5: Inferencia Bayesiana 

1 - Introducción (20 mins)
	- Problema de maximizar solo la verosimilitud, la necesidad de agregar conocimiento previo al problema.
	- Fórmula de Bayes, theta pasa a ser una cantidad aleatoria un poco de historia
2 - Elección de Prior: Conjugación (25 mins)
	- Def prior conjugado y ejemplo del modelo gaussiano mu y sigma conocido
	- Ejemplo para la regresión Lineal
3 - Máximo a posteriori (15 mins)
	- Definición y caso lineal equivale a utilizar regularizador p=1 y p=2

Comentario: Podría dejarse el modelo binomial como lectura propia tanto en prior conjugado como en MAP

Comentario: ¿mu y sigma^2 desconocido seguirá siendo tarea? (fue tarea el año pasado)

Comentario: La normal multivariada no se ve en muchos de los cursos de probabilidades y estadística no dim, por lo que debería ser agregado como apéndice

Clase 6: Predicciones y Regresión no Lineal 

1 - Predicciones (30 mins)
	- Definiciones, variables latentes
	- Concepto de desintegración
	- Mostrar gráficas para el caso lineal 

2 - Regresión no Lineal (30 mins)
	- Definiciones, seleccion de características y modelo lineal en los parámetros
	- Ejemplos de transformaciones y ejemplo de predicción pasajeros aerolinea

Clase 7: Clasificación

1 - 


