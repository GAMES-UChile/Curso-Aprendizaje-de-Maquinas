%!TEX root = notas_de_clase.tex

\section{Selección de modelos}

Supongamos que tenemos un montón de datos, de los cuales podemos ajustar un millón de modelos, tanto como la imaginación nos limite. Entonces ¿Con cuál te quedarías al final?
Cuando nos enfrentamos a la decisión de elegir un modelo usualmente lo que más interesa es la preciso del modelo a la hora de generalizar. El problema de solo buscar precisión es claro: \emph{overfitting} (ver figura \ref{fig:overfitting}).
\begin{figure}[h!]
    \centering
    \includegraphics[width = 0.9\linewidth]{img/cap3_ajuste.pdf}
    \caption{Ejemplos de sub, sobre y correcto ajuste.}
    \label{fig:overfitting}
\end{figure}

La elección de modelo es uno de los problemas más recurrentes en análisis de datos, un ejemplo es la elección de las variables para la regresión ¿Por qué elegir un modelo cuadrático y no uno cúbico? ¿Por qué no incorporar todas las variables cruzadas ($X_1X_2, X_1X_3, ..., X_nX_m$)? Estas preguntas son fácilmente abordables si volvemos a la realidad y queremos correr los modelos en nuestros computadores: Incorporar más variables usualmente aumenta la complejidad computacional y con esto el tiempo de cómputo.

Bajo la filosofía anterior, encontrar un  \textit{trade-off} entre \emph{bias} (la flexibilidad del modelo de ajustarse a los datos) y performance sería lo ideal. Es por esta razón que crear un estadístico que pudiera expresar correctamente el compromiso precisión - flexibilidad ayudaría en la elección de modelo. Esto último es lo que busca relizar el \emph{Akaike Information Criterion} (AIC) y el \emph{Bayesian Information Criterion} (BIC) los cuales son dos enfoques distintos para abordar esta problemática.


\subsection{Descomposición sesgo-varianza}

En el capítulo de regresión lineal se estudió el método de mínimos cuadrados regularizados, el cual generaba un regresor con un mayor error cuadrático medio (ECM) al evaluarlo dentro del conjunto de entrenamiento, pero un mejor desempeño que MC al evaluarlo fuera de muestra. Esto ocurría debido a que, si bien MCR perdía la propiedad de ser un estimador insesgado, la penalización en la norma del parámetro permitía disminuir la varianza del estimador, lo cual resultaba en una disminución del error de estimación debido a la descomposición sesgo-varianza.\\

El objetivo de esta sección será probar dicha descomposición.

\begin{definition}
	Sea $D=\{(x_i,y_i)\}_{i=1}^n\subset\R^N\times\R$ un conjunto de observaciones generadas por una función desconocida $f:\R^N\to\R$ mediante $y=f(x)+\epsilon$ donde $\epsilon$ es una v.a. (ruido) con $\E(\epsilon)=0$ y $\text{Var}(\epsilon)=\sigma^2$. Sea $\hat{f}(\cdot|D)$ un estimador de $f$ determinado a partir de $D$, entonces, se tienen las siguientes definiciones:
	
	\begin{itemize}
		\item Error (cuadrático) esperado: $\E_D\left[(y-\hat{f}(x|D))^2\right]$.
		\item Sesgo del estimador: $\text{Bias}_D(\hat{f}(x|D)):=\E_D(\hat{f}\left(x|D) - y\right) = \E_D(\hat{f}(x|D)) - f(x)$.
		\item Varianza del estimador: $\text{Var}_D(\hat{f}(x|D)) := \E_D\left[\left(\hat{f}(x|D)-\E_D(\hat{f}(x|D))\right)^2\right]$.
		
	\end{itemize}
\end{definition}

Bajo las definiciones anteriores, se tiene el siguiente teorema:

\begin{theorem}[descomposición sesgo-varianza] Sea $\hat{\theta}$ un estimador de un parámetro $\theta$ desconocido, entonces:

\begin{equation}
	\E_D\left[(y-\hat{f}(x|D))^2\right] = \text{Bias}_D(\hat{f}(x|D))^2 + \text{Var}_D(\hat{f}(x|D)) + \sigma^2
\end{equation}
	
\end{theorem}

\begin{proof}

Para evitar sobrecargar la notación, se utilizará $\hat{f}=\hat{f}(x|D)$ y se omitirán los subíndices.

\begin{align*}
&\E\big[(y - \hat{f})^2\big] = \E\big[(f+\epsilon  - \hat{f} )^2\big] \\
 & = \E\big[(f+\epsilon  - \hat{f} +\E(\hat{f})-\E(\hat{f}))^2\big] \\
 & = \E\big[(f-\E(\hat{f}))^2\big]+\E[\epsilon^2]+\E\big[(\E(\hat{f})- \hat{f})^2\big] 
+2\E\big[(f-\E(\hat{f}))\epsilon\big]
+2\E\big[\epsilon(\E(\hat{f})- \hat{f})\big]
+2\E\big[(\E(\hat{f})- \hat{f})(f-\E(\hat{f}))\big] \\
 & = (f-\E(\hat{f}))^2+\E[\epsilon^2]+\E\big[(\E(\hat{f})- \hat{f})^2\big] 
+2(f-\E(\hat{f}))\E[\epsilon]
+2\E[\epsilon]\E\big[\E(\hat{f})- \hat{f}\big]
+2\E\big[\E(\hat{f})- \hat{f}\big](f-\E(\hat{f})) \\
 & = (f-\E(\hat{f}))^2+\E[\epsilon^2]+\E\big[(\E(\hat{f})- \hat{f})^2\big]\\
 & = (f-\E(\hat{f}))^2+\operatorname{Var}[\epsilon]+\operatorname{Var}\big[\hat{f}\big]\\
 & = \operatorname{Bias}(\hat{f})^2+\operatorname{Var}[\epsilon]+\operatorname{Var}\big[\hat{f}\big]\\
 & = \operatorname{Bias}(\hat{f})^2+\sigma^2+\operatorname{Var}\big[\hat{f}\big]
\end{align*}
\end{proof}

Esta descomposición muestra que la varianza intrínseca del ruido afectará directamente y de forma aditiva sobre el error de la predicción, imposibilitando realizar predicciones exactas bajo cualquier modelo aleatorio. Por otra parte, tal como se vio en mínimos cuadrados regularizados, se puede introducir sesgo con el fin de disminuir la varianza y viceversa, lo cual crea la pregunta acerca de cuál es par sesgo-varianza óptimo que minimiza el error total. Dicha pregunta se conoce como dilema sesgo-varianza (bias-variance tradeoff) y juega un papel importante en la selección de hiperparámetros del modelo.\\

De esta forma, la combinación sesgo-varianza crea un error total convexo tal como se puede observar en la siguiente figura:


\begin{figure}[h]
    \centering
    \includegraphics[width = 0.4\linewidth]{img/cap5_biasvariance.pdf}
    \caption{Tradeoff entre el sesgo y la varianza.}
\end{figure}


\subsection{Validación cruzada}

Para poder evaluar el desempeño fuera de muestra, se suele particionar el conjunto de datos $\mathcal{D}$ en dos, donde con el primer conjunto se realizará el entrenamiento y con el segundo se medirá el rendimiento del estimador. Con el fin de evitar posibles sesgos provocados por una partición en específico, la evaluación de desempeño se debe realizar varias veces sobre conjuntos de validación distintos. De esta forma, al promediar los rendimientos de cada partición se obtiene un rendimiento fuera de muestra estimado. Las distintas formas de mezclar y particionar los datos se conocen como validación cruzada.

\subsubsection{Validación cruzada exhaustiva}

En este tipo de validación cruzada, se prueban todas las posibles permutaciones de los datos al particionar el conjunto $\mathcal{D}$. Se tienen 2 técnicas exhaustivas:

\begin{itemize}
	\item l\textbf{eave $p$ out (LpOCV):} el conjunto $\mathcal{D}$ se particiona dejando $p$ elementos para validación y el resto como elementos de entrenamiento. Este entrenamiento y cálculo de desempeño se repite $C_p^N$ veces, pasando por todos los posibles train sets de tamaño $p$.
	\item \textbf{leave one out (LOOCV):} corresponde al caso anterior con $p=1$. En este caso cada dato de $\mathcal{D}$ es utilizado como único elemento de validación mientras el resto de los datos se utiliza para entrenar.
\end{itemize}

\subsubsection{Validación cruzada no exhaustiva}

\begin{itemize}
	\item \textbf{$k$-fold:} el conjunto $\mathcal{D}$ es dividido en $k$ grupos de igual tamaño. Luego, uno de esos grupos es utilizado como validador y el resto como entranemiento. Esto se repite $k$ veces de forma de que todos los grupos sean validadores una y solo una vez.
	\item \textbf{Monte Carlo CV:} se realizan particiones binarias aleatorias de $\mathcal{D}$. Se entrena y evalúa usando el par de conjuntos creados en cada partición.
\end{itemize}

\subsection{Criterio de información Akaike}
La siguiente aproximación es válida cuando $N \to \infty$:

\begin{equation}
-2\mathbb{E}[\log P_{\hat{\theta}}(Y)] \approx \frac{-2}{N}\mathbb{E}[\log L] + 2\frac{d}{N},
\end{equation}

en donde $d$ es la cantidad de parámetros del modelo y  $L$ es la función de verosimilitud definida por:

\begin{equation}
L = \prod_{i=1}^N P(y_i|\hat{\theta}).
\end{equation}

De esta manera se define el estadístico AIC de la forma:

\begin{equation}
AIC(M) = 2d-2\log(L)
\end{equation}

Otros autores definen el estadístico como:

\begin{equation}
AIC(M) = 2\frac{d}{N}-2\frac{log(L)}{N}.
\end{equation}
Donde N es la cantidad de observaciones.

\textbf{\underline{Ejemplo:}} Si tenemos un conjunto de modelos $f_{\alpha}(x)$ indexados por un parámetro $\alpha$ de modo que el modelo $\alpha$ sigue un modelo gaussiano, es decir:
$$
\log(L) = -\sum_{i}(y_i-\hat{f}(x_i))^2/(2\sigma_e^2),
$$

\noindent entonces el estimador AIC se puede escribir como
\begin{equation}
\text{AIC}(M) = 2d(\alpha)-\overline{err}(\alpha),
\end{equation}
en donde la función $d(\alpha)$ denota una medida de la complejidad del modelo y $\overline{err}(\alpha)$ denota el error promedio del modelo $\alpha$.

\subsection{Criterio de información bayesiano}
Nuevamente, si nuestro setting nos permite obtener el máximo de la función de verosimilitud. Podemos utilizar el \emph{Bayesian Information Criterion} (BIC) el cual está definido por:

\begin{equation}
\text{BIC}(M) = log(N)d-2\log(L)
\end{equation}

En donde N es la cantidad de observaciones, $d$ es la cantidad de parámetros del modelo y L es el máximo de la función de verosimilitud. Este criterio también es conocido como \emph{Schwarz criterion}. Este estadístico se dice bayesiano por que se obtiene de realizar una aproximación de laplace sobre sobre $\log\mathbb{P}(X|M)$ obteniéndose:

\begin{equation}
\log\mathbb{P}(X|M) \approx \log\mathbb{P}(X|\hat{\theta}_m, M) - \frac{d_m}{2}(\log(N)-\log(2\pi))
\end{equation}

Donde $\hat{\theta}_m$ es el máximo del estimador de verosimilitud. Luego para $N$ muy grande se observa que la expresión $-2\log\mathbb{P}(X|M) = BIC(M)$.

\textbf{\underline{Ejemplo:}} Asumiendo que el modelo sigue un modelo gaussiano, es decir:
$$
\log(L) = -\sum_{i}(y_i-\hat{f}(x_i))^2/(2\sigma_e^2),
$$se tiene que el estadístico BIC está dado por:

\begin{equation}
BIC(M) = \frac{N\overline{err}(M)}{\sigma_e^2} + log(N)d
\end{equation}


\subsection{Evaluación y comparación de modelos}
Existen varias formas de evaluar un modelo, una de ella podría ser simplemente evaluar la precisión de sus predicciones. A veces fijarse es natural fijarse en la precisión, como en los problemas de pronóstico. Otras veces la precisión es importante para evaluar diferentes modelos y elegir uno de ellos. En esta sección presentaremos dos maneras distintas de evaluar modelos, cada forma sirve en distintos escenarios, los cuales se discutirán a través de la predicción puntal, que resume la predicción de un conjunto de datos en una solo valor.

\subsubsection{Error cuadrático medio}

El ajuste del modelo a nuevos datos se puede resumir en una predicción puntual llamada error cuadrático medio, el cual está definido por:

\begin{equation}
MSE(\theta) = \frac{1}{n}\sum_{i=1}^N (y_i-\mathbb{E}(y_i|\theta))^2
\end{equation}

o su versión ponderada:

\begin{equation}
MSE(\theta) = \frac{1}{n}\sum_{i=1}^N \frac{(y_i-\mathbb{E}(y_i|\theta))^2}{\mathbb{V}\text{ar}(y_i|\theta)}
\end{equation}

Esta forma de medir el error tiene la ventaja de ser fácilmente computable e interpretable, pero no es apropiada para modelos que están lejos de la distribución normal.

\subsubsection{log-densidad predictiva o log-verosimilitud}
Otra forma de realizar esta evaluación es utilizando el estadístico \emph{log-densidad preditiva} $\log p(y|\theta)$ el cual es proporcional a error cuadrático medio si el modelo es normal con varianza constante. Estudiaremos el caso de un solo punto, para luego extrapolar a más de un punto.

\textbf{Predictive accuracy para un punto}: Sea $f$ el modelo real, $y$ las observaciones (es decir, una realización del dataset $y$ de la distribución $f(y)$), y llamaremos $\tilde{y}$ a la data futura o un dataset alternativos que podemos ver. El ajuste predictivo out-of-sample para un nuevo punto $\tilde{y}_i$ está dado por:

\begin{equation}
\log p_{\text{post}}(\tilde{y}_i) = \log \mathbb{E}[p(\tilde{y}_i|\theta)] = \log \int p(\tilde{y}_i|\theta)p_{\text{post}}(\theta)d\theta
\end{equation}

\textbf{Promedio de las distribuciones para un punto}: Al tener un dato nuevo $\tilde{y}_i$ entonces se puede calcular el la log-densidad predictiva (elpd, por su sigla en inglés) para el nuevo punto:
\begin{align}
\notag \text{elpd} & = \mathbb{E}_f[\log p_{\text{post}}(\tilde{y}_i)]\\
& = \int \log p_{\text{post}}(\tilde{y}_i) f(\tilde{y}_i)d\tilde{y}
\end{align}

\textbf{Promedio de las distribuciones para datasets futuros}: Como usualmente, no se tiene solo un punto, se debe realizar la suma sobre el conjunto de puntos, calculando así la log-densidad predicitiva puntual (elppd, por su sigla en inglés).

\begin{align}
\text{elppd} & = \sum_{i=1}^N \mathbb{E}_f[\log p_{\text{post}}(\tilde{y}_i)]
\end{align}

En la práctica, como siempre se tiene la distribución de todos los modelos y la expresión anterior requiere de esto, se suele calcular el estadístico sobre una estimación de un modelo $\hat{\theta}$ (como por ejemplo, el máximo de la función de verosimilitud):

\begin{align}
\text{elppd}|\hat{\theta} & = \sum_{i=1}^N \mathbb{E}_f[\log p_{\text{post}}(\tilde{y}_i|\hat{\theta})]
\end{align}

Finalmente, una última extensión de este estadístico es cuando se puede tener \emph{draws} de la posterior, es decir, tenemos $\{ \theta^s\}_{s=1}^S$, entonces el lppd computado es:

\begin{equation}
\text{computed lppd} = \sum_{i=1}^N \left( \frac{1}{S} \sum_{s=1}^S p(y_i|\theta^s)\right)
\end{equation}

\subsubsection{Otros métodos}
Existen otros estadísticos o métodos que se pueden utilizar para comparar modelos. 

\begin{itemize}
    \item \textbf{Deviance Information Criterion:} Se podría decir que es una versión bayesiana de AIC, donde se realizan dos cambios principales 1) se cambia la estimación del estimador de máxima verosimilitud, por el promedio de la posterior y 2) se reemplaza k con una perrción de sesgo de los datos. Formalizando:
    
    \begin{equation}
        \text{DIC} = -2 \log(y|\hat{\theta}_{Bayes}) + 2p_{\text{DIC}},
    \end{equation}
    donde $p_{\text{DIC}}$ está dado por:
    
    \begin{equation}
        p_{\text{DIC}} = 2(\log(p(y|\hat{\theta}_{Bayes})- \mathbb{E}_{\text{post}}(\log(p(y|\theta)).
    \end{equation}
    
    Nuevamente, puede haber un versión \emph{computada} que está dada por:
    
    \begin{equation}
        \text{computed }p_{\text{DIC}} = 2[\log(p(y|\hat{\theta}_{Bayes})- \frac{1}{S}\sum_{s=1}^S \log (p(y|\theta^s))].
    \end{equation}
    %-------------------------------
    \item \textbf{Watanabe-Akaike o widely available information criterion:} WAIC es un forma aún más bayesiana para abordar el problema de asginación de evaluación. 
    
    \begin{equation}
        \text{WAIC} = -2 \text{lppd} + 2 p_{\text{WAIC}}
    \end{equation}
    
    donde hay dos formas de calcular $p_{\text{WAIC}}$.
    
    \begin{align}
    p_{\text{WAIC}1} &= 2 \sum_{i= 1}^n \left( \log(\mathbb{E}_{\text{post}}[p(y_i|\theta)]) - \mathbb{E}_{\text{post}}[\log (p(y_i|\theta))\right)\\
    p_{\text{WAIC}2} &= \sum_{i= 1}^n \left( \mathbb{V}\text{ar}_{\text{post}}[\log(p(y_i|\theta))]\right).
    \end{align}
    %-------------------------------    
    \item \textbf{Leave-one-out cross-validation:} También se puede usar validación cruzada bayesiana para estimar $-2\text{lppd} + 2\text{lppd}_{\text{loo-cv}}$ la cual sirve como estadístico de comparación:
    
    \begin{equation}
    \text{lppd}_{\text{loo-cv}} = \sum_{i=1}^n \log p_{\text{post}(-i)}(y_i)
    \end{equation}
    
    que se calcula como:
    
    \begin{equation}
    \text{computed lppd}_{\text{loo-cv}} = \sum_{i=1}^n \log \left( \sum_{s=1}^S \frac{1}{S} p(y_i|\theta^{is})\right)
    \end{equation}
    
    también se puede calcular a versión insesgada, la cual no se utiliza mucho pero se presenta por completitud:
    \begin{equation}
    \text{lppd}_{\text{cloo-cv}} = \text{lppd}_{\text{loo-cv}} + b.
    \end{equation}
    Donde $b$ está dado por:
    
    \begin{align}
    b = \text{lppd}_{-i} - \overline{\text{lppd}_{-i}},
    \end{align}
    
    con 
    
    \begin{align}
    \overline{\text{lppd}_{-i}} & = \frac{1}{n} \sum_{i=1}^n\sum_{j=1}^n \log p_{\text{post}(-i)}(y_j)\\
    \text{computed }\overline{\text{lppd}_{-i}} & = \frac{1}{n} \sum_{i=1}^n\sum_{j=1}^n \log \left( \frac{1}{S}\sum_{s=1}^S p(y_j|\theta^{is})\right).
    \end{align}
    
\end{itemize}

\subsection{Promedio de Modelos}
Hay veces que no queremos elegir solo un modelo, puesto que quizás nos interesan las estimaciones de dos o más modelos. Para estos casos, se puede utilizar la técnica de \emph{Model Averaging}, la cual consiste en simplemente ponderar los modelos de la siguiente forma:

\begin{equation}
\hat{\mu} = \sum_{s\in A} c(s) \hat{\mu}_s
\end{equation}
donde $A$ es el conjunto de todos los modelos. Además, se debe cumplir que:
\begin{equation}
\sum_{s\in A} c(s) = 1
\end{equation}

Notemos que la elección:

\begin{equation}
c(s) =
\begin{cases}
1 & \text{modelo con mejor criterio.}\\
0 & \sim
\end{cases}
\end{equation}

es una elección de pesos posible.

\subsubsection{Elección de pesos mediante softmax}

La principal dificultad para elegir los pesos, está en que se debe asegurar 1) la suma de los pesos sea unitaria, 2) que los pesos sean todos positivos. Una forma de asegurar esto es aplicar una función $f$ positiva sobre una función $g$ de \emph{score}, de modo que:

\begin{equation}
\sum_{s\in A} (f\circ g)(s) > 0
\end{equation}

Así los pesos quedan se pueden definir cómo:

\begin{equation}
c_f(s) = \frac{(f\circ g)(s)}{\sum_{z\in A} (f\circ g)(s)}
\end{equation}

La elección más común de para $f(x) = \text{softmax}(x)$, mientras que para la función de \emph{score} se puede utilizar las recién estudiadas AIC o BIC.

\begin{align}
c_{AIC}(s) = \frac{\exp\{ \frac{1}{2} \text{AIC}_s\}}{\sum_{z\in A} \exp\{ \frac{1}{2} \text{AIC}_z\}} & & c_{BIC}(s) = \frac{\exp\{ \frac{1}{2} \text{BIC}_s\}}{\sum_{z\in A} \exp\{ \frac{1}{2} \text{BIC}_z\}}
\end{align}


\subsubsection{Bayesian Model Averaring}

Supongamos que tenemos $\{M_j\}_{j=1}^k$ modelos, de los cuales todos realizan estimaciones razonables de una cantidad $\mu$, dado los datos $y$.
Para realizar el modelo promedio bayesiano de estos modelos es necesario encontrar la distribución posterior de $\mu$ dado los datos $y$ sin condicionar a un modelo en específico. De este modo, es necesari:
\begin{enumerate}
    \item Cada modelo tenga los mismos puntos de interpolación.
    \item Prior para cada modelo $p(M_j)$.
    \item Prior sobre los parámetros de los modelos $\pi(\theta_i|M_j)$\footnote{Se puede obtener mediante MCMC.}.
\end{enumerate}


Luego se puede calcular la función de verosimilitud para los parámetros:

\begin{equation}
\mathcal{L}_{\theta_j}(y) = p(y|\theta_j)
\end{equation}

Luego se marginaliza sobre los parámetros para obtener la función de verosimilitud del modelo:

\begin{align}
\lambda_{M_j}(y) & = p(y|M_j)\\
& = \int \mathcal{L}_{\theta_j}(y) \pi(\theta_i|M_j) d\theta_j\label{eq:lambda}
\end{align}

La expresión \eqref{eq:lambda} es también densidad marginal de las observaciones. Con esto la densidad posterior del modelo es:

\begin{align}
p(M_j|y) = \frac{p(M_j)\lambda_{M_j}(y)}{\sum_{l=1}^k p(M_{l})\lambda_{M_{l}}(y)}    
\end{align}

Finalmente, se puede obtener la distribución posterior para $\mu$ asumiendo que el modelo $M_j$ es verdadero ($\pi(\mu|M_j,y)$), y así, la distribución posterior buscada:

\begin{equation}
\pi(\mu|y) = \sum_{j = 1}^k p(M_j|y)\pi(\mu|M_j,y)
\end{equation}

