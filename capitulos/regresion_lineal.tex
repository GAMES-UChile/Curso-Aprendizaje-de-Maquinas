%!TEX root = ../notas_de_clase.tex

\section{Regresión Lineal} 

El problema de regresión busca determinar la relación entre una variable \emph{independiente} (entrada, estímulo o característica; usualmente denotada por $x$) y una variable \emph{independiente} (salida, respuesta o etiqueta; usualmente denotada $y$). Intuitivamente, un modelo de regresión permite entender cómo cambia la variable dependiente cuando la variable independiente es modificada. Esta relación entre ambas variables es representada por una función, consecuentemente, el problema de regresión es equivalente a encontrar una función definida desde el espacio de la entrada $x$ al de la salida $y$. De esta forma, en base a (i) el espacio de posible funciones donde se busque dicha relación, e.g., los polinomios de grado menor o igual a 5, y a (ii) el criterio de búsqueda que se aplique, e.g., mínimos cuadrados, podemos obtener distintas soluciones para el problema de regresión. 

El escenario básico de regresión, y que sirve de base para casos más complejos, es el de regresión lineal. En este caso, el espacio de funciones donde se busca la relación entre las variables dependientes e independientes es el de las funciones lineales afines. Específicamente, para un conjunto de entrenamiento $D$ que contiene $N\in\N$ observaciones de entrada y salida, respectivamente $\{x_i\}_{i=1}^N$ y $\{y_i\}_{i=1}^N$, de la forma
\begin{equation}
	D=\{(x_i,y_i)\}_{i=1}^N\subset \R^M \times \R,
	\label{eq:training_set}
\end{equation}
la regresión lineal busca encontrar un modelo lineal, es decir, una función $f(\cdot)$ definida por 
\begin{align}
  f \colon \R^M &\to \R\nonumber\\
  x &\mapsto f(x)=a^\top x + b,\quad a\in\R^M,b\in\R,
 \label{eq:reg_lin_fn} 
\end{align}
que \emph{mejor represente} la forma en que la variable $y$ depende de la variable $x$, en base a las las observaciones contenidas en el conjunto $D$ en la ec.~\eqref{eq:training_set}. Antes de proceder a definir un criterio de \emph{mejor representación}, el siguiente recuadro justifica la elección de modelos lineales. 

\begin{mdframed}[style=discusion, frametitle={\center ¿Por qué consideramos el caso lineal en particular?}]
	Existen distintas razones para estudiar los modelos lineales. En primer lugar, con el criterio de mínimos cuadrados que veremos a continuación, el modelo lineal es el único que admite resolución de forma explícita (o, como diremos alternativamente, \emph{tiene forma cerrada}). Además de calcular dicha solución, la existencia de su forma cerrada nos permite interpretar las propiedades de dicha solución y en qué casos ésta tiene sentido. En segundo lugar, los resultados que obtendremos a continuación requieren linealidad solo en los parámetros---ver ec.~\eqref{eq:reg_lin_fn}---y no necesariamente en la variable independiente $x$. Por esta razón, el estudio del modelo lineal también incluye modelos no lineales del tipo
\begin{align}
  f \colon \R^M & \to \R\nonumber\\
  x &\mapsto f(x)=\theta^\top \phi(x), \quad \theta\in\R^{M'},
 \label{eq:reg_no_lin_fn} 
\end{align}
donde $\phi \colon \R^M \to \R^{M'}$ es una función no lineal sin parámetros libres. Es decir, estrictamente hablando deberíamos referirnos a los modelos lineales como \emph{lineales en los parámetros} y no necesariamente \emph{lineales en la entrada}. 

Finalmente, cuando los parámetros a determinar en el problema de regresión afectan de forma no lineal la relación entre las variables dependiente e independiente, el análisis presentado a continuación no es válido y en general la solución óptima de mínimos cuadrados no tiene forma cerrada. 
\end{mdframed}

\subsection{Mínimos cuadrados} % (fold)
\label{ssub:min_cuad}
En el contexto recién presentado, aflora naturalmente la siguiente pregunta: \emph{¿qué es una buena función $f(\cdot)$?} o, equivalentemente, \emph{¿cómo cuantificar la bondad de un modelo de regresión lineal?} Una práctica ampliamente utilizada es elegir la función $f(\cdot)$ en la ec.~\eqref{eq:reg_lin_fn} de acuerdo al criterio de \textbf{mínimos cuadrados}. Es decir, elegir la función $f(\cdot)$ que minimiza la suma de los cuadrados de las diferencias entre las observaciones $\{y_i\}_{i=1}^N$ y las predicciones calculadas por la función $\{f(x_i)\}_{i=1}^N$ de acuerdo al siguiente costo:
\begin{equation}
	J(D,f) = \frac{1}{2}\sum_{i=1}^N(y_i-f(x_i))^2,
	\label{eq:least_squares_cost}
\end{equation}
donde hemos sido enfáticos en que el costo depende del conjunto de entrenamiento $D=\{(x_i,y_i)\}_{i=1}^N$ y la función $f$, sin embargo, cuando estas cantidades son claras, nos referiremos al costo simplemente como $J$. Además, denotamos la (o las) funciones que satisfacen el criterio de mínimos cuadrados mediante
\begin{equation}
	f^\star = \argmin_{f\text{ es lineal}} J.
\end{equation}
Debido a la forma lineal de $f(\cdot)$, resolver este problema de optimización es equivalente a encontrar los parámetros $a$ y $b$ en la ec.~\eqref{eq:reg_lin_fn}. Es decir: 
\begin{equation}
	a^\star,b^\star = \argmin_{a,b} \frac{1}{2}\sum_{i=1}^N(y_i-a^
	\top x_i - b)^2.
	\label{eq:lin_least_squares}
\end{equation}

Observemos que el costo en la  ec.~\eqref{eq:lin_least_squares} es cuadrático en $a$ y $b$, por lo qur  el problema de optimización tiene un único mínimo que puede ser encontrado explícitamente. Para esto, como la función $f$ en la ec.~\eqref{eq:reg_lin_fn} no es lineal sino que \emph{afín}, hacemos el siguiente cambio de variable:
\begin{equation}
  \left[ \begin{matrix}x \\  1\end{matrix}\right] \mapsto \tx\in\R^{M+1},\quad
  \left[ \begin{matrix}a \\  b\end{matrix}\right]\mapsto \theta\in\R^{M+1},
 \label{eq:truco_reg_lin} 
\end{equation}
con lo cual el funcional cuadrático a minimizar se convierte en
\begin{equation}
	J = \frac{1}{2}\sum_{i=1}^N(y_i-\theta^
	\top \tx_i)^2,
	\label{eq:lin_least_squares2}
\end{equation} 
y el parámetro $\theta$ de mínimos cuadrados puede ser encontrado aplicando las condiciones de primer orden de la siguiente forma:
\begin{align}
\nabla_\theta J=0 &\Leftrightarrow \sum_{i=1}^N(y_i-\theta^\top \tx_i)\tx_i^\top=0  							&&\text{def. $J$}\nonumber\\  
&\Leftrightarrow \sum_{i=1}^Ny_i\tx_i^\top = \sum_{i=1}^N\theta^\top \tx_i\tx_i^\top					&&\text{ordenar}\nonumber\\
&\Leftrightarrow \theta^\top = \sum_{i=1}^Ny_i\tx_i^\top \left(\sum_{i=1}^N \tx_i\tx_i^\top\right)^{-1}	&&\text{despejar $\theta^\top$}\nonumber\\
&\Leftrightarrow \theta =  \left(\sum_{i=1}^N \tx_i\tx_i^\top\right)^{-1} \sum_{i=1}^N \tx_i y_i 		&&\text{transponer}\nonumber\\
&\Leftrightarrow \theta = \left(\tX^\top\tX \right)^{-1} \tX^\top Y, 											&&\text{def. $\tX$ y $Y$} \label{eq:sol_mse}
\end{align}
donde $\tX$ y $Y$ son las matrices de datos definidas por

\begin{equation}
  \tX = \left[ \begin{matrix}\tx_1^\top \\\vdots \\ \tx_N^\top \end{matrix}\right]\in\R^{N\times (M+1)} ,\quad
  Y = \left[ \begin{matrix}y_1 \\\vdots \\y_N \end{matrix}\right] \in\R^{N}.
 \label{eq:matrices_X} 
\end{equation}

Con los parámetros del modelo regresión lineal encontrados con el criterio de mínimos cuadrados, es posible implementar la solución y comprarla visualmente con los datos. La Fig.~\ref{fig:reg_lin_1} muestra la regresión lineal correspondiente a chirridos de grillos por segundo en función de la temperatura \cite{insects}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{img/cap1_chirridos.pdf}\\
	\caption{Ejemplo de regresión lineal usando mínimos cuadrados sobre la base de datos de chirridos versus temperatura.}
	\label{fig:reg_lin_1}
\end{figure}

La expresión $\left(\tX^\top\tX \right)^{-1} \tX^\top$ en la ec.~\eqref{eq:sol_mse} es conocida como la pseudo-inversa de Moore-Penrose \cite[p.~7]{benisrael_greville_2006}. Observemos que una condición \textbf{necesaria} para que esta pseudo-inversa esté bien definida es que la cantidad de observaciones ($N$) sea mayor o igual que la cantidad de dimensiones ($M+1$). Esto es porque la matriz $\tX^\top\tX$ es de tamaño $(M+1)\times(M+1)$ y su rango es $\min \{N, M+1\}$, consecuentemente, para que $\tX^\top\tX$ tenga rango completo (y por ende sea invertible), se debe cumplir al menos que $N\geq M+1$. Adicionalmente, una condición \textbf{necesaria y suficiente}  para la existencia de la solución de mínimos cuadrados en la ec.~\eqref{eq:sol_mse}, requiere que las $N\geq M+1$ observaciones\footnote{Recordemos que nos referimos a las observaciones aumentadas $\tilde{x}$} sean linealmente independientes, pues de esta forma los términos que componen la pseudo-inversa son efectivamente linealmente independientes y ésta tiene rango completo. Es claro que para el caso de variables continuas es muy poco usual que dos observaciones sean perfectamente colineales, sin embargo, en el caso de variables categóricas donde las observaciones son asignadas a un número finito de símbolos es probable que dos o más valores para la variable dependiente sean exactamente iguales. 

En la práctica, generalmente tendremos más observaciones que parámetros al considerar un modelo lineal y éstas serán linealmente independientes. Sin embargo, es posible que las observaciones sean tal que la inversión de la matriz  $\tX^\top\tX$ sea numéricamente  inestable. Esto ocurre fundamentalmente en dos casos ilustrados en el siguiente recuadro.  

\begin{mdframed}[style=discusion, frametitle={\center ¿Matriz cuasi-singular o incorrectamente escalada?}]

Al tratar de invertir una matriz de forma computacional, probablemente hemos obtenido un mensaje de la forma \texttt{matrix is singular, close to singular or badly scaled}. Veremos dos ejemplos para entender de dónde viene esta advertencia. 

\noindent\textbf{Caso 1:} Consideremos la matriz 
\begin{equation}
	A = \left[ \begin{matrix}10^{50} & 1 \\  10^{50}  & 2\end{matrix}\right]
\end{equation}
dicha matriz es claramente invertible y su inversa puede ser calculada mediante
\begin{equation}
	A^{-1} = \frac{1}{10^{50} \cdot 2 - 10^{50}\cdot 1}\left[ \begin{matrix}2 & -1 \\  -10^{50}  & 10^{50}\end{matrix}\right]
	=\left[ \begin{matrix}2\cdot10^{-50} & -10^{-50} \\  -1  & 1\end{matrix}\right],
\end{equation}
donde cuyos elementos difieren en 50 órdenes de magnitud. Sin embargo, la representación usual que consideramos cuando programamos es la de punto flotante de precisión simple, la cual considera el menor valor (de magnitud mayor que cero) de $2^{-127}\approx 10^{-38}$. Consecuentemente, los valores más pequeños que este límite serán aproximados por el elemento más cercano, es decir, cero. Utilizando la inversa aproximada, denotada $\tilde{A}^{-1}$, resulta en errores como el siguiente:
\begin{equation}
	A\tilde{A}^{-1} = \left[ \begin{matrix}10^{50} & 1 \\  10^{50}  & 2\end{matrix}\right] \left[ \begin{matrix} 0 & 0 \\  -1  & 1\end{matrix}\right] = \left[ \begin{matrix} -1 & 1 \\  -2  & 2\end{matrix}\right]
\end{equation}

\noindent\textbf{Caso 2:} Consideremos
\begin{equation}
	A = \left[ \begin{matrix} a  & a \\  b  & b + \epsilon\end{matrix}\right]
\end{equation}
la cual también es invertible para $a,\epsilon>0$, pues su determinante está dado por
\begin{equation}
	\det{A} = a(b+\epsilon) - ab = a\epsilon>0,
\end{equation}
sin embargo, si $\epsilon\ll1$ entonces el cálculo de la inversa puede sufrir inestabilidades numéricas como en el caso anterior. Sin embargo, observe para un $\eta>0$ suficientemente grande, la matriz $A+\eta I$ puede tener un determinante arbitrariamente grande (ver Sección \ref{sub:min_cuad_reg}).
\end{mdframed}

Es relevante reflexionar por qué consideramos mínimos cuadrados como la métrica de error relacionada al problema de regresión. Existen varias razones por que lo hacemos, tanto técnicas como conceptuales, como también diversas desventajas de este criterio que es importante identificar.  Desde del punto de vista técnico, el costo convexo de un modelo lineal (en los parámetros) define un problema de optimización que también es convexo y por ende tiene una solución única. Además, en el caso particular del costo cuadrático, este óptimo puede ser determinado de forma explícita (lo cual es fuertemente deseado), pues está dado únicamente por la inversión de una matriz y no mediante, e.g., una búsqueda iterativa.

Desde un punto de vista conceptual, otra justificación para usar la medida del  error cuadrático es que éste representa la varianza muestral. Es decir, si considerásemos que $x_i$ e $y_i$ son observaciones iid de variables aleatorias (VAs) $x$ e $y$ respectivamente, entonces el error cuadrático (relacionado con la función $f$) definido por
\begin{equation}
	e= \sum_{i=1}^N (y_i-f(x_i))^2,
\end{equation}
es la varianza muestral de la variable aleatoria $y-f(x)$, definida como el \emph{error} de estimación. De igual forma, la varianza de la suma de múltiples variables aleatorias (pensemos en errores acumulados, los cuales son independientes) corresponde a la suma de las varianzas de dichas VAs. Esto ocurre precisamente cuando usamos el exponente igual a 2, y no si usáramos 1.95 o 2.05. 

Podemos además justificar el uso del error cuadrático con una motivación geométrica. Recordemos que el problema de regresión (lineal) requiere encontrar una solución aproximada de un sistema lineal sobredeterminado definido por 
\begin{equation}
	\tX \theta = Y,\label{eq:sist_lineal_sobredet}
\end{equation}
donde la cantidad de incógnitas ($M+1$) es ampliamente superada por el número de ecuaciones ($N$). Como esta solución, desde el punto de vista de un sistema lineal, no existe, uno puede proceder a encontrar la solución para $\theta$ que reporta \emph{la menor discrepancia} entre ambos lados de la ec.~\eqref{eq:sist_lineal_sobredet}. En este sentido, podemos identificar el espacio formado por todos los posibles valores que toma la combinación lineal $\tX \theta$ para distintos valores de $\theta\in\R^{M+1}$, es decir, el \emph{span} de todas las columnas de $\tX$ (los datos) definido como el \emph{espacio de las columnas de} $\tX$ o, simplemente, $\text{span}(\tX)$. Luego, podemos identificar el elemento de dicho espacio que está más cerca de $Y$  como la proyección del propio $Y$ en $\text{span}(\tX)$. Esto está ilustrado en la Fig.~\ref{fig:projection}, donde la condición para identificar dicha proyección es  precisamente que el vector error $\tX \theta- Y$ sea ortogonal al espacio  $\text{span}(\tX)$ generado por los datos (de entrada), consecuentemente, ocupando el producto interno tenemos que 
\begin{equation}
	(\tX \theta- Y)^\top \tX = 0,
\end{equation}
lo cual nos lleva directamente a la solución de mínimos cuadrados. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\textwidth]{img/LinRegGeo.pdf}\\
	\caption{Interpretación geométrica de la regresión lineal y mínimos cuadrados}
	\label{fig:projection}
\end{figure}

Finalmente, notemos que el criterio de  mínimos cuadrados (MC) también tiene desventajas. Implícitamente, MC está intrínsecamente relacionado con un supuesto de gaussianidad de los datos---esto será evidente cuando estudiemos el criterio de máxima verosimilitud---consecuentemente, el uso de MC produce estimaciones razonables cuando la relación entre $x$  e $y$ es simétrica y sin \emph{grandes desviaciones}. Por el contrario, cuando existen datos  que se alejan mucho de dicha la tendencia buscada, las estimaciones encontradas mediante MC pueden desviarse considerablemente de la solución buscada, esto se debe precisamente a la contribución cuadrática del error, donde, coloquialmente, una muestra \emph{muy alejada} pesa tanto o más que varias muestras \emph{ligeramente alejadas}. La Fig.~\ref{fig:reg_lin_2} ilustra este fenómeno para el mismo ejemplo de los chirridos en la Fig.~\ref{fig:reg_lin_1}, donde se ha introducido un \emph{outlier}, es decir, una observación que está inusualmente alejada de los datos y se ha recalculado el resultado de la regresión lineal mediante el criterio de MC. Se puede ver cómo se deteriora la estimación solo con la introducción de un nuevo dato. 



\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{img/cap1_chirridos_outlier.pdf}\\
	\caption{Efecto de un \emph{outlier} en la regresión lineal usando mínimos cuadrados: Se ha agregado un dato erróneo (\emph{outlier} en gris) y se ha recalculado la regresión lineal, note cómo la inclusión de dicho punto deteriora el resultado de la regresión.}
	\label{fig:reg_lin_2}
\end{figure}

La lección que queda de este ejemplo es que debemos considerar una métrica \emph{ad hoc} al problema que estamos considerando, por ejemplo, si es muy probable que existan outliers, no debemos penalizar cuadráticamente los errores. De igual forma, al elegir una métrica de error debemos verificar cuán relevante es que el error de regresión sea i) nulo vs muy pequeño, o bien ii) grande vs extremadamente grande. La Fig.~\ref{fig:reg_lin_err} presenta cuatro métricas de error (como función del propio error), donde podemos interpretar sus propiedades. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/cap1_errores.pdf}\\
	\caption{Distintas funciones de costo en función del error de estimación, de izquierda a derecha: cuadrático, absoluto (crecimiento lineal en función del error), $\epsilon$-insensible (es irrelevante si el error está entre 0 o  $\epsilon$) y acotado (es irrelevante si el error es mayor que cierto umbral).}
	\label{fig:reg_lin_err}  
\end{figure}


\subsubsection{Regularización: ajuste versus generalización} % (fold)
\label{sub:min_cuad_reg}

Perseguir ciegamente la solución de mínimos cuadrados puede resultar, como discutimos en la sección anterior, en situaciones donde la inversa de Moore-Penrose sea \emph{cercana} a singular, especialmente en los casos que las observaciones son parecidas o redundantes. En este sentido, debemos considerar un criterio que no simplemente busque un ajuste a los datos, sino que también promueva ciertas propiedades de la solución, por ejemplo, suavidad, bajas magnitudes de los parámetros o incluso pocos parámetros. Nos referiremos a estas soluciones como \emph{regulares}, y el objetivo de este apartado será \emph{regularizar} la solución de mínimos cuadrados.

Las penalizaciones a considerar en el problema de regresión pueden ser codificadas directamente en la función de costo. Por ejemplo, ésta puede incluir un término que promueve el ajuste de los datos y otro término que sanciona soluciones que se alejan de lo deseado. Un criterio estándar de penalización es el basado en la norma de los parámetros, es decir, 
\begin{equation}
	J_\rho = \frac{1}{2}\sum_{i=1}^N(y_i-\theta^
	\top \tx_i)^2 + \frac{\rho}{p}||\theta||_p^p,\ p\in\R_+,
	\label{eq:reg_least_squares}
\end{equation} 
donde $||\cdot||_p$ denota la norma $\ell_p$, es decir, $||\theta||_p=\left(\sum_{j=1}^N|\theta_j|^p\right)^\frac{1}{p}$ y el parámetro $\rho\geq0$ tiene el rol de balancear la importancia entre ajuste (primer término) y regularidad de la solución (segundo término). Distintos valores de $p$ inducen distintos propiedades sobre las soluciones, siendo las más usadas las correspondientes a $p=1$, conocido como \textbf{LASSO}\footnote{\emph{Least Absolute Shrinkage and Selection Operator.}} \cite{tibshirani_1996}, y $p=2$ conocido como \textbf{regularización de Tikhonov} \cite{tikhonov_arsenin_1977} o bien \textbf{\emph{Ridge Regression}} \cite{hoerl_kennard_1970}.  

Una ventaja de la regularización de Tikhonov es que su solución, al igual que el caso de mínimos cuadrados no regularizados, puede ser encontrada en forma exacta. En efecto, para $p=2$ el término de regularización puede ser expresado como $||\theta||_2 = \theta^\top\theta$, con lo que el minimizante del costo cuadrático regularizado está dado por: 
\begin{align}
\nabla_\theta J_\rho=0 &\Leftrightarrow \sum_{i=1}^N(\theta^\top \tx_i - y_i)\tx_i^\top + \rho\theta^\top=0  							&&\text{def. $J$}\nonumber\\  
&\Leftrightarrow \sum_{i=1}^Ny_i\tx_i^\top = \sum_{i=1}^N\theta^\top \tx_i\tx_i^\top + \rho\theta^\top					&&\text{ordenar}\nonumber\\
&\Leftrightarrow \theta^\top = \sum_{i=1}^Ny_i\tx_i^\top \left(\sum_{i=1}^N \tx_i\tx_i^\top + \rho \eye\right)^{-1}	&&\text{despejar $\theta^\top$}\nonumber\\
&\Leftrightarrow \theta =  \left(\sum_{i=1}^N \tx_i\tx_i^\top +\rho \eye\right)^{-1} \sum_{i=1}^N \tx_i y_i 		&&\text{transponer}\nonumber\\
&\Leftrightarrow \theta = \left(\tX^\top\tX +\rho \eye\right)^{-1} \tX^\top Y.								&&\text{def. $\tX$ y $Y$} \label{eq:least_sq_soln}
\end{align}

De la última expresión, es posible ver que el requerimiento de que las observaciones disponibles sean (i) más que la dimensión $M+1$ y que además (ii) éstas sean colineales ya no es necesario para que la solución esté bien definida. De hecho, la matriz $\tX^\top\tX$ puede efectivamente estar cercana a ser no invertible, sin embargo, es posible \emph{regularizar} la solución forzando que la matriz $\left(\tX^\top\tX +\rho \eye\right)$ sea arbitrariamente lejana de las matrices singulares (o tenga un determinante arbitrariamente grande) aumentando el valor de $\rho$. 




Es relevante entender por qué una disminución en la norma de los parámetros, puede ayudar a ajustar \emph{mejores} modelos. A primera impresión, un podría pensar que el criterio de mínimos cuadrados regularizados (MCR) en ningún caso puede reportar mejores modelos que su contraparte MC, pues MCR es una variante restringida del problema original y consecuentemente solo puede \emph{en el mejor de los casos} alcanzar la solución óptima. Esto es cierto si por \emph{mejor modelo} solo consideramos el error cuadrático medio (ECM), sin embargo, solo enfocarse en esta métrica no siempre es el mejor opción. Para ilustrar este concepto, tomemos las siguientes consideraciones: asumamos que efectivamente los datos cumplen la relación
\begin{equation}
	y_i = \underbrace{\theta^\top\tx_i}_{f_i} + \epsilon_i,	
 \end{equation}
 donde $\epsilon_i$ son observaciones iid de una variable aleatoria de varianza $\sigma^2$, $\theta$ es un parámetro fijo, los $\tx_i$ son fijos y $f_i= \theta^\top\tx_i$ se refiere a la  \emph{parte determinista} del modelo. Además, consideremos una estimación del parámetro $\theta$ construida en base a un conjunto de entrenamiento $D=\{(\tx_i,y_i)\}_{i=1}^N$, denotada $\hat\theta=\hat\theta_D$. Con estas consideraciones, para un nuevo par $(\tx_\star,y_\star)$, podemos escribir el \emph{costo (cuadrático) esperado} asociado a la \textbf{predicción} $\hat f_\star = \hat\theta^\top \tx_\star$ mediante el \emph{trade-off} entre sesgo y la varianza \cite{ISLbook} dado por
\begin{equation}
 	\E{(y_\star - \hat f_\star)^2} = \text{Sesgo}(\hat f_\star)^2 + \text{Varianza}(\hat f_\star) + \sigma^2,\label{eq:expected_sq_loss}
 \end{equation} 
 donde el valor esperado es tomado con respecto a la ley de $\epsilon$, la única fuente de incertidumbre en este escenario, y 
 \begin{itemize}
 	\item $\text{Sesgo}(\hat f_\star) = \E(\hat f_\star) - f_\star$, es una medida de exactitud: ¿cuán buena (en valor esperado) es la estimación con respecto al valor real?
 	\item $\text{Varianza}(\hat f_\star)= \E(\E(\hat f_\star)- \hat f_\star)^2$, es una medida de precisión: ¿cuán disperso es el estimador?
 	\item $\sigma^2$ es la varianza de \emph{ruido} $\epsilon$ y es la parte irreducible del costo, en el sentido que no puede ser controlada por la elección de $\hat\theta$.
 \end{itemize}

Podemos evaluar el sesgo y la varianza para el estimador de mínimos cuadrados, denotado $\hat\theta_{MC}$, eligiendo $\hat f_\star = \hat\theta_{MC}^\top\tx_\star$. En efecto,  
\begin{align}
	\text{Sesgo}(\hat f_\star) &= \E(\theta_{MC}^\top\tx_\star) - \theta^\top\tx_\star=0\\
	\text{Varianza}(\hat f_\star) &= \sigma^2 \tx_\star^\top (\tX^\top\tX)^{-1}	\tx_\star
\end{align}
Es decir, el modelo de regresión lineal ajustado mediante MC reporta un estimador insesgado (sesgo nulo) pero con una varianza que depende de los datos en el conjunto de entrenamiento $D$, la varianza del ruido $\sigma^2$ y la propia entrada $\tx_\star$. Si bien no es posible determinar cuánto es esta varianza sin tomar supuestos estadísticos sobre los $\tx_i$'s, recordemos que la matriz $\tX^\top\tX$ puede ser cercana a singular cuando los datos son pocos, redundantes o colineales, lo cual resultará en alta varianza para la predicción $\hat f_\star$. De hecho, si asumiéramos que $\tx_i\sim\cN(0,1)$ iid, tendríamos que $\text{Varianza}(\hat f_\star) = \sigma^2 M/N$, es decir, la varianza es inversamente proporcional a la razón entre la dimensión de las entradas ($M$) y la cantidad de muestras ($N$).

\begin{mdframed}[style=discusion, frametitle={\center Evaluaciones \emph{dentro de muestra} y \emph{fuera de muestra}}]
 Notemos que la expresión en la ec.~\eqref{eq:expected_sq_loss} es una medida de error \emph{fuera de muestra}, pues evalúa un estimador $\hat\theta$, construido en base a un conjunto $D$, en un nuevo dato $(\tx_\star,y_\star)$ que no está originalmente contenido en $D$. No debemos confundir esta expresión con el error cuadrático medio, en la ec.~\eqref{eq:least_squares_cost}, el cual representa un error \emph{dentro de muestra}. La  evaluación de ambos tipos de  costos es clave para diseñar modelos y estimadores que puedan \emph{generalizar} a datos no vistos. 
\end{mdframed}

Al penalizar la norma cuadrada del parámetro $\theta$, la regresión de Ridge sacrifica la propiedad insesgada del estimador, pero en retorno construye un estimador que tiene menos varianza que el de MC. Esto puede entenderse como el balance entre: i) confiar únicamente en los datos, los cuales pueden ser pocos o muy ruidoso y consecuentemente insuficientes para determinar un modelo apropiado, y ii) introducir un \emph{sesgo} al modelo (por ejemplo, parámetros pequeños) con la finalidad de robustecer el modelo encontrado en función de los datos disponibles. Esta noción de (sobre-)ajustar a los datos de entrenamiento versus generalizar a  nuevos  puede ser ilustrada con el siguiente ejemplo: Consideremos $N=1000$ datos relacionados linealmente (pares de entrada y salida) donde la dimensión de entrada es $M=100$ generados por el siguiente script.
\begin{lstlisting}[language=Python]
##generacion de datos relacionados linealmente 
n_samples, n_features = 1000, 100
rng = np.random.RandomState(0)
X = rng.randn(n_samples, n_features)
theta = rng.randn(n_features,1)
y = X@theta + 10*rng.randn(n_samples, 1)
\end{lstlisting}
 En vez de utilizar todas las muestras de entrenamiento, utilicemos solo $N'=15$ muestras para entrenar usando los criterios de MC, y regresión de Ridge (RR) con $\rho\in\{40,80\}$. Repitiendo este proceso 400 veces, podemos analizar cómo se comportan los distintos métodos en cuanto a la magnitud de los parámetros encontrados, el error dentro de muestra (con respecto a los datos de entrenamiento) y el error fuera de muestra (con respecto a los datos no usados para entrenar). La Fig.~\ref{fig:MCvsRR_Synth} muestra dichos histogramas, desde donde podemos ver que a mayor $\rho$ (recordemos que MC es equivalente a RR con $\rho=0$), los parámetros encontrados tienen menor magnitud. Adicionalmente, notemos que el modelo no regularizado (MC) se comporta mejor en evaluación dentro de muestra, sin embargo, sus papeles se invierten cuando se trata de evaluación fuera de muestra: el incluir un sesgo en el ajuste de modelos (RR) puede ayudar a generalizar y no sobreajustar cuando se tienen pocos datos. 

 \begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/cap1_bias-variance.pdf}\\
	\caption{Mínimos cuadrados versus regresión de Ridge ($\rho\in\{40,80\}$): determinación de parámetros usando solo 15 muestras para un parámetro de dimensión $M=100$. De derecha a izquierda podemos ver magnitud de los parámetros encontrados, error dentro de muestra y error fuera de muestra. Experimento repetido 400 veces.}
	\label{fig:MCvsRR_Synth}  
\end{figure}



\subsubsection{Formulación con restricciones y selección de variables} % (fold)
\label{sub:restricciones}

Para ilustrar cómo el proceso de regularización es equivalente a restringir los parámetros a cumplir con una condición específica, e.g., tener una magnitud dada, consideremos el siguiente problema de regresión lineal con restricciones:
\begin{align}
	\min_\theta &\frac{1}{2}\sum_{i=1}^N (y_i-\theta^\top\tx_i)^2\label{eq:MC_restriccion}\\
	\text{s.a.} &\ ||\theta||_p^p = \tau,\nonumber
\end{align}
donde asumiremos que $\tau\geq0$ es una constante conocida. Sabemos que este problema puede ser resuelto en su forma dual mediante la formulación del Lagrangiano dado por 
\begin{equation}
	L = \frac{1}{2}\sum_{i=1}^N (y_i-\theta^\top\tx_i)^2 - \lambda (||\theta||_p^p - \tau),
\end{equation}
donde $\lambda\geq0$ es conocido como el \emph{multiplicador de Lagrange}. Luego, las condiciones de primer orden necesarias para encontrar la solución del problema con restricciones en la ec.~\eqref{eq:MC_restriccion} están dadas por: 
\begin{align}
	\frac{\partial L}{\partial \theta} = 0 &\quad\Rightarrow\quad  \frac{\partial }{\partial \theta}\left( \frac{1}{2}\sum_{i=1}^N (y_i-\theta^\top\tx_i)^2 - \lambda ||\theta||_p^p \right) = 0\label{eq:dual_MCR1}\\
	\frac{\partial L}{\partial \lambda} = 0 &\quad\Rightarrow\quad ||\theta||_p^p = \tau, \label{eq:dual_MCR2}
\end{align}
lo cual recupera la forma del problema de minimización de mínimos cuadrados regularizados. Enfatizamos esta relación en el siguiente recuadro. 


\begin{mdframed}[style=discusion, frametitle={\center Mínimos cuadrados regularizados: optimización con restricciones}]
Observemos que el problema de mínimos cuadrados regularizados, determinado por el costo en la ec.~\eqref{eq:reg_least_squares}, es equivalente al dual de un problema de optimización con restricciones en las ecs.~\eqref{eq:dual_MCR1}-\eqref{eq:dual_MCR2} para un $\lambda$ dado tal que $\lambda =-{\rho}/{p}$. Consecuentemente, como el valor óptimo de $\lambda$ depende del nivel de la restricción $\tau$, podemos aseverar que \textbf{para cualquier $\rho\geq0$, existe un $\tau\geq0$ tal que la minimización de \eqref{eq:reg_least_squares} es equivalente a la de \eqref{eq:MC_restriccion}.} Consecuentemente,  podemos interpretar el problema de MCR como el de MC sujeto a una restricción para (en este caso la norma) del parámetro. 
	
\end{mdframed}

Con la interpretación del problema de optimización con restricciones sobre la norma del parámetro $\theta$, podemos entender distintos regularizadores (distintos $p\geq0$) mediante sus curvas de nivel. La Fig.~\ref{fig:reg_lin_reg} ilustra las curvas correspondientes al costo cuadrático (izquierda) y al término de regularización en la eq.~\eqref{eq:reg_least_squares} para órdenes $p\in\{0.5,1,2\}$. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/cap1_regularizadores.pdf}\\
	\caption{Curvas de nivel del costo cuadrático para un problema hipotético con solución $\theta=[10,20]$ (izquierda) y términos de regularización para órdenes $p\in\{0.5,1,2\}$. Observe cómo las curvas de nivel atraen el mínimo hacia el origen de distinta forma: $p=2$ lleva la solución directamente al origen, mientras que $p\in\{0.5,1\}$ lleva la solución a los bordes, es  decir, privilegiando soluciones ralas. La solución $\theta=[10,20]$ se ha denotado con una cruz azul, recuerde que solo el costo cuadrático depende de este valor, no los términos de regularización. }
	\label{fig:reg_lin_reg}  
\end{figure}

La formulación en base a restricciones es clave para entender la propiedad de \emph{selección de características} de los MCR. Al estimar el parámetro $\theta$, estamos verificando cuán importante es cada (coordenada de la) entrada o en la jerga de reconocimiento de patrones, cada \emph{característica}. Indirectamente estamos también implícitamente descubriendo cuáles son las características que importan y cuales no, a esto nos referimos como selección de características. Distintas normas para el término de regularización, como las ilustradas en la Fig.~\ref{fig:reg_lin_reg}, inducen distintas propiedades para la solución del problema de MCR. En particular, RR atrae \emph{homogéneamente} el parámetro hacia el origen, lo cual resulta  en estimaciones de menor varianza como vimos en el apartado anterior. LASSO ($p=1$) y el caso $p\leq1$ en general presenta una propiedad adicional, donde la forma de la curva de nivel permite que usualmente la solución del problema se concentre el las puntas del \emph{diamante} (ver Fig.~\ref{fig:reg_lin_reg}, $p=1$), llevando algunas coordenadas del parámetro $\theta$ directamente a cero. Por esto decimos que LASSO (y $p\leq1$ en general) tiene la propiedad de selección de variables y entrega modelos ralos con respecto a MC tradicional. 

Para ilustrar la propiedad de selección de características, consideremos el \emph{Breast Cancer Wisconsin Data Set}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)}}, el cual tiene $N=569$ muestras y un dimensión de entrada de $M=30$. Los valores para la variable de salida ($y$) son solo dos, pues este es un problema de clasificación (\emph{cáncer} vs \emph{no-cáncer}), sin embargo, nosotros ajustaremos un modelo de regresión lineal usando MC, RR y LASSO para luego evaluar los pesos encontrados. Usaremos 2/3 de los datos para entrenar y el 1/3 restante para calcular puntajes fuera de muestra. La Fig.~\ref{fig:MC_RR_LASSO_breastcancer} presenta los parámetros encontrados para cada uno de los métodos, donde podemos ver la propiedad de selección de variables de LASSO; adicionalmente, la Tabla \ref{tab:breastMC_RR_LASSO} muestra los puntajes de cada método, tanto dentro como fuera de muestras: en la línea de la discusión anterior, los modelos regularizados presentan mejor generalización y usan menos parámetros (o más parámetros iguales a cero). 


\begin{table}
\centering
\caption{Puntajes de modelos de regresión implementados en \emph{Breast Cancer Wisconsin Data Set}, más alto es mejor. Observe la superioridad de los modelos regularizados para generalizar.}
	\label{tab:breastMC_RR_LASSO}
	\begin{tabular}{ r|c|c } 
		 & in-sample & out-of-sample \\
		\hline
		MC & \textbf{0.7896} & 0.6911 \\ 
		RR & 0.6905 & 0.6903 \\ 
		LASSO & 0.7452 & \textbf{0.7242}
	\end{tabular}
\end{table}



 



\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/cap1_OLS_RR_LASSO.pdf}\\
	\caption{Parámetros de la regresión lineal del \emph{Breast Cancer Dataset} usando MC, RR y LASSO. Observe cómo RR y LASSO disminuye críticamente la magnitud de los parámetros y, además, LASSO lleva parámetros directamente a cero, resultando en un modelo más simple (i.e, con menos parámetros).}
	\label{fig:MC_RR_LASSO_breastcancer}  
\end{figure}



\begin{mdframed}[style=discusion, frametitle={\center Regularización: Consideraciones generales}]
Para concluir esta sección, enunciamos las siguientes preguntas para discusión posterior\\
$\bullet$ \textbf{¿es justo comparar MC y MCR en términos del ECM?} Ciertamente no, el criterio de MC siempre reportará un menor ECM, pues ha sido entrenado para minimizar dicho costo. Las ventajas de MCR están en su desempeño fuera de muestra, selección de variables o en  general en su habilidad de  incorporar  sesgo  \emph{de diseñador} en la soluciones que no afloren naturalmente de  los  datos. \\
$\bullet$ \textbf{¿cómo elegir $\rho$?} De forma general, este \emph{hiperparámetro} determina el balance entre regularización (cuán  sesgado) y ajuste (cuán bien replica  los datos ), consecuentemente lo debemos elegir según nuestra intención. En la práctica,  podemos evaluar el desempeño de distintos valores de $\rho$ fuera de muestra con la finalidad de elegir un valor apropiado. Esta técnica se llama \emph{validación cruzada} y busca determinar modelos que no sufran de subajuste, o sobreajuste.\\
$\bullet$ Vimos que la norma $\ell_p$ con $0<p\leq1$ tiene la propiedad de selección de características, pero, \textbf{¿qué pasa con la `norma' $\ell_0$?}. La cantidad  $\ell_0(\theta)$ denota la cantidad de elementos no nulos de $\theta$ y no es una norma, sin embargo, puede de todas formas ser usada en la definición del costo en la ecu.~\eqref{eq:reg_least_squares}, con la finalidad de directamente penalizar la cantidad de características usadas por el modelo. Desafortunadamente, encontrar la solución usando la ``norma'' $l_0$ es muy difícil, sin embargo, bajo ciertas condiciones la consideración de la norma $l_1$ puede llevar a la misma solución.
\end{mdframed}

\subsection{Máxima verosimilitud} % (fold)
\label{ssub:max_ver}


En el apartado anterior vimos que el criterio de  mínimos cuadrados, y su variante regularizada, ofrecen una alternativa simple, elegante, interpretable y con solución en forma cerrada. Sin embargo, también vimos que dicho criterio sufre de desventajas en cuanto a su capacidad de ajustar modelos en casos generales, pues el criterio de MC es particularmente apropiado para variables contínuas, con perturbaciones aditivas  y simétricamente dispersas con respecto a una tendencia dada. Existen distintos casos donde el criterio de MC no es apropiado, por ejemplo aplicaciones financieras con  perturbaciones multiplicativas, mediciones de intensidad como frecuencia de aparición de palabras o sismos en donde las perturbaciones son con alta probabilidad solo positivas, y problemas de clasificación o asignación (\emph{clustering}) en donde las métricas de error toman la forma como ``correcto''/``incorrecto'', con lo que una medida de error que que reporte ajustes ``más incorrectos'' no tiene sentido. 

Una alternativa natural es considerar una métrica de desempeño distinta y diseñada específicamente en función  de cada aplicación con la finalidad de capturar asimetrías, no-estacionariedad, asignación correcta e invarianzas (en el problema de \emph{clustering} por ejemplo) entre otras propiedades. Sin embargo, esta no solo es una tarea tediosa y poco elegante---en el sentido que va en contra los objetivos de inteligencia artificial expuestos en el Capítulo~\ref{cap:intro}---sino que puede ser muchas veces impracticable, pues precisamente no conocemos cuáles son las propiedades de los datos antes de ajustar modelos. Además, usar distintas métricas dificulta la interpretación y comparación de los enfoques considerados. Consecuentemente, nos proponemos considerar un criterio global de ajuste de modelos, el que en cada caso particular \emph{colapse} a una forma explícita que sí es \emph{ad hoc} al problema/modelo en cuestión y permite comparar distintos enfoques de manera unificada. 	

El enfoque general para ajuste de modelos que consideraremos en esta sección, y continuaremos utilizando durante el resto del curso, será el \emph{criterio  de máxima verosimilitud}, dado un conjunto de datos de entrenamiento $D$. Este es un criterio general para una amplia gama de modelos que, tal como se mencionó en el párrafo anterior, toma una  forma específica en cada problema, aunque su solución no siempre es calculable de forma explícita. Este enfoque es radicalmente distinto al de MC y a cualquier otro criterio de ``ajuste'': con el criterio de MC buscamos un modelo \emph{aproximado} a los datos, donde sabemos que ningún modelo es el modelo correcto, tal que la discrepancia entre el modelo candidato ya los datos sea mínima. Por el contrario, en el criterio de \emph{máxima verosimilitud} nuestro objetivo es encontrar el modelo que---con mayor probabilidad---ha generado \emph{exactamente} los datos observados. Debido a la naturaleza aleatoria de los datos, para implementar este concepto es necesario considerar modelos probabilísticos, de forma de poder calcular la probabilidad de que los datos $D$ hayan sido generados por un modelo $m$, luego, elegiremos  el modelo que maximice dicha probabilidad. 

El criterio de máxima verosimilitud (MV) es aplicable a modelos probabilísticos para la generación de datos, a los cuales nos referiremos como \emph{modelo generativos}. Para el caso del problema de regresión, el modelo generativo es cualquiera que modele la variable de salida como una variable aleatoria $y$ a través de una distribución condicional (a la entrada $x$ y el parámetro $\theta$) de la forma 
\begin{equation}
	y|x,\theta \sim p(y|x,\theta),\label{eq:mod_gen}
\end{equation}
donde enfatizamos que $y$ es  la única variable aleatoria y tanto el parámetro $\theta$ como la entrada $x$ son cantidades fijas (la primera desconocida y la segunda conocida).

\begin{mdframed}[style=discusion, frametitle={\center Notación sobre variables aleatorias}]
 Estrictamente, en base a la notación estándar en probabilidades, la expresión  correcta para el lado izquierdo de la ec.~\eqref{eq:mod_gen} debiese ser  
 \begin{equation}
  	Y=y|x,\theta,
  \end{equation}
  pues $Y$ denota la variable aleatoria, e $y$ el valor que ésta toma. Sin embargo, seguiremos la usanza de la comunidad de Aprendizaje de Máquinas donde denotamos la tanto la variable aleatoria como su valor indistintamente con la letra minúscula, e.g., $y$. Seguiremos esta notación a menos que sea estrictamente necesario para evitar confusión. Además, en todos los casos asumiremos que las distribuciones de probabilidad consideradas tienen densidad con respecto a alguna medida---usualmente \emph{Lebesgue} (caso continuo) o \emph{cuenta-puntos} (caso discreto)---ambas denotadas indistintamente por $p(\cdot)$.  Finalmente, usualmente escribiremos 
  \begin{equation}
  	y|x \sim p(y|x),
  \end{equation}
  sin enunciar explícitamente la dependencia del parámetro $\theta$.
\end{mdframed}


En particular, en el caso de la regresión lineal podemos considerar el siguiente modelo  generativo:
\begin{equation}
	y = a^\top x + b + \epsilon,\quad \epsilon\sim\cN(0,\sigma_\epsilon^2),
	\label{eq:lin_gauss}
\end{equation}
el cual consta de una parte determinística (lineal en $x$) y una parte aleatoria caracterizada por la variable aleatoria $\epsilon$, la cual hemos  elegido gaussiana con media cero y varianza $\sigma_\epsilon^2$. El modelo probabilístico en la ec.~\eqref{eq:lin_gauss} puede expresarse mediante la siguiente densidad condicional 
\begin{equation}
	y|x \sim p(y|x,\theta) = \cN(y;a^\top x + b ,\sigma_\epsilon^2),\label{eq:mod_lin_gau}
\end{equation}
donde hemos denotados el vector de todos los parámetros del modelo mediante $\theta = [a,b,\sigma_\epsilon^2]$.

Si bien, lo siguiente no es necesario en el caso general, usualmente asumiremos que las realizaciones del modelo anterior, i.e., los datos $\{y_i\}_{i=1}^N$ generados a partir de la entrada $\{x_i\}_{i=1}^N$, son \textbf{condicionalmente independientes} dado el modelo. Esto significa que \emph{si conociésemos el modelo}, o  equivalentemente, si conociésemos $\theta$, y dos entradas $x_i,x_j$, entonces las  salidas correspondientes $y_i,y_j$ son independientes. Es importante clarificar que los valores generados por el modelo $\{y_i\}_{i=1}^N$ \textbf{no son independientes}. En efecto, si fuesen independientes no podríamos hacer predicciones: la predicción de una observación nueva $y_\star$ en base a una secuencia de observaciones $\{y_i\}_{i=1}^N$ estaría dada por\footnote{Hemos ignorado la dependencia de las variables independientes $\{x_i\}_{i=1}^N$, $x_\star$.}
\begin{equation}
	\blueb{[esto es falso]}\qquad p(y_\star|\{y_i\}_{i=1}^N) 
	\stackrel{\text{(prob. cond.)}}{=} \frac{p(y_\star,\{y_i\}_{i=1}^N)} {p(\{y_i\}_{i=1}^N)} 
	\stackrel{\text{(indep.)}}{=} \frac{p(y_\star),p(\{y_i\}_{i=1}^N)} {p(\{y_i\}_{i=1}^N)}
	=p(y_\star), 
\end{equation}
es decir, las observaciones pasadas no aportarían para la predicción.  Por  el contrario, como nuestro supuesto es de \textbf{independencia condicional} la expresión correcta es  la siguiente: 
\begin{equation}
	\blueb{[esto es verdadero]} \qquad p(y_\star|\{y_i\}_{i=1}^N,\theta)  
	=p(y_\star|,\theta), \qquad  \qquad\qquad  \qquad \qquad  \qquad\qquad  \qquad
\end{equation}
lo cual quiere decir que las  observaciones pasadas no son útiles para predecir el  futuro \textbf{solo si conozco el modelo}. Esto es evidente, pues si conozco el modelo, no necesito datos para saber de $y_\star$. 


El supuesto de independencia condicional está garantizado al imponer que las realizaciones de $\epsilon\sim\cN(0,\sigma_\epsilon^2)$ sean \emph{independientes e idénticamente distribuidas} (iid). Esto es  fundamental para poder aprender el modelo desde múltiples observaciones, pues intuitivamente todas las observaciones aportan evidencia no redundante sobre el parámetro en común $\theta$. Por el contrario, si las observaciones fuesen condicionalmente dependientes, entonces la información que reportan para estimar $\theta$ sería redundante. Igualmente, si no todas las observaciones siguiesen la misma distribución, entonces cada una tendría ``su propio $\theta$'' y solo tendríamos ``un  dato'' para estimar cada parámetro. Más adelante en el curso veremos casos donde los datos no son iid pero asumimos cierta regularidad en los modelos que permiten determinar sus parámetros.  

\subsubsection{Función de verosimilitud} % (fold)
\label{sssub:verosimilitud} 

\begin{definition}[Verosimilitud]
Consideremos un  modelo generativo definido mediante la densidad de  probabilidad  $y\sim p(y|\theta)$, donde el  parámetro $\theta\in\Theta$ y un conjunto de observaciones $\{y_i\}_{i=1}^N$ generado por dicho modelo. La función $L(\theta): \Theta \to \R$ definida como la probabilidad de los datos observados condicional al parámetro $\theta$, es decir, 
\begin{equation}
			\theta   \mapsto L(\theta) =  p(\{y_i\}_{i=1}^N | \theta),
\end{equation}
es conocida como \emph{verosimilitud} del modelo $p(y|\theta)$ o, equivalentemente, del  parámetro $\theta$. En algunos casos, consideraremos la  notación $L_\y(\theta)$ para enfatizar que la verosimilitud es tomada con respecto a las observaciones  $\y$.
\end{definition}

La definición anterior es elocuente: la función de verosimilitud precisamente cuantifica cuán verosímil es un modelo (o equivalentemente, parámetro) de haber generado las observaciones $\{y_i\}_{i=1}^N$. En este sentido, ante dos valores candidatos para el parámetro, por ejemplo $\theta_1$ y $\theta_2$, éstos pueden se evaluados mediante la comparación de $L(\theta_1)$ y $L(\theta_2)$. En efecto, si la razón $L(\theta_1)/L(\theta_2)$ es, por ejemplo, 3, entonces diremos que \emph{el valor de $\theta$ sea $\theta_1$ es 3 veces más verosímil a que sea $\theta_2$}. En este sentido, la función de verosimilitud  $L(\theta)$ representa una medida relativa de la \emph{bondad} de cada valor que el parámetro pueda tomar en función de los datos observados.

Es importante enfatizar que la función $L(\theta)$ \textbf{no es una densidad de probabilidad}. En efecto, podemos considerar la siguiente función en dos variables: $\theta$ y $\y=\{y_i\}_{i=1}^N$   
\begin{equation}
	L(\theta,\y) = p(\y|\theta),
\end{equation}
la cual toma distintos significados si  fijamos una de las variables: Si fijamos el valor del parámetro $\theta$, entonces, $L(\theta,\cdot) = p(\cdot|\theta)$ es una densidad de probabilidad, en particular, 
\begin{equation}
	\int_{\R^N}L(\cdot,\y)\td\y = \int_{\R^N}p(\y|\theta)\td\y = 1,
\end{equation}
lo que quiere decir que para ``cualquier $\theta$'', entonces, $p(\y|\theta)$ es un modelo válido. Por el contrario, si fijamos $\y$, entonces obtenemos la función de verosimilitud $L(\cdot,\y) = p(\y|\cdot)$, la  cual no necesariamente integra uno con respecto a $\theta$.



\begin{mdframed}[style=ejemplo, frametitle={\center Ejemplo: Verosimilitud para el modelo gaussiano  (muestras  independientes)}]

Consideremos un  modelo gaussiano definido por 
\begin{equation}
	y \sim p(y|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-(y-\mu)^2}{2\sigma^2}\right), \label{eq:ejemplo_gaussiano}
\end{equation}
y las observaciones $\y = \{y_i\}_{i=1}^N$ iid. La verosimilitud  de $\theta  =  [\mu,\sigma]$ está dada por 
\begin{align}
  	L(\theta)  =  p(\y|\mu,\sigma^2) 
  				&\stackrel{\text{(iid)}}{=}\prod_{i=1}^N p(y_i|\mu,\sigma^2) 
  				 = \frac{1}{(2\pi\sigma^2)^{N/2}}  \exp\left(\frac{-\sum_{i=1}^N(y_i-\mu)^2}{2\sigma^2}\right)\\
  				\text{[expandir $(\cdot)^2$]}\  & = \frac{1}{(2\pi\sigma^2)^{N/2}}  \exp\left(\frac{-N(\mu^2 - 2\mu\sum_{i=1}^Ny_i/N + \sum_{i=1}^Ny_i^2/N)}{2\sigma^2}\right)\nonumber\\
  				\text{[nikita nipone]}\  & = \frac{1}{(2\pi\sigma^2)^{N/2}}  \exp\left(\frac{-N(\mu^2 - 2\mu\sum_{i=1}^Ny_i/N + \sum_{i=1}^Ny_i^2/N \sred{ \pm (\sum_{i=1}^Ny_i/N)^2)}}{2\sigma^2}\right)\nonumber\\
  				\text{[formar $(\cdot)^2$]}\  & = \frac{1}{(2\pi\sigma^2)^{N/2}}  \exp\left(\frac{-(\mu - \sum_{i=1}^Ny_i/N)^2}{2\sigma^2/N}\right)
  				\exp\left(\frac{-(\sum_{i=1}^Ny_i^2/N  - (\sum_{i=1}^Ny_i/N)^2)}{2\sigma^2/N}\right)
  				\nonumber\\
  				\text{[notación]}\  & = \frac{1}{(2\pi\sigma^2)^{N/2}}  \exp\left(\frac{-(\mu - \bar y)^2}{2\sigma^2/N}\right)
  				\exp\left(\frac{-(\bar s  - \bar y^2)}{2\sigma^2/N}\right)
  				\nonumber,
  \end{align}  
  donde $\bar y = \tfrac{1}{N}\sum_{i=1}^Ny_i$ es el promedio de las observaciones y $\bar s = \sum_ {i=1}^Ny_i^2/N$ es el promedio de los cuadrados de las observaciones. Observemos que como funciones de $\mu$ y $\sigma^2$, la expresión anterior es respectivamente  proporcional a las densidades Normal (para $\mu$) y Gamma-inversa (para $\sigma^2$). Sin embargo, recordemos que esta  expresión no necesariamente integra uno y por  ende es solo coincidentemente proporcional a una pdf conocida. La Fig.~\ref{fig:gaussian_likelihood} muestra la  densidad normal ($\mu=0,\sigma=1$) y la verosimilitud para ambos parámetros con 20 y 200 muestras. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth, frame]{img/cap1_gaussian_likelihood}\\
	\caption{Densidad normal (izquierda, $\mu=0$ y $\sigma=1$) y verosimilitud para la media y la varianza en base a 20 (centro) y 200 (derecha) observaciones.}
	\label{fig:gaussian_likelihood}  
\end{figure}
  


\end{mdframed}

La verosimilitud es fundamental cuando realizamos \emph{inferencia}, es decir, cuando nuestro objetivo es descubrir o identificar los modelos y parámetros en base a las observaciones que dicho modelo ha generado; esto muchas veces se refiere coloquialmente como \emph{probabilidad inversa}. La importancia de la función de verosimilitud está documentado en el \textbf{principio de la verosimilitud}, el cual sentencia que toda la información relevante que la observación $\y = \{y_i\}_{i=1}^N$ puede aportar a la estimación del parámetro $\theta$, está contenido en la función  de verosimilitud $L(\theta)$. Una consecuencia directa de este principio es que si diseñamos dos experimentos para realizar  inferencia  sobre un parámetro desconocido $\theta$ y ambos  resultan en la misma función de verosimilitud  (salvo una constante de proporcionalidad), entonces,  ambos experimentos, y los datos adquiridos en  ellos, reportan la misma  información  sobre $\theta$. Lo de igualdad salvo una constante de proporcionalidad es porque recordemos que la verosimilitud es una medida \emph{relativa} de la bondad de (cada valor del) parámetro a inferir. 

La pregunta  natural entonces es ¿cómo usar la función $L_\y(\theta)$ para determinar ``el buen $\theta$''? Por supuesto el título de esta sección hace las veces de \emph{spoiler} para esta pregunta: simplemente elegir el máximo de la función, pues éste nos da el (o los) valor más \emph{verosímil} para $\theta$ relativo a todo el resto de las opciones disponibles. Sin embargo, veamos que el uso del argumento que maximiza $L_\y(\theta)$ tiene un significado  mucho más  acabado. Consideremos la siguiente forma de encontrar el parámetro $\theta$: recordemos que el  modelo real es $p(y|\theta)$ y definamos una discrepancia entre modelos, denotada $D(p_1,p_2)$, luego, encontraremos el $\hat\theta$ tal que $p(y|\hat\theta)$ es lo más \emph{cercano} posible al modelo real con respecto a la discrepancia $D(\cdot,\cdot)$, es decir, el que minimiza la expresión
\begin{equation}
    	D(p(y|\theta),p(y|\hat\theta)).
\end{equation}  
Este criterio es interesante, pues notemos que no hemos incorporado ningún supuesto sobre la parametrización de  los  modelos (cómo el  modelo depende de $\theta$) ni de la  geometría del espacio  $\Theta$; estamos comparando directamente los modelos y no los  valores específicos de los parámetros. Desafortunadamente, notemos que formular y resolver  este problema no es posible en el caso general, pues la expresión de arriba depende del parámetro real $\theta$, el cual no conocemos, con lo que no podríamos resolver dicho problema de optimización. 

Sin embargo, veamos que podemos considerar una métrica que ofrece una alternativa para optimizar la discrepancia entre el modelo real y el aproximado independiente de que no conozcamos el valor de $\theta$. Dicha métrica, la cual es motivada desde la teoría de la información, es un estándar para comparar distribuciones de probabilidad generales y es conocida como la divergencia de Kullback-Leibler $\KL(p,q) $. Evaluada entre el modelo real $p =  p(y|\theta)$ y el aproximado $q  = p(y|\hat\theta)$, esta divergencia toma la siguiente forma 
\begin{equation}
 	\KL( p(y|\theta),p(y|\hat\theta)) =  \int_y\log\left(\frac{p(y|\theta)}{p(y|\hat\theta)}\right)p(y|\theta)\td y.\label{eq:KL_maxlike}
 \end{equation} 
 En general, no es claro que podamos calcular dicha integral, sin embargo, observemos que ésta es una  esperanza con respecto a la densidad $p(y|\theta)$, por lo que podemos considerar su aproximación de Monte Carlo usando las $N$ observaciones en $D$, las cuales están precisamente generadas por la medida de la integral en la ec.~\eqref{eq:KL_maxlike} de acuerdo a 
\begin{equation}
	\KL( p(y|\theta),p(y|\hat\theta)) 	\approx \KL_N( p(y|\theta),p(y|\hat\theta)) = \frac{1}{N}\sum_{i=1}^N\log\left(\frac{p(y_i|\theta)}{p(y_i|\hat\theta)}\right).
\end{equation}
 Denotemos ahora $\hat\theta_N$ el minimizante de la  expresión anterior, el cual podemos calcular mediante (ignoramos la constante $N^{-1}$)
 \begin{align}
 	\hat\theta_N & =  \argmin_{\hat\theta}  \sum_{i=1}^N\log\left(\frac{p(y_i|\theta)}{p(y_i|\hat\theta)}\right)\\
 				&= \argmin_{\hat\theta}  \sum_{i=1}^N  \log p(y_i|\theta) - \sum_{i=1}^N \log p(y_i|\hat\theta)\nonumber\\
 				&= \argmax_{\hat\theta}  \sum_{i=1}^N \log p(y_i|\hat\theta)\nonumber\\
 				&= \argmax_{\hat\theta}  \prod_{i=1}^N p(y_i|\hat\theta),\nonumber
 \end{align}
 donde hemos usado las propiedades del logaritmo y eliminado términos que no dependen de $\hat\theta$. Observemos que si nuestra muestras son condicionalmente independientes, entonces la expresión anterior implica que $\hat\theta_N$ es también el maximizante de la función de verosimilitud:
 \begin{align}
 	\hat\theta_N &= \argmax_{\hat\theta}  \prod_{i=1}^N p(y_i|\hat\theta) = \argmax_{\hat\theta}  p(\y|\hat\theta) = \argmax_{\hat\theta}  L_\y(\theta),
 \end{align}
 al que nos  referiremos como \emph{estimador de máxima verosimilitud}. Finalmente, queda la pregunta de cómo se relacionan el estimador de máxima verosimilitud (MV) $\hat\theta_N$ con el estimador óptimo en el sentido KL, $\hat\theta$. Para esto, notemos que la aproximación de Monte Carlo de la KL en la ec.~\eqref{eq:KL_maxlike} converge puntualmente a la KL,  i.e., para cada $\theta\in\Theta$,  $\KL_N( p(y|\theta),p(y|\hat\theta))\to \KL( p(y|\theta),p(y|\hat\theta))$ por la ley de los grande números  cuando $N\to\infty$. Consecuentemente, podemos asumir que los minimizantes  de la secuencia de aproximaciones de Monte Carlo también convergen al minimizante de la KL. Esta es la razón por la cual consideramos el estimados de máxima verosimilitud: en el límite $N\to \infty$ el estimador de MV es el que reporta la mínima divergencia (KL) entre  el modelo real y el aproximado. Esta condición nos da un sentido de \emph{consistencia} del estimador de MV, donde por consistencia entendemos que mientras más datos observamos nuestra aproximación del modelo converge al mejor modelo posible (en la métrica KL). 


Retomemos el problema  de regresión lineal: la verosimilitud del modelo lineal gaussiano  definido en la ec.~\eqref{eq:mod_lin_gau} (con parámetro $\theta  = [a,b,\sigma_\epsilon^2]$) está dada por (recordemos  que  los datos son condicionalmente independientes)
\begin{equation}
	L_\y(\theta) =  \prod_{i=1}^N \cN(y_i;a^\top x_i + b,\sigma_\epsilon^2). \label{eq:verosimilitud_lineal}
\end{equation} 
Usualmente, consideraremos el logaritmo de la verosimilitud, referido como \emph{log-verosimilitud}, $l(\theta) = \log L(\theta)$, por su facilidad de interpretación y optimización. Recordemos que esta transformación es válida, pues la función logaritmo es monótona y consecuentemente el (los) mínimo(s) no cambia(n).  La log-verosimilitud del modelo lineal y gaussiano está dada por
\begin{equation}
	l(\theta) 
		= \underbrace{-N\log \sqrt{2\pi\sigma^2_\epsilon}}_{\text{dispersión}} + \underbrace{\frac{-1}{2\sigma_\epsilon^2} \sum_{i=1}^N (y_i-a^\top x_i - b)^2}_{\text{ajuste}},
\end{equation}
donde podemos de inmediato reconocer que la maximización de $l(\theta)$ implica el balance entre dos términos. El de la izquierda es una medida de dispersión o complejidad, pues para aumentar éste termino necesitamos que la varianza sea pequeña o el modelo tenga errores poco dispersos. El término de la derecha, por otro lado, es una medida de ajuste, para aumentar éste término necesitamos que el modelo  represente bien, muestra a muestra, nuestros datos. 

En particular, el estimador  de máxima verosimilitud para los parámetros de  la parte lineal (i.e., ignorando $\sigma^2_\epsilon$) está dado por:
\begin{align}
	[a^{\text{MV}} ,b^{\text{MV}} ]
						&= \argmin_{a,b} \sum_{i=1}^N (y_i-a^\top x_i - b)^2. \label{eq:theta_ML}
\end{align}
Para  nuestra  sorpresa, observemos que es posible identificar esta última expresión con la del costo cuadrático en la ec.\eqref{eq:lin_least_squares2}, es decir, el estimador de máxima verosimilitud es el minimizante del mismo costo que el estimador de mínimos cuadrados. Consecuentemente, ambos estimadores son iguales y de acuerdo a la ecuación \eqref{eq:sol_mse} dados por 
\begin{equation}
	[a^{\text{MV}} ,b^{\text{MV}} ]=[a^{\text{MC}} ,b^{\text{MC}} ] = \left(\tX^\top\tx\right)^{-1} \tX^\top Y.
\end{equation}

Además, recordemos que luego de determinar el estimador con criterio de MC, es posible calcular la varianza de los errores de nuestro modelo mediante 
\begin{equation}
	\text{Varianza} = \frac{1}{N}\sum_{i=1}^N (y_i-a^\top x_i -b)^2,
\end{equation}
dicha cantidad es precisamente la suma de cuadrados e intuitivamente representa la bondad de ajuste del modelo considerado. 

En el contexto de máxima verosimilitud, recordemos que la varianza es un parámetro del modelo y no una cantidad asociada al modelo que calculamos de forma independiente. Este parámetro puede ser calculado maximizando la log-verosimilitud, tal como se hizo para la media en la ecuación \eqref{eq:theta_ML}, de acuerdo a
\begin{align}
	\sigma^2_{\text{MV}} &= \argmax \frac{N}{2} \log(\sigma_\epsilon^{2}) + \frac{1}{2\sigma_\epsilon^2}\sum_{i=1}^N {(y_i-a^\top x_i -b)^2}. \label{eq:sigma_ML}
\end{align}
Usando la condición de primer orden en esta expresión, tenemos que
\begin{align}
	\frac{N}{2\sigma^2_{\text{MV}}} - \frac{1}{2\sigma^4_{\text{MV}}}\sum_{i=1}^N {(y_i-a^\top x_i -b)^2} = 0 \Rightarrow \sigma^2_{\text{MV}} = \frac{1}{N}\sum_{i=1}^N {(y_i-a^\top x_i -b)^2}.
\end{align}
Con lo cual se obtiene, sin sorpresa alguna, la misma expresión de la varianza que al usar mínimos cuadrados. ¡Observemos que el estimador de MV de la varianza depende de los parámetros $a$ y $b$, pero no al revés!

En la práctica, consideraremos la minimización de la log-verosimilitud negativa (en vez de la maximización de la log-verosimilitud) en línea con la literatura y software dedicados a la minimización de funciones. 

\subsection{Regresión via inferencia bayesiana} 
\label{sub:inferencia_bayes}

En el apartado anterior vimos el ajuste de modelos, estimación de parámetros o \emph{inferencia}, mediante la maximización de la función de verosimilitud. Es decir, en base a un conjunto de observaciones $\y$ asignamos el valor más \emph{verosímil} al parámetro desconocido $\theta$, dado por $\hat\theta^\star = \arg\max  L_\y(\hat\theta)$. Es claro por qué esta estimación puntual tiene sentido, desde el punto de vista de la probabilidad de los datos y de la mínima discrepancia contra el modelo real en base a la  divergencia de Kullback-Leibler. Sin embargo, solo tomar el máximo ignora el resto de la información contenida en la función de verosimilitud, por ejemplo, si consideramos funciones de verosimilitud distintas, por ejemplo, bimodales, con colas pesadas/livianas, asimétricas, discretas, todas esa propiedades no se reflejan en la estimación del parámetro $\theta$ si  estas verosimilitudes coinciden en el máximo. Esta es una limitación fundamental no solo del método de máxima verosimilitud, sino que de la estimación puntual en general. 

Otra limitación del paradigma de máxima verosimilitud es que no da la posibilidad de incorporar conocimiento experto, es decir, introducir sesgos necesarios para la inferencia. Por ejemplo, si sabemos que el parámetro desconocido tiene magnitud pequeña, o está lejos del origen, o es ralo, etc. Esta propiedad es necesaria en el análisis moderno de datos, pues en la ciencia de datos los esfuerzos son colaborativos y el conocimiento experto sin duda ayuda a resolver problemas de forma eficiente y con interpretabilidad. 

El paradigma bayesiano busca conciliar estas dos desventajas del uso de máxima verosimilitud interpretando el parámetro $\theta$ como variable aleatoria, donde la disponibilidad de datos se interpreta como un evento que aporta evidencia sobre el valor de $\theta$. Consecuentemente, el proceso de inferencia ahora se centra en encontrar la distribución condicional  $p(\theta|\text{datos})$.


\subsubsection{¿Qué es ser bayesiano?}
\label{ssub:que_es_bayes}

Thomas Bayes (c.~1701-1761) fue un matemático, filósofo y pastor presbiteriano inglés interesado en el cálculo y uso de las probabilidades. En ese entonces, no existía la diferencia entre probabilidad descriptiva (que caracteriza la generación de datos dado un modelo) e inferencial (que nos permite identificar un modelo dado un conjunto de observaciones). Como vimos en la clase pasada, el uso de probabilidades para estimar (o \emph{inferir}) parámetros usando los propios datos recibía el nombre de \emph{probabilidad inversa}. Tanto Bayes, como el matemático francés Pierre Simon Laplace, estaba al tanto de la siguiente relación para el problema de inferencia, la cual hoy conocemos como el \emph{Teorema de Bayes}:
\begin{equation}
	p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)} \propto p(x|\theta)p(\theta),\label{eq:Bayes}
\end{equation}
donde $x\in\cX$ es el valor de una observación y $\theta\in\Theta$ es un parámetro. En la expresión anterior podemos identificar las siguientes cantidades:
\begin{itemize}
	\item el prior o distribución a priori: $p(\theta)$,
	\item la verosimilitud: $p(x|\theta)$,
	\item la distribución posterior: $p(\theta|x)$,
	\item la densidad marginal de $x$: $p(x) = \int_\Theta p(x|\theta)p(\theta)\d\theta$.
\end{itemize}
Recordemos que en el problema de inferencia, los datos (en este caso $x$) son conocidos y fijos, mientras que el parámetro es variable (en realidad desconocido). Por esta razón, podemos escribir el lado derecho de la ec.~\eqref{eq:Bayes}, pues nos importa la distribución posterior como función de $\theta$ únicamente, tal como fue el caso de la función de verosimilitud en el apartado anterior. En algunas aplicaciones, conocer la versión proporcional de la posterior dada por $p(x|\theta)p(\theta)$ es suficiente para realizar distintos análisis, por ejemplo, cuando usamos Markov chain Monte Carlo.

Bajo cualquier punto de vista de inferencia estadística, e.g., bayesiano u otro, el espacio que contiene los valores de $x$ debe tener suficiente \emph{estructura} para definir probabilidades (o, equivalentemente, modelos); en particular aquellos de la forma $p(x)$ y $p(x|\theta)$. Adicionalmente, una característica particular del enfoque bayesiano es que asume que el espacio donde donde se encuentra el parámetro $\theta$, denotado $\Theta$, también debe ser un espacio de probabilidad. El enfoque frecuentista, por el contrario, tiene un concepto de probabilidad totalmente distinto que resulta en la consideración de $\theta$ como un elemento fijo, como un índice o hipótesis cuyo valor queremos descubrir.

La inferencia bayesiana se sustenta en la noción de probabilidad como  medida de incertidumbre, es decir, en una perspectiva subjetiva del conocimiento disponible sobre la generación del valor $x$. En este sentido, es posible identificar dos tipos de incertidumbre, la primera es la incertidumbre \textbf{aleatoria} y dice relación con la variabilidad con la que el sistema/modelo en cuestión genera el dato $x$. El segundo tipo es la llamada incertidumbre \textbf{epistemológica} y representa nuestra inhabilidad de conocer el modelo que genera los datos. Consecuentemente, el enfoque bayesiano hace la distinción entre ambos tipos de incertidumbre mediante los siguiente elementos:
\begin{itemize}
	\item La verosimilitud $p(x|\theta)$: que modela la aleatoriedad del modelo, el cual produce datos de forma aleatoria incluso cuando el modelo es perfectamente conocido. Este tipo de incertidumbre no puede ser reducida observando datos. \textbf{Ejemplo:} En una urna con bolitas rojas y negras donde la probabilidad de elegir una bolita negra es $\theta$, incluso si $\theta$ fuese conocido, la incertidumbre aleatoria del modelo resulta en la imposibilidad de predecir el color de la próxima bolita.   
	\item La distribución a priori $p(\theta)$: que encapsula la incertidumbre epistemológica (lo que no sabemos) sobre el sistema, la cual pude ser reducida observando datos y calculando $p(\theta|x)$ mediante el Teorema de Bayes. Distintas distribuciones priori llevarán a distintas distribuciones a posteriori, lo cual establece la subjetividad del enfoque bayesiano.  \textbf{Ejemplo:} En la misma urna anterior, el no conocer $\theta$ es un ejemplo de incertidumbre epistemológica.
\end{itemize}

Una discusión trascendental desde los inicios del enfoque bayesiano apuntaba a  cómo elegir la distribución a  priori $p(\theta)$ para ser lo más objetivo posible, es decir, cómo no introducir sesgos en la inferencia (cálculo de la posterior) heredados de una elección subjetiva del prior---esta era el objetivo de los llamados \emph{bayesianos objetivistas}. La postura de Laplace con respecto de esta disyuntiva fue elegir simplemente un prior \emph{no-informativo}, i.e., uno que no asigne más probabilidad a ningún valor de $\theta$ en particular, para lo cual se puede elegir un prior uniforme $p(\theta)\propto 1$. Sin embargo, posteriormente se descubrió que esta elección introduce sesgo en la inferencia de todas formas.\footnote{Esto es consecuencia de que el prior uniforme no es invariante bajo transformaciones uno-a-uno de $\theta$, consecuentemente, si el modelo es re-parametizado de forma no lineal y se mantiene el prior uniforme, entonces la posterior cambia.} Hacia fines del sigo XIX comenzó a surgir una demanda por un tratamiento i) objetivo de la inferencia que no dependiese de la elección del prior, algo que los \emph{bayesianos objetivistas} no habían garantizado, y ii) riguroso y formal en el sentido de que fuese consistente con la teoría matemática. En este sentido, durante el siglo XX la inferencia estadística vio avances en la dirección de prescindir del uso del prior para evitar inferencias sesgadas, en particular, el foco estuvo en la maximización directa de la verosimilitud con respecto al \emph{índice} $\theta$, tal como vimos en la clase pasada. De forma más general, y a través de Fisher, Neyman y Pearson, fue posible construir un tratamiento formal de la inferencia estadística sobre la bases de una interpretación frecuentista de la probabilidad, esto es, definir la probabilidad de un evento como el límite de la frecuencia de \emph{casos favorables divido por casos totales} cuando el número de realizaciones de un experimento tiende a infinito. Entonces, al considerar los parámetros como cantidades fijas y desconocidas, y al mismo tiempo, concebir la probabilidad como una medida de frecuencias y no de incertidumbre, el punto de vista frecuentista no busca asociar probabilidades a los parámetros desconocidos.

En la segunda mitad del siglo XX hubo un resurgimiento de la inferencia Bayesiana, en parte por el descontento de algunos estadísticos (y científicos en general) con las métricas  frecuentistas estándar como los errores de Tipo-I \& Tipo-II, y los los llamados $p$-valores. Esto permitió utilizar la máquinaria desarrollada por la perspectiva frecuentista para formalizar el enfoque bayesiano, en particular, esta combinación permitió construir priors no informativos invariantes bajo transformaciones una-a-uno (e.g., el prior de Jeffreys), como también definir el concepto de \emph{consistencia}, i.e., si un estimador converge a la respuesta correcta y a qué velocidad ocurre esto. Esta unión entre los conceptos bayesianos y frecuentistas resulta en lo mejor de dos mundos: hace posible construir un modelo que es subjetivo, pues en la práctica queremos incorporar conocimiento experto en los problemas que estemos estudiando, pero al mismo tiempo tenemos una herramienta objetiva (pues el enfoque frecuentista es \emph{automático} en el sentido que no requiere conocimiento experto) para asegurar que nuestros métodos son rigurosos desde una perspectiva matemática. 

\begin{mdframed}[style=discusion, frametitle={\center ¿Qué hizo efectivamente Bayes y por qué?}]

Thomas Bayes fue uno de los primeros \emph{inconformistas anglicanos}.\footnote{ Este término se usa para denotar a los protestantes que no estaban de acuerdo con los métodos y la gobernanza eclesiástica establecida por la ``Iglesia de Inglaterra'' (The Church of England).} Su trabajo de 1763, titulado 
\begin{center}
\it
An Essay Towards Solving a Problem in the Doctrine of Chances
\end{center}
esbozó por primera vez el resultado que hoy conocemos como el Teorema de Bayes. Este trabajo fue terminado por el amigo y colega de Bayes, Richard Price (1723 – 1791), el cual  envió el artículo a la prestigiosa revista inglesa \emph{Philosophical Transactions of the Royal Society} (PTRS), donde fue publicado póstumamente. Este artículo consideraba el caso de \emph{invertir la distribución binomial}: en un experimento donde el resultado puede ser éxito  (con probabilidad $p$) o fracaso (con probabilidad $1-p$), el objetivo es encontrar la distribución del parámetro $p$ dado que se han observado $q$ éxitos luego de $n$ experimentos. Para resolver este caso particular, Bayes descubrió que efectivamente la distribución posterior es proporcional a la verosimilitud. Sin embargo, a pesar de que Price completara el trabajo de Bayes  luego de la repentina muerte de éste último, este trascendental descubrimiento quedó momentáneamente en el olvido. Fue finalmente Laplace quien en 1774 publicó la relación entre las distribuciones  prior y posterior como la conocemos hoy, sin dejar de reconocer el trabajo de Bayes. En efecto, en su \emph{Essai Philosophique dur les Probabilites} (1814), Laplace menciona que Bayes ya había llegado al mismo resultado de forma ``refinada y muy ingeniosa, aunque un poco confusa''. 


Existen dudas sobre la autoría de Bayes del artículo en cuestión y cuánto de éste efectivamente venía de las propias notas no publicadas de Bayes, y cuánto de la edición y cálculos de R. Price \cite{bellhouse_2004}. En efecto, al año siguiente del artículo original, Price publicó en 1764 otro artículo en PTRS llamado \emph{A Demonstration of the Second Rule in the Essay toward the Solution of a Problem in the Doctrine of Chances}, esta vez de su propia autoría. Tanto Bayes como Price estaban muy lejos de ser  matemáticos prolíficos de su época, de hecho, las publicaciones de ambos no están particularmente ligadas a las matemáticas sino que son de índole religiosa---por mucho que sus contribuciones hayan sido suficientemente  conceptuales y generales para ser consideradas como avances en ambos campos. Una posibilidad para entender las motivaciones de Bayes (y Price) para el desarrollo de los artículos mencionados, pueden encontrarse en el artículo \cite{stigler2013} el cual postula que el verdadero título del artículo de Bayes aparentemente es: 
\begin{center}
\it
A Method of Calculating the Exact Probability of All Conclusions founded on Induction.
\end{center}
Este título, según explica \cite{stigler2013}, pudo haber sido perdido en la edición del artículo o probablemente el propio editor de PTRS lo modificó por encontrarlo un tanto \emph{atrevido}.  Si este fuese efectivamente el título, y no el menos informativo título original, se entiende que el artículo tiene por objetivo caracterizar  el problema más general de \emph{inducción}. Esto tiene un sentido particular dado el contexto filosófico y religioso en ese entonces, pues pocos años antes la obra de Hume `Of Miracles' (1748) presentaba un argumento probabilístico para desestimar los milagros (como la resurrección). En  pocas  palabras, Hume postulaba que la considerable \emph{improbabilidad} de un milagro sobrepasaba ampliamente la \emph{probabilidad}  de  que el milagro fuese incorrectamente  documentado. Este ensayo de Hume fue ampliamente leído, discutido y---evidentemente---atacado, en particular, tanto Bayes como Price, ambos inconformistas anglicanos, consideraron  el ensayo de Hume como una agresión. Consecuentemente, Bayes consideró responder al argumento de  Hume mediante la aplicación de probabilidades al problema de inducción, lo cual es confirmado por las notas no publicadas de Bayes con fecha anterior al 31 de diciembre de 1749 \cite{bellhouse_2004}, las que eventualmente llegaron al famoso artículo de 1763. Sin embargo, el trabajo de Bayes no estaba completo, en particular, no contaba una satisfactoria aproximación de la distribución posterior en el ejemplo de la inversión de la distribución binomial mencionado anteriormente. En este sentido, la motivación de Price para terminar este trabajo no era únicamente concluir el trabajo póstumo de su amigo, sino que también desarrollar un respuesta eficaz contra el argumento de Hume. En efecto, luego de una serie de  trabajos relacionados, fue  finalmente en 1767 que Price logró publicar su disertación \emph{On the Importance of Christianity, its Evidences, and the Objections which have been made to it}, donde hace una crítica directa a “Of miracles”. Básicamente, Price sentencia que Hume habría subestimado el impacto que una (gran) cantidad de observadores \textbf{independientes} reportando la existencia del milagro puede tener. En dicho caso, el Teorema de Bayes mostraba que la multiplicación de evidencia, la cual puede ser falible de forma individual, podría sobrepasar la improbabilidad de un evento (el milagro) y establecerlo como verdad (o en realidad muy probable). 

Existen varias razones atribuibles a que el trabajo de Bayes haya sido ignorado parcialmente hasta Laplace, e.g., la incompletitud del trabajo del propio Bayes, el rol incierto que tuvo Price en el desarrollo de éste, y el nombre poco elocuente del artículo que probablemente desvió atención al trabajo. Todo esto nos hace cuestionarnos si es correcto referimos a este trascendental Teorema en honor al Rev.~Thomas Bayes, el cual lanzó el primer atisbo de este resultado y no a Laplace, el cual fue el primero en enunciar la forma general para la relación entre prior, posterior y modelo en la forma que hoy conocemos y usamos. 
\end{mdframed}

\subsubsection{Elección de prior: conjugación}
\label{ssub:reg_lin_bayes}

Desde ahora, nuestra misión calcular y analizar la distribución posterior del parámetro $\theta$ dado un conjunto de datos $D$, nos referimos a esto como \emph{inferencia bayesiana} a diferencia de las más general \emph{inferencia estadística} en la cual a veces ses consideran solo estimaciones puntuales de $\theta$. 

La distribución posterior está dada, mediante el teorema de Bayes, por
\begin{equation}
	p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}
	\label{eq:posterior}
\end{equation}
donde $p(D|\theta)$ es la verosimilitud o el modelo que elegimos y $p(\theta)$ es la \emph{distribución a priori} del parámetro y encapsula todos nuestros supuestos, creencias y sesgo sobre el espacio de parámetros (modelos) a considerar. Finalmente, recordemos que en el denominador encontramos la  \emph{distribución marginal de los datos} $p(D)$ que actúa como constante de normalización para el numerador, pues el numerador puede ser considerado como una densidad pero no de probabilidad. Esta constante de normalización puede ser calculada mediante el uso de la ley de probabilidades totales, o bien imponiendo la restricción de que la expresión en la ecuación \eqref{eq:posterior} debe integrar uno:
\begin{equation}
	p(D) = \int p(D|\theta)p(\theta)d\theta.
\end{equation}
En base a la forma explícita de $p(D|\theta)p(\theta)$, calcular esta integral puede ser un desafío considerable. Sin embargo, enfatizamos que como esta cantidad no depende del parámetro $\theta$, no es necesario conocerla para explorar o aproximar la (forma de la) distribución posterior $p(\theta|D)$. 

Entonces, con el modelo $p(D|\theta)$ acordado, solo nos queda elegir la distribución a priori, los cual es guiado por dos objetivos. En primer lugar, debemos encapsular lo que efectivamente que sabemos del parámetro $\theta$, por ejemplo, en el caso de regresión lineal, podemos tener evidencia que los datos que observamos representan una cantidad que creciente en el tiempo, en cuyo caso, sabemos que $\theta\geq 0$. El segundo objetivo es obtener una forma \emph{amigable} de la distribución posterior, en el sentido que ésta sea un distribución con propiedades que deseemos, en particular, que la podamos calcular, evaluar, y samplear de ella. Una práctica usual en este sentido es elegir un prior $p(\theta)$ tal que la distribución posterior $p(D|\theta)$ están en la misma familia. 
\begin{definition}
	Diremos que el prior $p(\theta)$ es \emph{conjugado} a la verosimilitud $p(D)$, cuando la posterior $p(\theta|D)$ está en la misma familia, es decir, tienen la misma distribución con parámetros distintos.   
\end{definition}
El uso de un prior arbitrario resulta en que la posterior tenga una forma arbitraria también, con lo que incluso si tanto el prior como la verosimilitud tienen formas \emph{conocidas}, no tenemos ninguna garantía de que el posterior también la tenga y consecuentemente sea difícil de interpretar. Por el contrario, si usamos priors conjugados la distribución posterior pertenece a \emph{en la misma familia} del prior cuando vamos observando más datos, lo cual tiene un significado relevante: la actualización de prior a posterior mediante la incorporación de datos---conocida como actualización bayesiana---es simplemente un cambio de parámetros, lo cual ofrece una clara interpretación (en el caso que los parámetros tengan significado como media, varianza, o alguna taza), y la nueva distribución ocupan la misma cantidad de memoria que el prior (pues la cantidad de parámetros no cambia). A continuación veremos dos ejemplos de priors conjugados y cómo se interpreta la variación que sufren los parámetros de la posterior al incorporar datos.

\textbf{Modelo gaussiano.} Consideremos un conjunto de observaciones\footnote{Observe que en esta sección no estamos solo enfocados en el problema de regresión, sino que cualquiera que requiera inferencia paramétrica bayesiana.} $\datos=\{x_i\}_{i=1}^n\subset\R$ generadas independiente e idénticamente distribuidas (iid) por el modelo $\cN(\mu,\sigma^2)$. Recordemos que la verosimilitud de la media y varianza respectivamente está dada por 
\begin{equation}
	L_\datos(\mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right).
 \end{equation}

A continuación veremos priors conjugados para esta verosimilitud de forma incremental: primero cuando sola la media $\mu$ es desconocida, luego cuando solo la varianza $\sigma^2$ es desconocida y finalmente cuando ambos parámetros son desconocidos.  

\textbf{Modelo gaussiano ($\sigma^2$ conocido).} Consideremos el prior sobre la media $p(\mu) = \cN(\mu_0,\sigma_0^2)$, con lo que la posterior está dada por  
 \begin{align}
 	p(\mu|\datos) &\propto \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right) \frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\right)\label{eq:post_normal_mu_1}\\
 	&\propto \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\right),\label{eq:post_normal_mu_2}
 \end{align} 
 donde la proporcionalidad viene de ignorar la constante $p(\datos)$ en la primera línea e ignorar todas las contantes que no dependen de $\mu$ en la segunda línea; recordemos que estas constantes para $\mu$ incluyen a la varianza de $x$, $\sigma^2$, por lo que ignorar esta cantidad es solo posible debido a que estamos considerando el caso en que $\sigma^2$ es conocido. Completando el la forma cuadrática para $\mu$ dentro de la exponencial en la ec.~\eqref{eq:post_normal_mu_2}, obtenemos
 \begin{equation}
 	p(\mu|\datos) \propto \exp\left(-\frac{1}{2\sigma_n^2}(\mu - \mu_n)^2\right),\label{eq:post_normal_mu_3}
 \end{equation} 
 donde (ya definiremos $\mu_n$ y $\sigma_n^2$ en breve) como $p(\mu|\datos)$ debe integrar uno, la única densidad de probabilidad proporcional al lado derecho de la ecuación anterior es la Gaussiana de media $\mu_n$ y varianza $\sigma_n^2$. Es decir, la constante de proporcionalidad necesaria para la igualdad en la expresión anterior es $\int_\R\exp\left(-\frac{1}{2\sigma_n^2}(\mu - \mu_n)^2\right)\d\mu = (2\pi\sigma_n^2)^{n/2}$. Consecuentemente, confirmamos que el prior elegido era efectivamente conjugado con la verosimilitud gaussiana y la posterior está dada por la siguiente gaussiana:
  \begin{equation}
 	p(\mu|\datos) = \cN(\mu;\mu_n,\sigma_n^2) = \frac{1}{(2\pi\sigma_n^2)^{N/2}}\exp\left(-\frac{1}{2\sigma_n^2}(\mu - \mu_n)^2\right),\label{eq:post_normal_mu_4}
 \end{equation} 
 donde la media y la varianza están dadas respectivamente  por 
 \begin{align}
 	\mu_n &= \frac{1}{\tfrac{1}{\sigma_0^2} + \tfrac{n}{\sigma^2}} \left(\frac{1}{\sigma_0^2}\mu_0 + \frac{n}{\sigma^2}\bar{x} \right), \quad \text{donde } \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\label{eq:post_Gm}\\
 	\sigma_n &= \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1}.\label{eq:post_Gv}
 \end{align}

\begin{remark}
	La actualización bayesiana transforma los parámetros del prior de  $\mu$ desde  $\mu_0$ y $\sigma_0^2$ hacia $\mu_n$ y $\sigma_n^2$ en las ecs.~\eqref{eq:post_Gm} y \eqref{eq:post_Gv} respectivamente. Notemos que los  parámetros de la posterior son combinaciones (interpretables por lo demás) entre los parámetros del prior y los datos, en efecto, la $\mu_n$ es el promedio ponderado entre  $\mu_0$ (que es nuestro candidato para $\mu$ antes de ver datos) con factor $\sigma_0^{-2}$ y el promedio de los datos $\bar{x}$ con factor $(\sigma^{2}/n)^{-1}$, que a su vez es el estimador de máxima verosimilitud. Es importante también notar que  estos  factores son las varianzas inversas---i.e., precisión---de $\mu_0$ y de $\bar{x}$. Finalmente, observemos que $\sigma_n$ es la \emph{suma paralela} de las varianzas, pues  si expresamos la ec.~\eqref{eq:post_Gv} en términos de \emph{precisiones}, vemos que la precisión inicial $\sigma_0^2$ aumenta un término $\sigma^2$ con cada dato que vemos; lo cual tiene sentido pues con más información es la precisión la que debe aumentar y no la incertidumbre (en este caso representada por la varianza).
\end{remark}

\textbf{Modelo gaussiano ($\mu$ conocido).} Ahora procedemos con el siguiente prior para la varianza, llamado Gamma-inverso:
 \begin{equation}
 	p(\sigma^2)= \text{inv-}\Gamma(\sigma^2;\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha) (\sigma^2)^{\alpha+1}}\exp(-\beta/\sigma^2)
 \end{equation}
 esta densidad recibe dicho nombre pues es equivalente a modelar la precisión, definida como el recíproco de la varianza $1/\sigma^2$, mediante la distribución Gamma. Los hiperparámetros $\alpha$ y $\beta$ son conocidos como parámetros de forma y de tasa (o precisión) respectivamente. 

 Con este prior, la posterior de la varianza toma la forma:
 \begin{align}
 	p(\sigma^2|\datos) &\propto \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right) \frac{\beta^\alpha}{\Gamma(\alpha) (\sigma^2)^{\alpha+1}}\exp(-\beta/\sigma^2)\\
 	&\propto  \frac{1}{(\sigma^2)^{N/2+\alpha+1}}\exp\left(-\frac{1}{\sigma^2}\left(\frac{1}{2}\sum_{i=1}^N(x_i-\mu)^2 +\beta\right) \right)\nonumber
 \end{align} 
 donde nuevamente la proporcionalidad ha sido mantenida debido a la remoción de las constantes. Esta última expresión es proporcional a una distribución Gamma inversa con hiperparámetros $\alpha$ y $\beta$ ajustados en base a los datos observados. 

\textbf{Modelo gaussiano ($\mu$ y $\sigma^2$ desconocidos).} Tarea. 

\begin{mdframed}[style=ejemplo, frametitle={\center Ejemplo: Distribución posterior modelo lineal y gaussiano}]
Recordemos que, para observaciones $\datos=\{(x_i,y_i)\}_{i=1}^n$, el modelo de regresión lineal puede ser escrito en forma vectorial mediante
\begin{equation}
 	Y = \tX\theta + \bepsilon,
 \end{equation}
 donde $[\tX]_{i:} = [x_i^\top,1],\ [Y]_i = y_i,\ [\bepsilon]_i = \epsilon_i\sim\cN(0,\sigma^2),\ \theta = [a;b]$. Con lo que su verosimilitud está dada por 
\begin{align}
	L(\theta,\sigma^2) &= \MVN(Y; \tX\theta, \eye\sigma^2)\\
					&\propto \left(\sigma^2\right)^{-n/2}   \exp\left(-\frac{1}{2\sigma^2} (Y - \tX\theta)^\top(Y - \tX\theta)\right),\nonumber
\end{align}
donde la distribución $\MVN$ denota la normal multivariada. Observe que esta última  expresión es proporcional a una distribución Gamma-Inversa para $\sigma^2$ y proporcional a una MVN para $\theta$. Consecuentemente, esta verosmilitud tiene los mismos priors conjugados que el modelo gaussiano en la ec.~\eqref{eq:ejemplo_gaussiano}. Consideremos entonces el caso en que $\sigma^2$ es conocido y elegimos el prior gaussiano para $\theta$ dado por
\begin{equation}
	p(\theta) \propto \exp\left(-\frac{1}{2\sigma^2}(\theta-\theta_0)^\top\Lambda_0(\theta-\theta_0)\right), 
\end{equation}
obtenemos una distribución posterior dada por $\MVN(\theta; \theta_n,\sigma^2\Lambda_n^{-1})$ con parámetros 
\begin{align}
	\theta_n &= (\tX^\top \tX + \Lambda_0)^{-1} (\tX^\top  Y + \Lambda_0\theta_0) = (\tX^\top \tX + \Lambda_0)^{-1} (\tX^\top \tX \sred{(\tX^\top \tX)^{-1}\tX^\top  Y} + \Lambda_0\sblue{\theta_0})\\
	\Lambda_n &= (\tX^\top \tX + \Lambda_0).
\end{align}
Es decir, la media posterior $\theta_n$ es un promedio ponderado entre la media a priori $\theta_0$ y el estimador de máxima verosimilitud $(\tX^\top \tX)^{-1}\tX^\top  Y$. Observemos además cómo la varianza $\Lambda_n$ se mueve desde la varianza a priori $\Lambda_0$ hacia $(\tX^\top \tX)^{-1}$ a medida recibimos más observaciones, resultando en un modelo más preciso. 

\noindent
\begin{minipage}[t]{0.32\textwidth}
\vspace{2em}
La Figura \ref{fig:bayesian_lin_reg}  muestra una implementación de la regresión lineal bayesiana, para el modelo 
\begin{equation*}
	y = ax + b + \epsilon= 2x-2	+ \epsilon,
\end{equation*}
donde $\epsilon\sim\cN(0,0.5^2)$. Desde arriba hacia abajo, se han considerado $\{0,1,2,50\}$ observaciones, donde  cada columna de la figura muestra (de izquierda a derecha): los datos observados, la distribución conjunta de los parámetros $a$ y $b$, las distribuciones marginales de los parámetros, y el modelo real (azul) junto a muestras del modelo posterior (rojo). Observe cómo rápidamente las distribuciones posteriores se concentran en los valores de los parámetros reales. 
\end{minipage}\hfill
\begin{minipage}[t]{0.65\textwidth}

\begin{figure}[H]
	\includegraphics[width=\textwidth,frame]{img/cap1_bayesian_lin_reg_0.pdf}\\
	\includegraphics[width=\textwidth,frame]{img/cap1_bayesian_lin_reg_1.pdf}\\
	\includegraphics[width=\textwidth,frame]{img/cap1_bayesian_lin_reg_2.pdf}\\
	\includegraphics[width=\textwidth,frame]{img/cap1_bayesian_lin_reg_50.pdf}
	\caption{Regresión lineal bayesiana}
	\label{fig:bayesian_lin_reg}
\end{figure}
\end{minipage}


\end{mdframed}

\subsubsection{Modelo binomial}

Ahora ilustraremos la elección de la distribución a priori a través de un segundo ejemplo basado en el modelo binomial, el cual fue considerado en el artículo original de \cite{bayes}. Al comienzo de su artículo, Bayes enuncia el problema que motiva su trabajo: 
\begin{center}
\it
Given the number of times in which an unknown event has happened and
failed: Required the chance that the probability of its (specific event)
happening in a single trial lies somewhere between any two degrees of
probability that can be named.
\end{center}

A pesar de lo confuso de la jerga usada por Bayes, al menos ya podemos notar que se hace una diferencia entre la probabilidad de los datos (\emph{probability}) y la del modelo (\emph{chance}). Si tradujéramos esta formulación del problema al español y con una notación moderna, diríamos: 

\begin{center}
\it
Dados $n$ lanzamientos Bernoulli iid con parámetro $\theta$ donde se han registrado $k$ aciertos, ¿cuál es la probabilidad de que el parámetro $\theta$ se encuentre entre dos cotas dadas?
\end{center}

Consideremos entonces el evento de obtener ``$k$ aciertos en $n$ intentos''. Por ejemplo anotar $k$ goles con $n$ intentos de penales, u obtener $k$ veces un número par al lanzar un dado $n$ veces. La probabilidad de obtener entonces los ``$k$ aciertos en $n$ intentos'' puede ser modelada mediante una distribución binomial, la cual asume que cada acierto es independiente y  equiprobable con probabilidad $\theta$ (o bien, \emph{Bernoulli}). La verosimilitud de este modelo (con parámetro $\theta$) está dada por la distribución binomial de los datos ($n$ lanzamientos y $k$ aciertos) dada por
\begin{equation}
	p(k| n, \theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k}.
\end{equation}

Antes de proceder con el prior conjugado para esta verosimilitud, revisemos qué hizo T. Bayes. Él sabía que la verosimilitud era proporcional a la verosimilitud porque consideró---implícitamente---un prior \textbf{uniforme} para $\theta\in [0,1]$ (el caso binomial fue el único considerado por Bayes). La elección del prior uniforme tiene sentido, pues $\theta$ es una probabilidad de la cual, \emph{a priori}, sabemos nada. Consecuentemente, como solo conocemos una versión proporcional de la posterior a través del Teorema de Bayes,  la posterior es necesariamente (recordemos que consideramos un prior uniforme)
\begin{equation}
	p(\theta|n,k) = \frac{p(n, k| \theta)}{\int_0^1 p(n, k| \theta)\d \theta}= \frac{\theta^k (1-\theta)^{n-k}}{\int_0^1 \theta^k (1-\theta)^{n-k}\d \theta}= \frac{\theta^k (1-\theta)^{n-k}}{\mathcal{B}(k+1,n-k+1)},
\end{equation}
donde $\mathcal{B}(\alpha,\beta) = \int_0^1x^{\alpha-1}(1-x)^{\beta-1}\d x$ es conocida como la función Beta.

Este resultado era al que Bayes llegó en su artículo, aunque no de forma explícita. Sin embargo, al no entregar una forma cerrada para esta probabilidad, el trabajo de Bayes era difícil de implementar. Actualmente, el cálculo de esta constante de proporcionalidad no es problemático\footnote{En particular conocemos la relación entre la función Beta y la función Gamma, $\mathcal{B}(\alpha,\beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$, que facilitan el cálculo de la función Beta.} pues sabemos que éstas son densidades de probabilidad que integran 1, un concepto que no existía en la era de Bayes. 

A simple vista, el prior uniforme propuesto por Bayes no es conjugado, pues la posterior no es uniforme. Sin embargo, al darle una mirada más detallada, podemos identificar que éste sí es un caso particular de un prior conjugado. En efecto, consideremos el prior dado por una distribución Beta para $\theta$, con parámetros ($\alpha, \beta$), con densidad dada por 
\begin{equation}
	p(\theta) = \text{Beta}(\theta;\alpha,\beta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\mathcal{B}(\alpha, \beta)},\ \theta\in [0,1],
	\label{eq:distribucion_beta}
\end{equation}
donde la función Beta definida anteriormente actúa como contante de normalización. Notemos que eligiendo $\alpha=\beta=1$, obtenemos que este prior es efectivamente la densidad uniforme entre 0 y 1. Además, observemos el rol de los parámetros de la distribución Beta, la cual está formada por la multiplicación de dos potencias de $\theta$ con ceros en $\theta=0$ y $\theta=1$, donde $\alpha$ y $\beta$ representan, informalmente, los pesos relativos entre de los aciertos y fallos respectivamente. De hecho, $\E(\theta|\alpha,\beta) = \frac{\alpha}{\alpha + \beta}$.

Para calcular la posterior que resulta del uso del prior Beta, veamos un caso un poco más general que el anterior. Consideremos las observaciones $\datos = \{(n_i,k_i)\}_{i=1}^N$ correspondientes a $N$ juegos (no solo uno como el caso anterior), donde el $i$-ésimo juego consistió en $n_i$ intentos y $k_i$ aciertos. Con estos datos y el prior $\text{Beta}(\theta;\alpha,\beta)$ de la ec.~\eqref{eq:distribucion_beta}, la (versión proporcional de la) distribución posterior de $\theta$ está dada por
\begin{align}
	p(\theta|\datos) & 	\propto \prod_{i=1}^N  p(k_i|n_i,\theta)p(\theta)  \\
			 & \propto  \prod_{i=1}^N\binom{n_i}{k_i}\theta^{k_i}(1-\theta)^{n_i-k_i}\theta^{\alpha-1}(1-\theta)^{\beta-1} \nonumber\\
			 & \propto  \theta^{\ssum k_i + \alpha - 1}(1-\theta)^{\ssum (n_i-k_i) + \beta-1}, \nonumber
\end{align}
donde los símbolos de proporcionalidad se han mantenido debido a la remoción de constantes y hemos usado la notación compacta $\ssum k_i = \ssum_{i=1}^N k_i$. Notemos que la última expresión es proporcional a la definición de distribución Beta en la ecuación \eqref{eq:distribucion_beta}, por lo que ajustando la constante de proporcionalidad tenemos que, al igual que el prior, la posterior es Beta también: 
\begin{equation}
	p(\theta|\datos) = \text{Beta}(\theta;\ssum_{i=1}^N k_i + \alpha,\ssum_{i=1}^N (n_i-k_i) + \beta).
\end{equation}
Vemos entonces explícitamente cómo la distribución posterior es nuevamente una \emph{mezcla} entre el prior y la verosimilitud en función de los parámetros, los cuales de prior a posterior han cambiado: 
\begin{align}
	\alpha &\to \alpha + \ssum_{i=1}^N k_i \label{eq:bayes_update_bin1}\\
	\beta  &\to \beta + \ssum_{i=1}^N (n_i-k_i) \label{eq:bayes_update_bin2},
\end{align}
lo cual es consistente con la interpretación inicial de los parámetros de la distribución Beta: el peso de los aciertos ($\alpha$) se modifica en función de la suma de todos los aciertos y el peso de los fracasos ($\beta$) se modifica en función de la suma de todos los fracasos. Adicionalmente, notemos que la razón entre los parámetros (la cual convergerá cuando el número de observaciones tiende a infinito), la magnitud de éstos va aumentando: la consecuencia de esto es que la varianza de la posterior va disminuyendo, pues 
\begin{equation}
	\V(\theta|\alpha,\beta) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)},
\end{equation}
con lo que la incertidumbre de la distribución posterior de $\theta$ va disminuyendo a medida que vemos más observaciones. 



\begin{mdframed}[style=ejemplo, frametitle={\center Ejemplo: Modelo binomial}]

Apliquemos la actualización bayesiana en el modelo binomial. Consideremos la variable binomial $k$ dada por
\begin{equation}
	k \sim \binom{n}{k}\theta^k(1-\theta)^{n-k},
\end{equation}
con probabilidad $\theta=0.66$ (desconocida). Nuestro objetivo es ver cómo depende la distribución posterior de $\theta$ de la elección del prior a medida que vamos obteniendo más observaciones. Para esto, elegimos 20 priors Beta distintos para $\theta$ dados por 
\begin{equation}
	p_i(\theta)  = \text{Beta}(\alpha_i,\beta_i),\  i=1,\ldots,20,
\end{equation}
donde $\alpha_i,\beta_i\sim U(1,10)$ (iid y discreto).

Ahora, realizaremos tres experimentos, en los cuales elegimos $n\in\{10,100,1000\}$ lanzamientos y calculamos los parámetros de las distribuciones a posteriori correspondientes a cada prior de acuerdo a las ecs.~\eqref{eq:bayes_update_bin1}-\eqref{eq:bayes_update_bin2}. En la Figura \ref{fig:bayes_binomial} podemos ver, desde izquierda a dercha, las  distribuciones a priori elegidas y luego las posteriores correspondientes a 10, 100 y 1000 observaciones. Notemos cómo a medida obtenemos más y más datos, la elección del prior es vuelve irrelevante y, como es de esperar, la evidencia de los datos es suficiente para aprender el modelo exitosamente (pues $\theta  =  0.66$) sin  depender del prior.

\begin{figure}[H] 
	\centering	
	\includegraphics[width=0.75\textwidth,frame ]{img/cap1_bayesian_binomial.pdf}
	\caption{Concentración de la distribución posterior en el modelo binomial.}
	\label{fig:bayes_binomial}
\end{figure}

\end{mdframed}


\subsubsection{Maximo a posteriori}
\label{sub:map}

Como ha sido el criterio en esta sección, mediante la distribución posterior podemos representar toda la información que nuestros sesgos y los datos aportan a la caracterización de un modelo (o parámetro), es decir, simetrías, barras de error, momentos, etc. En particular, es relevante verificar cómo podemos obtener estimaciones puntuales, en una forma similar a las obtenidas mediante mínimos cuadrados o máxima verosimilitud, a través del análisis de la distribución posterior. La motivación para la obtención de estimaciones puntales viene de i) la necesidad de comparar con dichas estimaciones con las obtenidas por MC, MCR y MV, y además ii) los casos donde en efecto necesitamos una estimación puntual y no distribucional, como el problema de toma de decisiones.

Hay distintas alternativas evidentes para extraer una estimación puntual del parámetro $\theta$ desde la distribución $p(\theta|\datos)$, como la media, la mediana y la moda, las cuales son equivalentes cuando la posterior es  Gaussiana (o unimodal y simétrica en general). Siguiendo un criterio similar al de máxima verosimilitud consideraremos estimaciones puntuales mediante la maximización de la distribución posterior, consecuentemente, resumiendo la información de la posterior mediante su moda. 

\begin{definition}[Máximo a posteriori]
Sea $\theta\in\Theta$ un parámetro con distribución posterior $p(\theta|\datos)$ definida en todo $\Theta$, entonces nos referimos a estimación puntal dada por
\begin{equation}
	\thetaMAP = \argmax_{\Theta}p(\theta|\datos),
\end{equation}
como \emph{máximo a posteriori (MAP)}.

\end{definition}

Notemos que es posible encontrar el MAP incluso cuando solo tenemos acceso a una versión \emph{proporcional} a la distribución posterior, un escenario usual en inferencia bayesiana, o también mediante la maximización del logaritmo de ésta última. En efecto, 
\begin{equation}
	\thetaMAP = \argmax_{\theta\in\Theta}p(\theta|\datos) = \argmax_{\theta\in\Theta}p(\datos|\theta)p(\theta)= \argmax_{\theta\in\Theta}\left(\underbrace{\log p(\datos|\theta)}_{l(\theta)} + \log p(\theta)\right),
\end{equation}
donde  nuevamente vemos la maximización de  la función de log-verosimilitud, pero ahora junto al log-prior. Es evidente de esta expresión que si el prior no depende de $\theta$, ess decir, si es uniforme, entonces el criterio MAP es equivalente al MV.

\begin{mdframed}[style=ejemplo, frametitle={\center Ejemplo: Máximo a posterior para el modelo lineal y gaussiano}]
En particular, para el modelo lineal y gaussiano que hemos considerado hasta ahora, podemos calcular $\thetaMAP$ para un prior Gaussiano de media cero y varianza $\sigma_\theta^2$. Éste está dado por (asumimos la varianza del ruido $\sigma_\epsilon^2$ conocida):	
\begin{align}
	\theta_\text{MAP}^\star 	&= \argmax p(Y|\theta,\tX)p(\theta)\nonumber\\
						\text{[independencia, definición]}\ &= \argmax \prod_{i=1}^N \cN(y_i;\theta^\top\tx_i,\sigma_\epsilon^2)\cN(\theta;0,\sigma_\theta^2) \nonumber\\
								%&= \argmax \prod_{i=1}^N \frac{1}{\sqrt{2\pi}\sigma_\epsilon} \exp\left({\frac{-1}{2\sigma_\epsilon^2}(y_i-\theta^\top\tx_i)^2}\right)											\frac{1}{(\sqrt{2\pi}\sigma_\theta)^{M+1}} \exp\left({\frac{-||\theta||^2}{2\sigma_\theta^2}}\right) \nonumber\\
								\text{[reordenar]}\  &= \argmax  \frac{1}{\sqrt{2\pi}\sigma_\epsilon} \frac{1}{(\sqrt{2\pi}\sigma_\theta)^{M+1}} \exp\left( \sum_{i=1}^N{\frac{-1}{2\sigma_\epsilon^2}(y_i-\theta^\top\tx_i)^2} -{\frac{||\theta||^2}{2\sigma_\theta^2}}\right) \nonumber\\
								\text{[logaritmo]}\  &= \argmin \sum_{i=1}^N{(y_i-\theta^\top\tx_i)^2} +{\frac{\sigma_\epsilon^2}{\sigma_\theta^2}||\theta||^2}.\label{eq:MAP_reg_lin}
\end{align}

Observemos que esta expresión es equivalente al costo cuadrático regularizado de la ec.~\eqref{eq:reg_least_squares} con orden $p=2$, es decir, la solución \emph{máximo a posteriori} del modelo lineal y Gaussiano con prior Gaussiano es equivalente a la de mínimos cuadrados regularizados (con orden de  regularización $p=2$). 

\end{mdframed}

Si bien en la ec.~\eqref{eq:MAP_reg_lin} elegimos un prior Gaussiano, pudimos haber elegido un prior exponencial $p(\theta)\propto\exp(\gamma|\theta|)$, con lo que habríamos llegado a MCR con regularización $p=1$ (o LASSO). Esto conecta claramente el uso de una distribución a priori dentro de la inferencia Bayesiana con el criterio general de regularización: El imponer un prior sobre $\theta$ es \emph{promover}, mediante probabilidades relativas, algunas soluciones para $\theta$; mientras que, por el contrario, el uso de un regularizador \emph{penaliza} algunas soluciones. Ajustando apropiadamente la función de regularización y la distribución a priori, podemos llegar a soluciones equivalentes en ambos casos, en particular, la elección de un prior uniforme equivale a un coeficiente de regularización $\rho$ nulo. Sin embargo, debemos recordar que además de la estimación puntual MAP, el enfoque bayesiano entrega la distribución completa sobre el parámetro desconocido.  

Un ejemplo de la conexión entre regularizadores y priors puede ser obtenido directamente del desarrollo anterior. La razón entre las varianzas del ruido y del prior en la ec.~\eqref{eq:MAP_reg_lin}, dada por $\sigma_\epsilon^2/\sigma_\theta^2$, toma el mismo rol que el peso del regularizador $\rho/p$ en la formulación de MCR en la  ec.~\eqref{eq:reg_least_squares}. Esto implica que un prior con varianza $\sigma_\theta^2$ pequeña (cf. grande) es equivalente a un problema de MCR con un $\rho$ grande (cf. pequeño), lo cual tiene sentido:  $\theta$ puede ser ``llevado a cero'' mediante la asignación de un prior concentrado en cero o bien penalizando el magnitud de $\theta$ (MCR).

Desde ahora, podemos referirnos como MAP a las estimaciones puntuales en general pues, como acabamos de ver, ésta es equivalente a MCR y al mismo tiempo contiene al criterio de máxima verosimilitud como caso particular (cuando el prior es uniforme). Además, para modelos generales (distintos al caso lineal y gaussiano) el MAP no podrá ser calculado de forma explícita imponiendo 
\begin{equation}
 	\nabla_\theta  \log p(\theta|\datos) = 0,
 \end{equation}
 sino que tendremos que considerar algoritmos de optimización. En particular consideraremos algoritmos basados en derivadas con iteraciones de la forma
 \begin{equation}
 	\theta_{i+1} = \theta_i + \eta \nabla_\theta  \log p(\theta_i|\datos), \label{eq:itera_MAP_grad}
 \end{equation}
 donde esperamos que la secuencia $\{\theta_i\}_i$, al avanzar en el \emph{ascenso del gradiente de $\log p(\theta_i|\datos)$}, tienda al punto fijo de la ec.~\eqref{eq:itera_MAP_grad}, es decir, $\nabla_\theta  \log p(\theta|\datos) = 0$. 

\begin{remark} 
En el resto del curso, veremos distintos modelos que requieren técnicas sofisticadas de optimización, las cuales permiten no solo \emph{entrenar} dichos modelos (i.e., encontrar sus parámetros), sino también entender cómo funcionan; estos incluyen redes neuronales,  máquinas de soporte vectorial y procesos gaussianos. Con lo visto hasta ahora podemos ver que el aprendizaje de máquinas, se sustenta  en dos elementos fundamentales: el modelamiento probabilístico que permite definir \emph{posibles} modelos, y el proceso d optimización que permite encontrar el \emph{buen modelo}. Podemos entonces, a \emph{grosso modo}, decir que ``aprender es optimizar''.
\end{remark}


\begin{mdframed}[style=ejemplo, frametitle={\center Ejemplo: MAP para el modelo binomial}]

Siguiendo el ejemplo anterior de la distribución binomial, recordemos que al asumir un prior $p(\theta) = \text{Beta}\left(\theta; \alpha,\beta \right)$ para el parámetro $\theta$, la posterior del mismo (dado un conjuntos de $N$ experimentos con $\{n_i\}$ lanzamientos  y $\{k\}_i$ aciertos) está dada por 
\begin{equation}
  	\text{Beta}\left(\theta; \alpha + \sum_{i=1}^N k_i, \beta + \sum_{i=1}^N n_i - k_i \right).
  \end{equation}
  Denotando los parámetros de la posterior mediante $\alpha_N = \alpha + \sum_{i=1}^N k_i$ y $\beta_N = \beta + \sum_{i=1}^N n_i-k_i$, podemos encontrar el MAP mediante
  \begin{align}
  	\thetaMAP &= \argmax \theta^{\alpha_N - 1}(1-\theta)^{\beta_N -1} = \frac{\alpha_N - 1}{\alpha_N + \beta_N - 2,
  	}\nonumber
  \end{align}
  pues el cálculo no es necesario ya que la moda de la distribución Beta es conocida. Además, la varianza posterior, está dada por 
  \begin{equation}
  	\V[\theta|\datos] = \frac{\alpha_N \beta_N }{(\alpha_N  + \alpha_N )^2(\alpha_N  + \beta_N +1)}.
  \end{equation}
  Es directo ver que, como función de las sumas de aciertos/fallos, cuando  observamos muchos datos $\thetaMAP$ tiende a la razón entre aciertos y lanzamientos  totales y $\V[\theta|\datos] $ tiende a cero. Verificaremos esto numéricamente de la misma forma que el ejemplo anterior: consideremos 20 priors Beta con parámetros $\alpha$ y $\beta$ aleatorios entre 1 y 10 y grafiquemos tanto $\thetaMAP$ como $\V[\theta|\datos] $ en función de un númeo creciente de lanzamientos binomiales. La Figura \ref{fig:binomial_MAP} confirma nuestra intuición donde (con escala logarítmica para el eje $x$) podemos ver que con 10 lanzamientos, ya se ve una clara tendencia, mientras que con 100 lanzamientos la convergencia es indudable. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{img/cap1_bayesian_binomial_MAP.pdf}
	\caption{Concentración de MAP y reducción de varianza posterior: modelo binomial.}
	\label{fig:binomial_MAP}
\end{figure}


\end{mdframed}





\subsection{Predicciones} % (fold)
\label{sub:predicciones}
En el caso de las estimaciones puntuales como la de máxima verosimilitud (mínimos cuadrados) o bien máximo a posteriori (mínimos cuadrados regularizados), la predicción puede ser calculada simplemente reemplazando el valor estimado para el parámetro en el modelo. Es decir, si hemos calculado el parámetro mediante máxima verosimilitud (denotado como $\theta_{\text{MV}}$) entonces el modelo lineal es simplemente 
\begin{align}
	 y &= \theta_{\text{MV}}^\top \tilde{x} + \epsilon\\
	 \epsilon &\sim \cN(0,\sigma^2)
\end{align}

Con lo que la distribución del valor de la variable dependiente $y_\star$ que corresponde al valor $x_\star$ de la variable independiente, está simplemente dado por 

\begin{align}
	 y_\star \sim  \cN(\theta_{\text{MV}}^\top \tilde{x}_\star,\sigma^2) 
\end{align}
donde si consideramos la esperanza como estimación puntual, ésta coincide con la estimación del modelo determinístico y está dada por 
\begin{align}
	 \hat{y}_\star  = \theta_{\text{MV}}^\top \tilde{x}_\star
\end{align}

A diferencia de las estimaciones puntuales, cuando realizamos una estimación bayesiana del parámetro $\theta$, es decir, disponemos de su distribución posterior, la distribución sobre valores de $y_\star$ debe tomar en cuenta todos los posibles valores de $\theta$. En efecto, denotando el conjunto de datos como $\datos=\{(x_i,y_i)\}_{i=1}^N$, la distribución de la variable dependiente $y_\star$ dada una nueva entrada $x_\star$ está dada por la distribución condicional del $y$ condicional al conjunto de observaciones $\datos$, lo cual se puede calcular integrando con respecto a la posterior del parámetro, es decir, 
\begin{equation}
 	p(y|x,\datos) = \int p(y|x, \theta)p(\theta|\datos) \td\theta.
 \end{equation} 
Como se vio en la ecuación XXXXX, bajo el supuesto del modelo lineal y gaussiano la posterior sobre $\theta$ también es Gaussiana. Consecuentemente, la integral en la última ecuación puede calcularse de forma analítica y es también gaussiana. 

Finalmente, veamos que la estimación puntual de $y_\star$ usando el enfoque bayesiano también coincide con la estimación puntual usando máxima verosimilitud (o mínimos cuadrados) cuando el modelo es lineal y gaussiano. En efecto, asumiendo la notación para posterior $\cN(\theta;\mu_\theta, \sigma_\theta^2)$, esta estimación puntual está dada por la siguiente esperanza
\begin{align}
	\E\left[y|x_\star,\datos\right] 
	&= \int y p(y|x_\star,\datos) \td y \\
	&= \int y p(y|x_\star, \theta)p(\theta|\datos) \td \theta \td y \nonumber\\
	&= \int y \cN(y;\theta^\top x_\star ,\sigma_e^2)\cN(\theta;\mu_\theta, \sigma_\theta^2) \td\theta \td y \nonumber\\
	&= \int \theta^\top x_\star \cN(\theta;\mu_\theta, \sigma_\theta^2) \td \theta \nonumber\\
	&= \left(\int \theta \cN(\theta;\mu_\theta, \sigma_\theta^2) \td \theta\right)^\top x_\star \nonumber\\
	&= \mu_\theta^\top x \nonumber
\end{align}




\subsection{Máxima verosimilitud y divergencia de Kullback-Liebler}

\begin{mdframed}[style=pendiente, frametitle={\center Discusión}]
1) Definir información y entropía\\
2) Presentar KL-divergence como distancia entre distribuciones\\
3) Interpretar \\
4) Conectar min KL y max verosimilitud
	
\end{mdframed}







% subsection predicciones (end)

\subsection{Ejercicios} % (fold)
\label{sub:ejercicios_regresion_lineal}


Se sabe que el $1\%$ de las mujeres tienen cancer de mamas, y se tiene un test para detectar si una mujer lo presenta o no. Si la paciente tiene cancer (C), el test dará postitivo (PT) con una probabilidad del $80\%$ y negativo (NT) con $20\%$, en cambio cuando la paciente está sana (NC), hay un $9.6\%$ de probabilidad que el test salga erroneo y si detecte cancer (PT).

Una paciente se realiza el test y este sale positivo, nos gustaría obtener la probabilidad de que en realidad tenga cancer dado este resultado.

\begin{align}
	p(C|PT) & =\frac{p(PT|C)p(C)}{p(PT)} \\
			& = \frac{p(PT|C)p(C)}{p(PT|C)p(C)+p(PT|NC)p(NC)} \\
			& = \frac{0.8 \cdot 0.01}{0.8 \cdot 0.01 + 0.096 \cdot 0.99}\\
			& = 0.0776
\end{align}

De esta misma forma podemos completar todos los casos.
\\
{
\centering
\begin{tabular}{c|cc}
\toprule
   & C ($1\%$) &  NC($99\%$) \\\hline
PT($10.3\%$) & $7.7\%$ & $92.3\%$\\
NT($89.6$\%) & $0.2\%$ & $99.8\%$ \\
\bottomrule
\end{tabular}
}




i) Considere el caso en que sus observaciones (entrada $x$, salida $y$) solo consisten en 
\begin{equation}
D = \{(1,a),(2,b)\}.
\end{equation}
Usando la expresión para la solución óptima de mínimos cuadrados de la ecuación \eqref{eq:sol_mse}, encuentre los parámetros del modelo lineal dado por 
\begin{equation}
	y = \theta_1 x +\theta_0 
\end{equation}
e interprete esta solución para distintos valores de $a$ y $b$.

ii) ¿Cuál es la estimador muestral de la covarianza entre $x$ e $y$ para las observaciones disponibles? 

ii) Interprete la correlación entre $x$ e $y$ 




