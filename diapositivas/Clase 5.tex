\documentclass[9pt, handout]{beamer}

\input{preambuloBeamer}
\usetheme{simple}

\title{Clase 5: Inferencia bayesiana (parte 1)}
\subtitle{MA5204 Aprendizaje de Máquinas}
\date{\today}
\author{Felipe Tobar} 
\titlegraphic{
\begin{figure}[htp] 
    \centering
        \includegraphics[width=0.15\textwidth]{../img/Uchile.pdf}% 
\end{figure}
}
\institute{Department of Mathematical Engineering \&\\ Center for Mathematical Modelling\\Universidad de Chile}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

%Limitaciones del enfoque de máxima verosimilitud.
\begin{frame}{Limitaciones del enfoque de máxima verosimilitud}

\begin{itemize}
	\setlength\itemsep{2em}
	\item Una limitación del paradigma de MV es que no da la posibilidad de incorporar conocimiento experto, es decir, introducir sesgos sobre el parámetro al realizar inferencia.\pause
	\item El paradigma bayesiano busca solucionar este problema interpretando el parámetro $\theta$ como variable aleatoria, donde la disponibilidad de datos se interpreta como un evento que aporta evidencia sobre el valor de $\theta$.\pause
	\item Consecuentemente, el proceso de inferencia ahora se centra en encontrar la distribución condicional:
	\begin{equation*}
		p(\theta|\datos),
	\end{equation*}
	y no simplemente $\arg\max_{\theta} p(\datos|\theta)$.
\end{itemize}

\end{frame}

%Teorema de Bayes en el aprendizaje automático.
\begin{frame}{Teorema de Bayes en el aprendizaje automático}

Del curso de probabilidades, es sabido que el teorema de Bayes permite invertir la condicionalidad de una distribución. Asumiendo que el parámetro  ahora es una variable aleatoria $\theta\sim p(\theta)$, de acuerdo a Bayes se tiene que:

\begin{equation*}
	p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)} \propto p(x|\theta)p(\theta),\label{eq:Bayes}
\end{equation*}
donde $x\in\cX$ es una observación, y $\theta\in\Theta$ es un parámetro. \pause En la expresión anterior podemos identificar las siguientes cantidades:
\begin{itemize}
	\item el prior o densidad a priori: $p(\theta)$. \pause
	\item la verosimilitud: $p(x|\theta)$. \pause
	\item la densidad posterior: $p(\theta|x)$. \pause
	\item la densidad marginal de $x$: $p(x) = \int_\Theta p(x|\theta)p(\theta)\d\theta$. \pause
\end{itemize} 

En el problema de inferencia, los datos ($x$) son conocidos y fijos, mientras que el parámetro es desconocido (y por ende, aleatorio). Por esta razón, podemos considerar una versión proporcional de la posterior, omitiendo la distribución de los datos, pues solo importa la posterior como función de $\theta$.\\~\ \pause

\textbf{Nota:} Incertidumbre epistemológica y aleatoria\\~\

Notemos que, a diferencia de MV, aquí $\theta$ es aleatorio por lo que $\Theta$ debe ser un espacio de probabilidad.
\end{frame}

%Elementos de la inferencia bayesiana.
\begin{frame}{Elementos de la inferencia bayesiana}

De la forma proporcional del teorema de Bayes, se reconocen dos elementos que constituyen el enfoque bayesiano:

\begin{itemize}
	\setlength\itemsep{1em}
	\item \textbf{Verosimilitud $p(x|\theta)$:} que modela la aleatoriedad del modelo, el cual produce datos de forma aleatoria incluso cuando el modelo es perfectamente conocido. Este tipo de incertidumbre no puede ser reducida observando datos.\pause
	\item \textbf{La distribución a priori $p(\theta)$:}  encapsula la incertidumbre epistemológica (lo que no sabemos) sobre el sistema, la cual pude ser reducida observando datos y calculando $p(\theta|x)$ mediante el Teorema de Bayes. Distintas distribuciones a priori llevarán a distintas distribuciones a posteriori, lo cual establece la subjetividad del enfoque bayesiano.\pause
\end{itemize}
\vspace{1em}
\textbf{Nota:} $p(\theta, x) = p(x|\theta)p(\theta)$\\~\

Uno de los problemas que tuvo este enfoque al comienzo viene dado por la subjetividad introducida en el prior $p(\theta)$. Si bien se podría proponer un prior uniforme $p(\theta)\propto 1$, este introduce sesgos de todas formas ya que no es invariante bajo transformaciones inyectivas. Actualmente, existen priors no informativos invariantes bajo inyecciones como por ejemplo el prior de Jeffreys.
	
\end{frame}

%Elección del prior: conjugación.
\begin{frame}{Elección del prior: conjugación}

El prior sobre el parámetro $p(\theta)$ encapsula toda la información experta que se le quiera dar al modelo. De esta forma, de acuerdo a $p(\theta|x) \propto p(x|\theta)p(\theta)$, el prior funciona como un ponderador de la verosimilitud de acuerdo a la importancia que se le dé a cada $\theta$. \\~\ \pause

Con el modelo $p(\datos|\theta)$ acordado, solo  queda elegir la distribución a priori, los cual es guiado por dos objetivos:\pause

\begin{itemize}
	\item  En primer lugar, debemos encapsular lo que efectivamente que sabemos del parámetro $\theta$.\pause
	\item  El segundo objetivo es obtener una forma \emph{amigable} de la distribución posterior, en el sentido que esta sea un distribución con propiedades que deseemos, en particular, que la podamos calcular, evaluar, y samplear de ella.
	
\end{itemize}

\end{frame}

%Elección del prior: conjugación.
\begin{frame}{Elección del prior: conjugación}
	
El uso de un prior arbitrario resulta en que la posterior tenga una forma arbitraria también, con lo que, incluso si tanto el prior como la verosimilitud tienen formas \emph{conocidas}, no tenemos ninguna garantía de que el posterior también la tenga y consecuentemente sea difícil de interpretar y calcular.\\~\ \pause

 Una práctica usual es elegir un prior $p(\theta)$ tal que esté en la misma familia que la distribución posterior $p(\theta|\datos)$:

\begin{definition}[prior conjugado]
	Diremos que el prior $p(\theta)$ es \emph{conjugado} a la verosimilitud $p(D|\theta)$, cuando la posterior $p(\theta|\datos)$ está en la misma familia, es decir, tienen la misma distribución con parámetros distintos.   
\end{definition}\pause

Una ventaja de utilizar priors conjugados es que la actualización de prior a posterior al actualizar la verosimilitud debido a la incorporación de datos, es simplemente un cambio de parámetros, lo que permite:
\begin{itemize}
		\item poder interpretar los nuevos parámetros de acuerdo al modelo inicial
		\item que la actualización Bayesian sea equivalente a moverse en un espacio de dimensión fija (e.g. $\R^p$)
\end{itemize} 

\end{frame}

%Prior conjugado sobre el modelo gaussiano.
\begin{frame}{Prior conjugado sobre el modelo gaussiano}

Consideremos un conjunto de observaciones $\datos=\{x_i\}_{i=1}^n\subset\R$ generadas independiente e idénticamente distribuidas (iid) por el modelo $\cN(\mu,\sigma^2)$. Recordemos que la verosimilitud de la media y varianza respectivamente está dada por 
\begin{equation}
	L_\datos(\mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right).
 \end{equation}
\pause
A continuación veremos priors conjugados para esta verosimilitud para dos casos:

\begin{itemize}
	\item la media $\mu$ es desconocida y la varianza $\sigma^2$ es conocida.
	\item la varianza $\sigma^2$ es desconocida y la media $\mu$ es conocida.
	\item El caso donde tanto la media como la varianza son desconocidas, queda propuesto.
\end{itemize}

\end{frame}

%Prior conjugado sobre el modelo gaussiano: mu es desconocido.
\begin{frame}{Prior conjugado sobre el modelo gaussiano: $\mu$ es desconocido}

Consideremos el prior sobre la media $p(\mu) = \cN(\mu_0,\sigma_0^2)$ donde $\mu_0$ y $\sigma^2_0$ son parámetros fijados (hiperparámetros) y por lo tanto conocidos. Bajo este prior, la posterior está dada por:
 \begin{align*}
 	p(\mu|\datos)& \propto p(\datos|\mu)p(\mu)\\
 	& \propto \left(\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right) \right)\frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\right)\\
 	&\propto \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2\right)
 \end{align*} 
 \pause
 Reordenando los términos dentro de la exponencial, se tiene que:
 
 \begin{equation*}
 	p(\mu|\datos) \propto \exp\left(-\frac{1}{2\sigma_n^2}(\mu - \mu_n)^2\right) \text{ donde $\sigma_n^2$ y $\mu_n$ serán definidos en breve.}
 	\end{equation*} 
\pause
como $p(\mu|\datos)$ debe integrar uno, la única densidad de probabilidad proporcional al lado derecho de la ecuación anterior es la Gaussiana de media $\mu_n$ y varianza $\sigma_n^2$.\\~\ \pause

Consecuentemente, se prueba que el prior elegido era efectivamente conjugado con la verosimilitud gaussiana ya que la posterior también tiene distribución gaussiana.

\end{frame}

%Prior conjugado sobre el modelo gaussiano: mu es desconocido.
\begin{frame}{Prior conjugado sobre el modelo gaussiano: $\mu$ es desconocido}

Se probó que la posterior está dada por la siguiente gaussiana:
  \begin{equation*}
 	p(\mu|\datos) = \cN(\mu;\mu_n,\sigma_n^2) = \frac{1}{(2\pi\sigma_n^2)^{N/2}}\exp\left(-\frac{1}{2\sigma_n^2}(\mu - \mu_n)^2\right)
 \end{equation*} 
 donde, luego de reordenar términos, la media y la varianza están dadas por 
 \begin{align*}
 	\mu_n &= \frac{1}{\tfrac{1}{\sigma_0^2} + \tfrac{n}{\sigma^2}} \left(\frac{1}{\sigma_0^2}\mu_0 + \frac{n}{\sigma^2}\bar{x} \right), \quad \text{donde } \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i\label{eq:post_Gm}\\
 	\sigma_n &= \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1}
 \end{align*}\pause

Se observa que los parámetros de la posterior son combinaciones entre los parámetros del prior y los datos. Además, a medida que se tienen más datos, el prior va perdiendo importancia y comienzan a predominar las observaciones.
	
\end{frame}

%prior conjugado sobre el modelo gaussiano: sigma^2 es desconocido.
\begin{frame}{Prior conjugado sobre el modelo gaussiano: $\sigma^2$ es desconocido}

Ahora procedemos con el siguiente prior para la varianza, llamado gamma-inverso:
 \begin{equation*}
 	p(\sigma^2)= \text{inv-}\Gamma(\sigma^2;\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha) (\sigma^2)^{\alpha+1}}\exp(-\beta/\sigma^2)
 \end{equation*}\pause
Con este prior, la posterior de la varianza toma la forma:
 \begin{align*}
 	p(\sigma^2|\datos) &\propto \left(\prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right)\right) \frac{\beta^\alpha}{\Gamma(\alpha) (\sigma^2)^{\alpha+1}}\exp(-\beta/\sigma^2)\\
 	&\propto  \frac{1}{(\sigma^2)^{N/2+\alpha+1}}\exp\left(-\frac{1}{\sigma^2}\left(\frac{1}{2}\sum_{i=1}^N(x_i-\mu)^2 +\beta\right) \right)\nonumber
 \end{align*} 
 donde nuevamente la proporcionalidad ha sido mantenida debido a la remoción de las constantes. \pause Esta última expresión es proporcional a una distribución gamma-inversa con hiperparámetros $\alpha_n$ y $\beta_n$, es decir:
 
 \begin{equation*}
 	p(\sigma^2|\datos) \sim \text{inv-}\Gamma(\sigma^2;\alpha_n,\beta_n) \text{ donde } \alpha_n = \frac{n}{2}+\alpha \text{ y } \beta_n = \frac{1}{2}\sum_{i=1}^N(x_i-\mu)^2 +\beta
 \end{equation*}
	
\end{frame}

\end{document}
