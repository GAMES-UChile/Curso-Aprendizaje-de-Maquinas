\documentclass[9pt, handout]{beamer}

\input{preambuloBeamer}
\usetheme{simple}

\title{Clase 15: Reducción de dimensionalidad}
\subtitle{MDS7104 Aprendizaje de Máquinas}
\date{\today}
\author{Felipe Tobar} 
\titlegraphic{
\begin{figure}[htp] 
    \centering
        \includegraphics[width=0.15\textwidth]{../img/Uchile.pdf}% 
\end{figure}
}
\institute{Iniciativa de Datos e Inteligencia Artificial\\Universidad de Chile}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

%Reducción de dimensionalidad.
\begin{frame}{Reducción de dimensionalidad}

El problema de reducción de dimensionalidad es parte del \textbf{aprendizaje no-supervisado} y consiste con construir una representación de dimensión estrictamente menor que los datos originales. El objetivo de reducir la dimensión de los datos es i) ayudar a la interpretación de la información contenida en los datos, y ii) disminuir el costo computacional en el entrenamiento.\\~\ \pause

Hoy veremos dos técnicas de reducción de dimensionalidad:
  
\begin{itemize}
	\item Análisis de componentes principales (PCA).
	\item Discriminante lineal de Fisher.
\end{itemize}
	
\end{frame}

%Análisis de componentes principales (PCA): idea general.
\begin{frame}{Análisis de componentes principales (PCA): idea general}

Consideremos un conjunto de observaciones de $\{\x_i\}_{i=1}^N\subset\R^M$, se denotará por $x_{ij}$ al valor del atributo $j$ para la observación $i$. \\~\ \pause

Cada observación puede descomponerse en la base canónica $\{\e_i\}_{i=1}^M$ de $\R^M$ de la forma 
\begin{equation*}
	\x_i = x_{i1}\e_1 +  x_{i2}\e_2 + \cdots + x_{iM}\e_M.	
\end{equation*}\pause

Notemos que es posible aproximar cada observación $\x_i$ mediante una cantidad $M'<M$ de términos, truncando la representación anterior, es decir,  
\begin{equation*}
	\x_i \approx \sum_{j=1}^{M'} x_{i\sigma(j)}\e_{\sigma(j)}.
\end{equation*}

donde $\sigma:\{1,2,\ldots,M\}\mapsto\{1,2,\ldots,M\}$ es una permutación que prioriza las coordenadas más representativas de los datos. Dichas aproximaciones son una versión de baja dimensión de las observaciones $\{\x_i\}_{i=1}^N$.\\~\ 

$\star$ ¿qué es \textbf{más} representativa?
\end{frame}

%Análisis de componentes principales (PCA): criterio.
\begin{frame}{Análisis de componentes principales (PCA): criterio}
	
	Dada una dimensión $M'<M$:    	
	\begin{itemize}
		\item ¿es efectivamente un subconjunto de los vectores canónicos la mejor base para descomponer las observaciones? \pause
		\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{../img/cap6_pca.pdf}
	\end{figure}\pause

		\item ¿cómo encontramos la \emph{mejor} base?\\~\ \pause
		
		Lo primero que se requiere es definir qué se entiende por \emph{mejor}. Nos enfocaremos en determinar una base cuyos componentes \textbf{ordenados} $\cvector_1,\cvector_2,\ldots$ capturan las $M'$ direcciones ortogonales de máxima variabilidad de nuestros datos.
	\end{itemize}
	Este criterio es conocido como \textbf{análisis de componentes principales (PCA)}.
\end{frame}


%Análisis de componentes principales (PCA): criterio.
\begin{frame}{Análisis de componentes principales (PCA): criterio}

De esta forma, dado que $\langle\cvector,\x\rangle$ representa la proyección ortogonal de $\x$ sobre $\cvector$, el primer elemento de la nueva base estará dado por 
\begin{equation*}
	\cvector_1 = \argmax_{||\cvector||=1} {\langle\cvector,\x\rangle}, \label{eq:PCA_max}
\end{equation*} \pause
donde $\x$ es la VA cuya realización son los datos.


\begin{itemize}
	\item La restricción $||\cvector_1||=1$ es necesaria ya que $\langle\lambda\cvector_1,\x\rangle = \lambda\langle\cvector_1,\x\rangle$ por lo que $\langle\cvector_1,\x\rangle$ puede crecer indefinidamente si no se fija una restricción sobre la norma de $\cvector$. \pause
	\item Además, es importante estandarizar los datos:\\~\ \pause

	\begin{enumerate}
	\item \textbf{Características de media nula:} la matriz $X$ debe tener columnas con media $0$. El objetivo de este ajuste es poder centrar los datos.\\~\ \pause
	\item \textbf{Varianzas marginales unitarias:} si una dimensión tiene una varianza marginal mayor que el resto, esta será más importante en la determinación de la dirección de máxima varianza solo por su magnitud y no por la relación entre variables.
	\end{enumerate}
\end{itemize}

	
\end{frame}

%Análisis de componentes principales (PCA): formulación.
\begin{frame}{PCA: Formulación}
En general no contamos con la distribución de las observaciones $p(\x)$, por lo que se puede considerar una aproximación muestral de la varianza y resolver 
\begin{align*}
	\cvector_1 = \argmax_{||\cvector||=1} \sum_{i=1}^N \langle\cvector,\x_i\rangle^2.\label{eq:PCA_max2}
\end{align*} \pause
Usando la siguiente notación
$$
X=\begin{bmatrix}
        \x_1^\top\\
        \vdots\\
        \x_N^\top\\
        \end{bmatrix}=
        \begin{bmatrix}
        {x}_{11}    & \dots & {x}_{1M}  \\
        \vdots          & \ddots& \vdots        \\
        {x}_{N1}    & \dots & {x}_{NM}
        \end{bmatrix},
$$
\pause
se puede reescribir el problema como
\begin{equation*}
	\cvector_1 = \argmax_{||\cvector||=1} ||X\cvector||^2 
			= \argmax_{||\cvector||=1} \cvector^\top X^\top X \cvector
			= \argmax_{\cvector} \frac{\cvector^\top X^\top X \cvector}{\cvector^\top \cvector}.
			\label{eq:PCA_max3}
\end{equation*}

\end{frame}


%Análisis de componentes principales (PCA): cociente de Rayleigh.
\begin{frame}{PCA: Cociente de Rayleigh}
	
Para el problema anterior, se tiene la siguiente propiedad:

\begin{lemma}[Cociente de Rayleigh]

Sea $M\in\mathcal{M}_{nn}(\R)$ matriz cuadrada simétrica, entonces, el cociente de Rayleigh

\begin{equation*}
	R(M,x):=\frac{x^\top Mx}{x^\top x},
\end{equation*}
toma valores mínimo y máximo dados por los valores propios mínimo y máximo de $M$ (denotados $\lambda_{\text{min}},\lambda_{\text{máx}}$ respectivamente), los cuales son alcanzados por los vectores propios corrspondientes, es decir,  $x=v_{\text{min}}$ y $ x=v_{\text{máx}}$ respectivamente.

\end{lemma}
\pause
De esta forma, dado que $X^\top X$ es simétrica, su cociente de Rayleigh es maximizado en el vector propio asociado al valor propio máximo de $X^\top X$.\\~\ \pause

Consecuentemente, la proyección de una observación $\x_i$ en la dirección de máxima varianza, o bien la \emph{primera componente principal}, está dada por 
\begin{equation*}
	\x_i^{(1)} = \langle \x_i, \cvector_1 \rangle,
\end{equation*}
donde $\cvector_1$ es el vector propio asociado al mayor valor propio de la matriz de covarianza muestral $XX^\top$.\\	
\end{frame}

%Kernel PCA.
\begin{frame}{Estudio personal: Kernel PCA}

Es posible utilizar el truco del kernel en PCA. En ese sentido, en vez de calcular la matriz de covarianza empírica $X^\top X$, se utiliza la matriz de Gram dada por un kernel $K$ donde

$$
K_{ij} = K(x_i,x_j) = \langle\phi(x_i),\phi(x_j)\rangle
$$

Luego, se realiza PCA utilizando dicha matriz.\\ \pause

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{../img/cap6_kpca.pdf}
    \caption{Ejemplo de KPCA sobre un conjunto de datos que no es linealmente separable.}
    \label{fig:kpca}
\end{figure}

\end{frame}

%Discriminante lineal de Fisher: idea general.
\begin{frame}{Discriminante lineal de Fisher: idea general}
	
Es posible interpretar el problema de clasificación como uno de \emph{reducción de dimensionalidad}: \pause

\begin{itemize}
	\item La reducción consiste representar nuestros datos  en solo una dimensión, la cual representa su grado de pertenencia a una clase. \pause
	\item Al proyectar un objeto $M$-dimensional en un espacio  1-dimensional, se pierde gran parte de la información, por lo que clases claramente separadas en el espacio $M$-dimensional puedan traslaparse al ser proyectadas a 1 dimensión.\pause
	\item Sin embargo, es posible ajustar el vector $a$ con la finalidad de obtener una proyección de $x$ que maximice el grado de separación entre clases.\pause
	\item Para el problema de clasificación binaria de $x\in \R^M$, se proyecta $x$ en un espacio \textbf{unidimensional} con respecto a un vector $a\in\R^M$ de acuerdo a:
\begin{equation*}
	y = a^\top x,
\end{equation*}
donde se puede definir un umbral $b$ para asignar $x$ a $\cC_1$ si $y+b\geq 0$ y $x$ a $\cC_2$ en caso contrario. Esto recupera el modelo lineal para clasificación.
\end{itemize}
	
\end{frame}

%Discriminante lineal de Fisher: primera formulación.
\begin{frame}{Discriminante lineal de Fisher: primera formulación}
Se buscará un vector $a$ con la finalidad de obtener una proyección de $x$ que maximice el grado de separación entre clases. Se propone el siguiente esquema:\pause

\begin{itemize}
	\item Cardinales de clase: $N_1 = |\{x\in\datos:x\in\cC_1\}|$ y $N_2 = |\{x\in\datos:x\in\cC_2\}|$.\pause
	\item Estos permiten calcular los promedios muestrales (centros de masa) de cada clase: 
\begin{equation*}
	\mu_1=\frac{1}{N_1}\sum_{n\in\mathcal{C}_1}x_n,
	\quad\quad\quad
	\mu_2=\frac{1}{N_2}\sum_{n\in\mathcal{C}_2}x_n,
\end{equation*}
\pause
	\item De esta forma, la medida más simple de separación entre las proyecciones de las clases sobre $a$ es la distancia entre las medias  de sus proyecciones:
\begin{equation*}
	m_1 - m_2 = a^\top(\mu_1-\mu_2),
\end{equation*}
donde $m_k= a^\top\mu_k$ corresponde al promedio de los elementos de  la clase $\mathcal{C}_k$ proyectado sobre el  vector $a$ (centro de masa sobre la recta).
	\item el $a$  que maximiza la expresión anterior es simplemente $a\propto (\mu_1-\mu_2)$
\end{itemize}

\end{frame}


%Discriminante lineal de Fisher: primera formulación (desventaja).
\begin{frame}{Discriminante lineal de Fisher: primera formulación (desventaja)}

El problema de este enfoque es que pueden existir 2 clases bien separadas en el espacio $M$-dimensional, pero que al proyectar los datos sobre la recta que une sus promedios, las proyecciones de cada clase se traslapen. \pause

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{../img/cap6_fisher.pdf}

\end{figure}

\end{frame}


%Discriminante lineal de Fisher: segunda formulación.
\begin{frame}{Discriminante lineal de Fisher: segunda formulación}

Para resolver este problema, Fisher propuso el siguiente esquema:

\begin{itemize}
	\item maximizar la distancia entre las medias de las clases proyectadas (primera formulación).\pause
	\item Adicionalmente, minimizar la dispersión de los elementos de una misma clase con el objetivo de disminuir el traslape entre las proyecciones de las clases.\pause
\end{itemize}

 Como medida de dispersión, se define la varianza muestral proyectada de los elementos de la clase $\cC_k$ mediante
\begin{align*}
	s_k^2 &= \sum_{n\in \mathcal{C}_k}(a^\top(x_n-\mu_k))^2= \sum_{n\in \mathcal{C}_k}(y_n-m_k)^2,
\end{align*}
donde el factor de correción $\frac{1}{N_k-1}$ fue omitido ya que de lo contrario, todas las clases pesarían lo mismo sin importar la cantidad de elementos de la clase.\\~\ \pause

Lo anterior permite definir la siguiente función objetivo:
\begin{equation*}
J(a) = \frac{(m_1-m_2)^2}{s_1^2+s_2 ^2}.
\end{equation*}

\end{frame}


%Discriminante lineal de Fisher: segunda formulación.
\begin{frame}{Discriminante lineal de Fisher: segunda formulación}

Se puede expresar este costo directamente como función del vector de proyección $a$:
\begin{equation*}
	J(a) = \frac{(m_1-m_2)^2}{s_1^2+s_2 ^2} = \frac{a^\top S_B a}{a^\top S_Wa},
\end{equation*}
donde la matriz de covarianza entre clases $S_B$ y matriz total de covarianza dentro de clases $S_W$ están dadas por
\begin{align*}
	S_B &= (\mu_1-\mu_2)(\mu_1-\mu_2)^\top\\
	S_W &= \sum_{n\in \cC_1}(x_n-\mu_1)(x_n-\mu_1)^\top +
	\sum_{n\in \cC_2}(x_n-\mu_2)(x_n-\mu_2)^\top. 
\end{align*}

\end{frame}


%Discriminante lineal de Fisher: optimización.
\begin{frame}{Discriminante lineal de Fisher: optimización}

Aplicando la condición de primer orden para $J(a)=\frac{a^\top S_B a}{a^\top S_Wa}$, obtenemos que el vector $a$ óptimo debe cumplir
\begin{equation*}
	(a^\top S_B a)S_W a = (a^\top S_W a)S_B a.	
\end{equation*}
\pause
\begin{itemize}
	\item La norma del vector  $a$ es irrelevante (solo interesa su orientación), con lo que  ignorando los escalares $a^\top S_B a$ y $a^\top S_W a$ tenemos que la relación de optimalidad es $S_W a \propto S_B a$.\pause
	\item por la definición de $S_B$, sabemos que $S_B a\propto(\mu_1-\mu_2)$, con lo que la relación de optimalidad se convierte en es $S_W a \propto (\mu_1-\mu_2)$.
\end{itemize}
\pause
Consecuentemente, el vector optimo $a$ en el  criterio de Fisher debe cumplir
\begin{equation*}
	a \propto S_W^{-1}(\mu_1-\mu_2).
\end{equation*}

\end{frame}

%Discriminante lineal de Fisher: ejemplo.
\begin{frame}{Discriminante lineal de Fisher: ejemplo}
En la siguiente figura se observa el  discriminador lineal que solo considera los promedios (izquierda) y la corrección de Fisher (derecha). 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{../img/cap6_dos_clases_proyeccion.pdf}

\end{figure}
\end{frame}


\end{document}
