\documentclass[handout, 9pt]{beamer}

\input{preambuloBeamer}
\usetheme{simple}

\title{Clase 12: Selección de modelos (parte 2)}
\subtitle{MDS7104 Aprendizaje de Máquinas}
\date{17 de abril de 2023}
\author{Felipe Tobar} 
\titlegraphic{
\begin{figure}[htp] 
    \centering
        \includegraphics[width=0.15\textwidth]{../../img/Uchile.pdf}% 
\end{figure}
}
\institute{Iniciativa de Datos e Inteligencia Artificial\\Universidad de Chile}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

%Selección de modelos.
\begin{frame}{Selección de modelos}

Como se comentó la clase pasada, el método de validación cruzada tiene la limitación de requerir una gran cantidad de datos para poder realizar la partición de $\datos$.\\~\ \pause

En esta clase se estudiarán dos métodos más sofisticados para la elección de modelos:

\begin{itemize}
	\item Criterio de información de Akaike (AIC).
	\item Criterio de información bayesiano (BIC).
\end{itemize}
	
\end{frame}


%Criterio de información de Akaike (AIC).
\begin{frame}{Criterio de información de Akaike (AIC)}

Sea $\datos=(x_i)_{i=1}^N$ un conjunto de observaciones generadas por una distribución desconocida perteneciente a una familia paramétrica cuyos parámetros están en $\Theta\subset\R^d$.\\ \pause

Bajo este modelo, se puede utilizar el estimador de máxima verosimilitud:
\begin{equation*}
	\hat{\theta} = \argmax_{\theta\in\Theta} L(\theta|\datos) =  \argmax_{\theta\in\Theta} l(\theta|\datos).
\end{equation*}

Una forma de evaluar el desempeño real de este estimador es mediante el \textbf{riesgo de predicción}, el cual se ve reflejado en la log-verosimilitud de $\hat{\theta}$ sobre todas las posibles observaciones: $\E(l(\hat{\theta}|x))$.\\~\ \pause

Dado que solo se cuenta con una cantidad finita de muestras, solo es posible obtener un riesgo empírico.\\

\end{frame}


%AIC: riesgo empírico y real.
\begin{frame}{AIC: derivación (riesgo empírico y real)}
	AIC busca ajustar este riesgo para obtener un \textbf{estimador asintóticamente insesgado del riesgo real}.\\ \pause
	
Para eso, se tienen las siguientes definiciones para el estimador de máxima verosimilitud $\hat{\theta}$:

\begin{itemize}
	\item \textbf{Riesgo empírico:} $R_\datos(\hat{\theta})=-\hat{l}$, donde $\hat{l}=l(\hat{\theta}|\datos)$ es la log-verosimilitud del EMV empírico. \pause
	\item \textbf{Riesgo real:} $R(\hat{\theta})=-\E(N\cdot l_0(\hat{\theta}))$, donde $l_0(\theta)=\E(l(\theta|x))$ corresponde a la log-verosimilitud de $\theta$ sobre todo el espacio muestral. Notar que se multiplica por $N$ ya que en el riesgo empírico no se normalizó por $N$. \pause
\end{itemize}

Para obtener el AIC se analizará el sesgo asintótico del riesgo empírico con respecto al riesgo real. Para esto:

\begin{itemize}
	\item Se utilizarán aproximaciones sobre ambos riesgos. \pause
	\item Se considerará que medida que $N$ crece, el EMV empírico tiende al EMV global. \pause
	\item Lo anterior implica que el residuo de Taylor tenderá a 0.
\end{itemize}

\end{frame}

%AIC: derivación (aproximaciones para los riesgos).
\begin{frame}{AIC: derivación (aproximaciones para los riesgos)}

Sea $\theta_0 = \argmax_{\theta\in\Theta} l_0(\theta)$ el EMV sobre todo el espacio muestral. Utilizando una aproximación de Taylor de segundo orden sobre $l_0$ alrededor de $\theta_0$ se prueba que:
\begin{align*}
	l_0(\hat{\theta})&\approx l_0(\theta_0) + \frac{1}{2}(\hat{\theta}-\theta_0)^\top H_{l_0}(\theta_0) (\hat{\theta}-\theta_0).
\end{align*}\pause
De esta forma, se tiene una aproximación de segundo orden para el riesgo real:

\begin{equation*}
	R(\hat{\theta}) \approx -N \cdot l_0(\theta_0) - \frac{N}{2}\E\left((\hat{\theta}-\theta_0)^\top H_{l_0}(\theta_0) (\hat{\theta}-\theta_0)\right),\pause
\end{equation*}
y de forma análoga se prueba que:
\begin{equation*}
	\hat{l} \approx \sum_{i=1}^N l(\theta_0|x_i) + N(\hat{\theta}-\theta_0)^\top \E(H_l(\theta_0|x_i)) (\theta_0-\hat{\theta}) + \frac{N}{2}(\hat{\theta}-\theta_0)^\top \E(H_l(\theta_0|x_i)) (\hat{\theta}-\theta_0)
\end{equation*}\pause

Obteniendo una aproximación de segundo orden para el riesgo empírico:

\begin{equation*}
	\E(R_\datos(\hat{\theta})) = -N\cdot l_0(\theta_0) + \frac{N}{2} \E\left((\hat{\theta}-\theta_0)^\top H_{l_0}(\theta_0) (\hat{\theta}-\theta_0)\right)
\end{equation*}

	
\end{frame}

%AIC: derivación (corrección del sesgo).
\begin{frame}{AIC: derivación (corrección del sesgo)}


De este modo, el sesgo del riesgo empírico como estimador del riesgo real es:

\begin{equation*}
	\E(R_\datos(\hat{\theta})) - R(\hat{\theta}) = -N \E\left((\hat{\theta}-\theta_0)^\top H_{l_0}(\theta_0) (\hat{\theta}-\theta_0)\right)\pause
\end{equation*}

Dado que $\sqrt{N}\left(\hat{\theta}-\theta_0\right)\approx\mathcal{N}\left(0,H_{l_0}(\theta_0)^{-1}\right)$, la forma cuadrática anterior puede ser aproximada por una distribución de Pearson: $N(\hat{\theta}-\theta_0)^\top H_{l_0}(\theta_0) (\hat{\theta}-\theta_0)\approx\mathcal{X}^2_d$, donde $\E(\mathcal{X}^2_d)=d$. De este modo:

\begin{equation*}
	\E(R_\datos(\hat{\theta})) - R(\hat{\theta}) \approx -d
\end{equation*}\pause

Por lo que corrigiendo $R_\datos(\hat{\theta})$ se obtiene un estimador asintóticamente insesgado del riesgo real: $R_\datos(\hat{\theta})+d$.

\end{frame}

%AIC: definición.
\begin{frame}{AIC: definición}

La corrección anterior motiva la siguiente definición:

\begin{definition}[AIC]
	Sea $M$ un modelo estadístico $d$-paramétrico y $\datos=(x_i)_{i=1}^N$ un conjunto de observaciones. El AIC del modelo (aproximado por $\datos$) se define como
	
	\begin{equation*}
		AIC(M,\datos):=2d-2\log\left(\hat{L}(\datos)\right)
	\end{equation*}
	
	Donde $\hat{L}(\datos)$ corresponde a la verosimilitud del EMV asociado a $\datos$, es decir:
	
	\begin{equation*}
		\hat{L}(\datos) = \prod_{i=1}^N p(x_i|\hat{\theta}),\text{ para } \hat{\theta} = \argmax_{\theta\in\Theta} L(\theta|\datos)
	\end{equation*}
\end{definition}\pause

Como se puede ver, el AIC corresponde al estimador asintóticamente insesgado del riesgo real multiplicado por 2. Esta ponderación es realizada por motivos históricos.
	
\end{frame}

%Observaciones acerca de AIC.
\begin{frame}{Observaciones acerca de AIC}
\begin{itemize}
	\item Para un conjunto de posibles modelos, se debe elegir el modelo que presente el menor valor AIC ya que será el que minimice el riesgo de predicción.\pause
	\item AIC no se basa únicamente en la verosimilitud del modelo sino que agrega una penalización de acuerdo a la cantidad de parámetros, evitando elegir un modelo sobreajustado a los datos.\pause
	\item  Una de las hipótesis de AIC es que el espacio muestral es infinito ya que se asume que el error de Taylor es despreciable. Para una cantidad finita de datos, se puede realizar una corrección del estimador dada por:
	
	\begin{equation*}
		AICc(M,\datos) := AIC(M,\datos) + \frac{2d(d+1)}{N-d-1}
	\end{equation*}
	
	Es importante notar que cuando $N\to\infty$ se recupera el AIC original.
\end{itemize}
	
\end{frame}

%Criterio de información bayesiano (BIC).
\begin{frame}{Criterio de información bayesiano (BIC)}

Otro enfoque para la selección de modelos corresponde al criterio de información bayesiano (o criterio de Schwarz).\pause

\begin{itemize}
	\item Dada una familia de modelos $\mathcal{M}$, se define un prior $p(m)$ para cada modelo $m\in\mathcal{M}$.\pause
	\item Además, se define un prior $p(\theta|m)$ sobre los parámetros de cada modelo.\pause
\end{itemize}

   El criterio de información bayesiano (BIC) elige al mejor modelo de acuerdo a la posterior $p(m|\datos)$, la cual viene dada de acuerdo al teorema de Bayes:

\begin{equation*}
	p(m|\datos)=\frac{p(\datos|m)p(m)}{p(\datos)}\propto p(\datos|m)p(m)
\end{equation*}\pause

De forma similar al criterio de Akaike, se puede calcular la verosimilitud del modelo $p(\datos|m)$ mediante aproximaciones de Taylor, probando que es independiente del prior. 
	
\end{frame}


%BIC: definición.
\begin{frame}{BIC: definición}

La derivación de $p(\datos|m)$ lleva a la siguiente definición:

\begin{definition}[BIC]
	Sea $M$ un modelo estadístico $d$-paramétrico y $\datos=(x_i)_{i=1}^N$ un conjunto de observaciones. El BIC del modelo (aproximado por $\datos$) se define como
	
	\begin{equation*}
		BIC(M,\datos):= d\cdot\log(N) - 2\log\left(\hat{L}(\datos)\right)
	\end{equation*}
	
	Donde nuevamente $\hat{L}(\datos)$ corresponde a la verosimilitud del EMV asociado a $\datos$.
\end{definition}\pause

En este caso, se vuelve a elegir el modelo que presente el menor BIC. Se observa que, al igual que AIC, BIC contiene una penalización sobre el número de parámetros por lo que también evita el sobreajuste a los datos.

\end{frame}

%Teoremas de Stone y Shao.
\begin{frame}{Teoremas de Stone y Shao}

Al igual de validación cruzada, los criterios de Akaike y bayesiano buscan elegir el mejor modelo de acuerdo a su capacidad predictiva fuera de muestra. Existe una estrecha relación entre ambas técnicas, las cuales se resumen en el siguiente teorema:

\begin{theorem}[Stone (1977) - Shao (1997)] Para una familia de modelos, minimizar el AIC es asintóticamente equivalente a realizar LOOCV. Por otra parte, minimizar el BIC es asintóticamente equivalente a realizar leave $p$ out cross validation para

\begin{equation*}
	p=\left\lfloor N\left(1-\frac{1}{\log(N)-1}\right)\right\rfloor
\end{equation*}
	
\end{theorem}

\end{frame}

%AIC y BIC para la regresión lineal.
\begin{frame}{AIC y BIC para la regresión lineal}

Para el modelo lineal gaussiano $ y = c^\top x + \epsilon$, con $\epsilon\sim\cN(0,\sigma^2)$ se tiene que $y|x \sim \cN(y;c^\top x,\sigma^2)$. Sean $\hat{c}$ y $\hat{\sigma}^2$ los EMV del modelo, entonces la log-verosimilitud máxima viene dada por:
\begin{align*}
	\hat{l}(\datos) &= \frac{-N}{2}\log(2\pi\hat{\sigma}^2) - \frac{1}{2\hat{\sigma}^2} \sum_{i=1}^N( y_i-\hat{c}^\top x_i)= -\frac{N}{2}\log(2\pi) - \frac{N}{2}\log(\hat{\sigma}^2) - \frac{1}{2\hat{\sigma}^2}  N\hat{\sigma}^2\\
	&= C(N) - \frac{N}{2}\log(\hat{\sigma}^2) = C(N)- \frac{N}{2}\log\left(\frac{1}{N}\text{RSS}(\datos)\right)
\end{align*} 

Donde $C(N) = -\frac{N}{2}\log(2\pi) - N$ y $\text{RSS}(\datos)$ corresponde a la suma de cuadrados residuales: $\text{RSS}(\datos) := \sum_{i=1}^N \left(y_i - c^\top x_i\right)^2$.\\~\ \pause

Dado que $C(N)$ es una constante independiente del modelo, puede ser omitida en la comparación de modelos, por lo tanto:

\begin{itemize}
	\item $AIC=2d-N\log(\frac{1}{N}\text{RSS}(\datos))$
	\item $BIC = d\log(N) - N\log(\frac{1}{N}\text{RSS}(\datos))$
\end{itemize}
	
\end{frame}

\end{document}
