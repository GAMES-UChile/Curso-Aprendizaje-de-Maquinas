\documentclass[handout, 9pt]{beamer}

\input{preambuloBeamer}
\usetheme{simple}

\title{Clase 18: Redes neuronales (parte 2)}
\subtitle{MA5204 Aprendizaje de Máquinas}
\date{\today}
\author{Felipe Tobar} 
\titlegraphic{
\begin{figure}[htp] 
    \centering
        \includegraphics[width=0.15\textwidth]{../img/Uchile.pdf}% 
\end{figure}
}
\institute{Iniciativa de Datos e Inteligencia Artificial\\Universidad de Chile}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

\section{Entrenamiento de una Red Neuronal}

\begin{frame}{Forward propagation}
En una red neuronal \textit{feedforward}, la información fluye a través de la red desde la \emph{entrada} $x$ hasta la \emph{salida} $\hat{y}$. Esto se conoce como \textbf{forward propagation} y es la forma en que, durante el entrenamiento, se calcula el costo $J(X, \theta)$ \pause
\newline 
El algoritmo para \textbf{forward propagation} es el siguiente 
\newline
\hspace{0.5cm} \textbf{Requisitos: }   $W^{(k)} , b^{(k)} , f^{(k)} \hspace{0.2cm} k \in \{1,...,l\}$ , $U$ , $c$ , $g$ , $x$ input e $y$ output  \pause
\begin{enumerate}
  \item $h^{(0)} \gets x$ \pause
  \item para $k = 1 , \dots , l$
  \begin{itemize}
    \item $u^{(k)} \gets h^{(k-1)}W^{(k)} + b^{(k)}$ \pause
    \item $h^{(k)} \gets f^{(k)}(u^{(k)})$ \pause
  \end{itemize}
  \item $\hat{y} \gets g(h^{(l)}U + c)$ \pause
  \item $J \gets L(\hat{y},y)$ \pause

\end{enumerate}

\begin{observacion}
En la primera iteración del entrenamiento, los pesos son generados aleatoriamente
\end{observacion}

\end{frame}

\begin{frame}{Backpropagation - Introducción}
El algoritmo de $\textbf{backpropagation}$ consiste en transmitir la *información* en sentido inverso, es decir, desde la salida hasta la entrada. La utilidad de esto es el cálculo eficiente de cómo cada peso (o parámetro) afecta a la función de costo, donde *eficiente* es en este caso *recursivo*. Específicamente, se calculará el gradiente del función de costo con respecto de cada parámetro, donde la recursión es natural debido a la forma composicional de la red. Evaluar el gradiente sin la recursión es computacionalmente demandante e implica un cálculo innecesario de términos redundantes.\\~\


\begin{itemize}
\item gradiente estocástico
\item GPU, paralelización
\end{itemize}




\end{frame}

\begin{frame}{Backpropagation - Contexto}

\begin{itemize}
  \item Notación: $h^{(k-1)} \mapsto u^{(k)} = W^{(k)} h^{(k-1)}  + b^{(k)} \mapsto h^{(k)} = f(u^{(k)})$ \pause

  \item Entrenamiento mediante \textbf{mini-batches}
  \item Entrada $x_d$ produce estímulo $u_{dj}^{(k)}$  y activación $h_{dj}^{(k)}$  en nodo $j$ de capa $k$ \pause
 
 \item Función de costo: MSE en un problema de regresión:  \pause

\end{itemize} 
\[
J(X , \theta) = \frac{1}{N}\sum_{d=1}^N(\hat{y}_d-y_d)^2.
\]
\textbf{Objetivo:} actualizar $w_{ij}^{(k)}$: peso que conecta neurona $i$ en capa $k-1$ con neurona $j$ en la capa $k$. \\~\

Para utilizar gradiente estocástico calculamos $\frac{\partial J(X , \theta) }{\partial w_{ij}^{(k)}}$, donde \pause
\[
\frac{\partial J(X , \theta) }{\partial w_{ij}^{(k)}} = \frac{1}{N}\sum_{d=1}^N \frac{\partial}{\partial w_{ij}^{(k)}} \left ( \frac{1}{2}(\hat{y}_d-y_d)^2 \right) = \frac{1}{N}\sum_{i=1}^N \frac{\partial J_d}{\partial w_{ij}^{(k)}}.
\]
\end{frame}

\begin{frame}{Backpropagation - Resolución I}

Utilizando la regla de la cadena 
\[
\frac{\partial J_d}{\partial w_{ij}^{(k)}} = \frac{\partial J_d}{\partial u_{dj}^{(k)}}\frac{\partial u_{dj}^{(k)}}{\partial w_{ij}^{(k)}}.
\] \pause
La expresión $\frac{\partial J_d}{\partial u_{dj}^{(k)}}$ corresponde a un término de \textbf{error} y lo denotaremos \
\[
\delta_{dj}^{(k)} \equiv \frac{\partial J_d}{\partial u_{dj}^{(k)}}.
\] \pause
Mientras que para el otro término tenemos que 
\[
\frac{\partial u_{dj}^{(k)}}{\partial w_{ij}^{(k)}} = \frac{\partial}{\partial w_{ij}^{(k)}} \left ( \sum_{a = 1}^{k_k}w_{aj}^{(k)}h_{da}^{(k-1)} + b_j^{(k)} \right) = h_{di}^{(k-1)},
\] \pause
y así 
\[
\frac{\partial J_d}{\partial w_{ij}^{(k)}} = \delta_{dj}^{(k)}  h_{di}^{(k-1)}.
\] \pause
El gradiente total es la suma de los $N$ gradientes:
\[\frac{\partial J}{\partial w_{ij}^{(k)}} = \sum_{d=1}^N \delta_{dj}^{(k)}  h_{di}^{(k-1)}  \Rightarrow  \frac{\partial J}{\partial W^{(k)}} = (h^{(k-1)})^\top \otimes \hspace{0.1cm} \delta^{(k)},
\]
donde $\otimes$ es el producto elemento-por-elemento y omitiremos la constante $1/N$.
\end{frame}


\begin{frame}{Backpropagation - Resolución II}
Ahora, veremos que $\delta_{dj}^{(k)}$ sigue una recursión. 

Nuevamente usando la regla de la cadena: \pause
\[
\delta_{dj}^{(k)} = \frac{\partial J_d}{\partial u_{dj}^{(k)}} = \sum_{a=1}^{k_{k+1}} \frac{\partial J_d}{\partial u_{da}^{(k+1)}} \frac{\partial u_{da}^{(k+1)}}{\partial u_{dj}^{(k)}} = \sum_{a=1}^{k_{k+1}} \delta_{da}^{(k+1)} \frac{\partial u_{da}^{(k+1)}}{\partial u_{dj}^{(k)}}, 
\] 
\pause 
donde es directo que

\[
\frac{\partial u_{da}^{(k+1)}}{\partial u_{dj}^{(k)}} = w_{ja}^{(k+1)}f'(u_{dj}^{(k)})  \Rightarrow  \delta_{dj}^{(k)} = f'(u_{dj}^{(k)})\sum_{a=1}^{k_{k+1}}w_{ja}^{(k+1)}\delta_{da}^{(k+1)}.
\]
\pause
Obtuvimos: una expresión para el gradiente en función de $\delta_{dj}^{(k)}$ y una recursión para $\delta_{dj}^{(k)}$, con lo que podemos calcular el gradiente en la capa $k$ en base al gradiente de la capa $k+1$ (de aquí el nombre \emph{backward propagation}).
\newline \pause

Lo anterior en su forma matricial
\[
\delta^{(k)} = f'(u^{(k)}) * \left ( \delta^{(k+1)} \otimes  \hspace{0.1cm} (W^{(k+1)})^T \right ).
\] \pause
Lo único que queda para presentar el algoritmo final es calcular los gradientes en la última capa (capa de output)

\end{frame}

\begin{frame}{Backpropagation - Capa de salida}
Asumiendo el problema de regresión con salida escalar función de error MSE, tenemos \pause
\[
\delta_{d1}^{(l)} = \frac{\partial J_d}{\partial u_{d1}^{(l)}} = (\hat{y}_d-y_d) (\hat{y}_d)'.
\] \pause
Además, la función de activación en la salida es lineal y por tanto $(\hat{y}_d)' = 1$, finalmente el término de normalización $N$ se agrega en este paso: \pause
\[
\delta_{1}^{(l)} = \frac{1}{N}(\hat{y}-y)
\]
\pause
\begin{propuesto}
\begin{itemize}
  \item Encontrar una expresión para el gradiente de los bias $(b^{(k)})_k$
  \item Calcular el gradiente para un problema con unidad de output sigmoidal (problema de clasificación binario) o para un problema con unidad de salida softmax (problema de clasificación multiclase) 
\end{itemize}
\end{propuesto}


\end{frame}

\begin{frame}{Backpropagation - Algoritmo}

\begin{enumerate}


  \item Calcular la fase \emph{forward}, guardar los valores ($\hat{y}$) , $(u^{(k)})$ y $(h^{(k)})$. \pause


  \item Evaluar el error de la última capa $\delta_1^{(l)}$ según la unidad de output \pause
  
  \item Actualizar $U \gets U - \lambda \frac{\partial J}{\partial U}$ \pause
  
  \item Actualizar $c \gets c - \lambda \frac{\partial J}{\partial c}$ \pause

  \item Para $k=l, \dots ,1$ capas de la red \pause

  \begin{itemize}
    \item Propagar hacia atrás y calcular el error $\delta^{(k)}$ de las ecuaciones en la capa oculta  \pause
    \item Evaluar las derivadas parciales $\frac{\partial J}{\partial W^{(k)}}$ y $\frac{\partial J}{\partial b^{(k)}}$ para todos los nodos de la capa $k$ mediante las ecuaciones preliminares, guardar los valores \pause


    \item Actualizar los pesos y bias mediante descenso de gradiente 
    \[
    W^{(k)} \gets W^{(k)} - \lambda \frac{\partial J}{\partial W^{(k)}}, \quad b^{(k)} \gets b^{(k)} - \lambda \frac{\partial J}{\partial b^{(k)}}
    \] \pause 

  \end{itemize}

\end{enumerate}

\begin{observacion}

 La cantidad de veces que se pase por todos los datos de entrenamiento se conoce como \textbf{épocas}

\end{observacion}

\end{frame}

\section{Regularización para una Red Neuronal}

\begin{frame}{Regularización}
Las redes neuronales, recientemente referidas como \textit{deep learning} por su cantidad de capas, son aplicadas a tareas desafiantes como el procesamiento de imágenes, audio, y texto. \\~\

Controlar la complejidad de un modelo no solo se reduce a encontrar la topología de la red y sus parámetros, sino que en la práctica  modelo con el mejor ajuste es, en general, un modelo profundo apropiadamente regularizado. \\~\

El objetivo de las técnicas de regularización es el de reducir el \textit{error de generalización}, es decir, el error esperado al clasificar datos nuevos pero manteniendo la capacidad del modelo (profundidad, cantidad de nodos, funciones de activación, etc...) \\~\ 

A continuación veremos algunas de ellas: 


\end{frame}


\begin{frame}{Técnicas de regularización}

\begin{itemize}
  \item \textbf{Regularización $L^2$} \\ \pause
  Se basa en limitar la norma de los parámetros del modelo es la ya conocida \textbf{regularización} $\bm{L}^{2}$ (o \textbf{ridge regression}), mediante la cual se obtiene la función objetivo regularizada $\tilde{J}$: \pause

  \begin{equation*}
  \tilde{J}(\bm{\theta};\bm{X},\bm{y}) = J(\bm{\theta};\bm{X},\bm{y}) + \frac{\alpha}{2}||\bm{\theta}||^{2}_{2}.
  \end{equation*} \pause

  No es difícil notar que esta regularización es equivalente a la actualización de parámetros según gradiente estocástico de la forma \pause

  \begin{equation*}
  W^{(k)} \gets (1-\beta) W^{(k)} - \lambda \frac{\partial J}{\partial W^{(k)}}, \quad \beta = \lambda \alpha
  \end{equation*}
  y por eso también es llamada \textit{weight decay}.

\end{itemize}

\end{frame}

\begin{frame}
\begin{itemize}
  \item \textbf{Dropout} \\ 

  Esta técnica de regularización consiste en entrenar en cada iteración una fracción de los pesos mediante una elección aleatoria. Su implementación es bastante simple y basta con definir para cada capa $k$ la probabilidad $1-p_k$ de \emph{apagar} una neurona en particular. \pause

    \[
    h^{(k)} = f^{(k)}(h^{(k-1)}W^{(k)} + b^{(k)}) *  \frac{M^{(k)}}{p_k}, \quad M^{(k)}_i \sim \text{Bernoulli}(p_k) \hspace{0.2cm} \forall i,k.
    \]

  Notar que al aplicar backward, también es necesario 'apagar' las neuronas que no participaron del forward para evitar que sean entrenadas. \pause

  \item \textbf{Early Stopping} \pause

  La cantidad de épocas disminuye el error de entrenamiento pero no siempre el de generalización, es más, con una cantidad grande de épocas, el error de generalización aumenta y esto es porque la red aprende con exactitud cada input en el conjunto de train y no sus características más importantes, a esto se le denomina \textbf{overfitting}. \\ \pause

  La técnica de \textit{Early stopping} consiste en evitar esto, deteniendo el algoritmo antes de llegar al overfitting según un parámetro de decisión. Es importante destacar que esto se realiza evaluando sobre el conjunto de validación. \pause



\end{itemize}


\end{frame}

\begin{frame}
  \titlepage
\end{frame}






%Quitar de comentarios apenas se agregue alguna referencia 
%\bibliography{../capitulos/referencias} %Bibliografía
%\bibliographystyle{apacite}
\end{document} 