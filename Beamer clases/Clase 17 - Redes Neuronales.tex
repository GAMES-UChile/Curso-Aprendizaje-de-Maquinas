\documentclass[9pt]{beamer}
%\include{config/commands}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\input{preambuloBeamer}
\usetheme{simple}

% \usepackage[T1]{fontenc} %Este es el mismo utilizado en el tex de clases salvo modificaciones
\title{Clase 17 - Redes Neuronales II}
\subtitle{Aprendizaje de Máquinas - MA5204}
\date{\today}
\author{Felipe Tobar} 
\titlegraphic{
\begin{figure}[htp] 
    \centering
        \includegraphics[width=0.15\textwidth]{../img/Uchile.pdf}%
    %\hspace{2em}% 
    %    \includegraphics[width=0.15\textwidth]{img/CMM.pdf}%
         
\end{figure}
}
\institute{Department of Mathematical Engineering \&\\ Center for Mathematical Modelling\\Universidad de Chile}
%-------------------------------------------
% Inicio del documento, no tocar la config. de portada
%-------------------------------------------
\begin{document}
% Portada
\begin{frame}
  \titlepage
\end{frame}
% Tabla de contenidos
\begin{frame}{Contenido}
  \tableofcontents
  
\end{frame}
% Portada de seccion
\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par
  \end{beamercolorbox}
  \vfill
  \end{frame}}

%tcolorbox
%-------------------------------------------
% Contenido
%-------------------------------------------
% Nueva sección, o capitulo, tiene diapo de portada propio, basta con ponerla fuera del entorno frame
\section{Entrenamiento de una Red Neuronal}

\begin{frame}{Forward propagation}



\end{frame}

\begin{frame}{Backpropagation - Introducción}

\end{frame}

\begin{frame}{Backpropagation - Preliminares}

\begin{itemize}
  \item Utilicemos la notación para el paso intermedio antes de aplicar la función de activación como $u = hW + b$ \pause

  \item Supongamos además que el entrenamiento de la red se realiza con un paquete de datos (batch), es decir $(x^d)_{d=1}^N$  conjunto de $N$ inputs, luego $u_{dj}^k$ corresponde al valor de $u$ para el input $d$ en el nodo j para la capa $k$ \pause
 
 \item Se verá el algoritmo de backpropagation utilizando funciones de activación sigmoide y para función de error MSE. \pause

\end{itemize} 
\[
J(X , \theta) = \frac{1}{2N}\sum_{d=1}^N(f(X,\theta)_d-y_d)^2
\]
Nuestro objetivo será actualizar todos los valores $w_{ij}^k$ (peso del nodo $i$ al nodo $j$ en la capa $k$). Es decir, para utilizar el método del descenso de gradiente es necesario calcular $\frac{\partial J(X , \theta) }{\partial w_{ij}^k}$ notemos que \pause
\[
\frac{\partial J(X , \theta) }{\partial w_{ij}^k} = \frac{1}{N}\sum_{d=1}^N \frac{\partial}{\partial w_{ij}^k} \left ( \frac{1}{2}(\hat{y}_d-y_d)^2 \right) = \frac{1}{N}\sum_{i=1}^N \frac{\partial J_d}{\partial w_{ij}^k}
\]
Veamos esto último
\end{frame}

\begin{frame}{Backpropagation - Preliminares}

Utilizando la regla de la cadena 
\[
\frac{\partial J_d}{\partial w_{ij}^k} = \frac{\partial J_d}{\partial u_{dj}^k}\frac{\partial u_{dj}^k}{\partial w_{ij}^k}
\] \pause
La expresión $\frac{\partial J_d}{\partial u_{dj}^k}$ corresponde a un término de \textbf{error} y lo denotaremos \
\[
\delta_{dj}^k \equiv \frac{\partial J_d}{\partial u_{dj}^k}
\] \pause
Mientras que para el otro término tenemos que 
\[
\frac{\partial u_{dj}^k}{\partial w_{ij}^k} = \frac{\partial}{\partial w_{ij}^k} \left ( \sum_{a = 1}^{k_k}w_{aj}^kh_{da}^{k-1} + b_j^k \right) = h_{di}^{k-1}
\] \pause
y así 
\[
\frac{\partial J_d}{\partial w_{ij}^k} = \delta_{dj}^k  h_{di}^{k-1}
\] \pause
El gradiente final, será la suma de todos los gradientes y que expresaremos en su forma matricial
\[\frac{\partial J}{\partial w_{ij}^k} = \sum_{d=1}^N \delta_{dj}^k  h_{di}^{k-1}  \Rightarrow  \frac{\partial J}{\partial W^k} = (h^{k-1})^T @ \hspace{0.1cm} \delta^{k} 
\]
Omitiremos la constante $1/N$ hasta el final del algoritmo.
\end{frame}
\begin{frame}{Backpropagation - Capas ocultas}
Nuevamente, de la regla de la cadena \pause

\[
\delta_{dj}^k = \frac{\partial J_d}{\partial u_{dj}^k} = \sum_{a=1}^{k_{k+1}} \frac{\partial J_d}{\partial u_{da}^{k+1}} \frac{\partial u_{da}^{k+1}}{\partial u_{dj}^k} = \sum_{a=1}^{k_{k+1}} \delta_{da}^{k+1} \frac{\partial u_{da}^{k+1}}{\partial u_{dj}^k} 
\] \pause 
no es difícil ver que

\[
\frac{\partial u_{da}^{k+1}}{\partial u_{dj}^k} = w_{ja}^{k+1}f'(u_{dj}^k)  \Rightarrow  \delta_{dj}^k = f'(u_{dj}^k)\sum_{a=1}^{k_{k+1}}w_{ja}^{k+1}\delta_{da}^{k+1}
\]
\pause
Hemos encontrado una expresión para calcular el gradiente en una capa $k$ en base al gradiente de la siguiente capa $k+1$ (De aquí viene el nombre backward propagation).
\newline \pause

Lo anterior en su forma matricial resulta en 
\[
\delta^{k} = f'(u^k) * \left ( \delta^{k+1} @ \hspace{0.1cm} (W^{k+1})^T \right )
\] \pause
Lo único que queda para presentar el algoritmo final es calcular los gradientes en la última capa (capa de output)

\end{frame}

\begin{frame}{Backpropagation - Capa de output}
Estamos suponiendo un problema de regresión por lo que el output será de una sola salida
\[
\delta_{d1}^l = \frac{\partial J_d}{\partial u_{d1}^k} = (\hat{y}_d-y_d) (\hat{y}_d)' 
\] 
Además, la función de activación en el output será lineal y por tanto $(\hat{y}_d)' = 1$, finalmente el término de normalización $N$ se agrega en este paso.  La forma matricial queda en 
\[
\delta_{1}^l = \frac{1}{N}(\hat{y}-y)
\]
\begin{propuesto}
\begin{itemize}
  \item Encontrar una expresión para el gradiente de los bias $(b^{k})_k$
  \item Calcular el gradiente para un problema con unidad de output sigmoidal (problema de clasificación binario) o para un problema con unidad de output softmax (problema de clasificación multiclase) 
\end{itemize}
\end{propuesto}


\end{frame}

\begin{frame}{Backpropagation - Algoritmo}


\end{frame}






%Quitar de comentarios apenas se agregue alguna referencia 
%\bibliography{../capitulos/referencias} %Bibliografía
%\bibliographystyle{apacite}
\end{document} 