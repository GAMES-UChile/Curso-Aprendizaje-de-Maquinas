%!TEX root = deepLearningBeamer.tex
\documentclass[9pt]{beamer}
\include{config/commands}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{ \usetheme{Madrid}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{beaver} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
}

% \usepackage[T1]{fontenc}
\input{preambuloBeamer} %Este es el mismo utilizado en el tex de clases salvo modificaciones

\title[Aprendizaje de Máquinas]{ Redes Neuronales\\
\textit{Aprendizaje de Máquinas}}
\author[]{Felipe Tobar}
\institute{CMM - Universidad de Chile}
\date{\today}
%-------------------------------------------
% Inicio del documento, no tocar la config. de portada
%-------------------------------------------
\begin{document}
% Portada
\begin{frame}
  \titlepage
\end{frame}
% Tabla de contenidos
\begin{frame}{Contenido}
  \tableofcontents
  
\end{frame}
% Portada de seccion
\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par
  \end{beamercolorbox}
  \vfill
  \end{frame}}

%tcolorbox
%-------------------------------------------
% Contenido
%-------------------------------------------
% Nueva sección, o capitulo, tiene diapo de portada propio, basta con ponerla fuera del entorno frame

\section{Redes Neuronales - Introducción y Arquitectura}
\begin{frame}{Redes Neuronales - Introducción}
Una \textit{Red Neuronal} es un modelo computacional basado en la conexión de múltiples unidades (neuronas) cuyo objetivo es aproximar una función $f^{*}$. \pause
\newline 

\begin{columns}

  \begin{column}{0.5\textwidth}

  Su principal ventaja radica en la posibilidad de entrenarla con datos crudos, es decir, no se presentan las características relevantes de la data y en vez, se permite que el algoritmo aprenda cuales son. \pause

  \vspace{0.2cm}
  Los modelos escenciales de redes neuronales se conocen como \textbf{feedforward neural networks}, o \textbf{multilayer perceptrons} (MLPs), Una red feedforward define un mapping $\bm{y}=f(\bm{x}; \bm{\theta})$ y aprende los parámetros $\bm{\theta}$ que resultan en la mejor aproximación posible. \pause 
  \vspace{0.2cm}

  Estos modelos se conocen como redes ya que son típicamente el resultado de composiciones sucesivas de varios tipos de funciones $f^{(1)}$, $f^{(2)}$, $f^{(3)}, \dots$. \pause 

  \end{column}

  \begin{column}{0.5\textwidth}
  \begin{figure}[H]
  \centering
  \includegraphics[scale=.4]{../img/F1NN1L.png}
  \caption{Ejemplo de una red neuronal de 1 capa: (\textit{izquierda}). \\  Representación vectorial de las capas. (\textit{derecha})}
  \end{figure}

  \end{column}

\end{columns}



\end{frame}

\begin{frame}{Redes Neuronales - El perceptrón}

El \textit{perceptrón} corresponde a la forma más básica de una red neuronal, esta recibe un input numérico $x = (x_i)_{i=1}^n \in \R^n$ y computa la suma ponderada $u = x_1w_1 + x_2w_2 + \dots + w_nx_n + b$ donde $W = (w_i)_{i=1}^n \in \R^n$ corresponden a los \textbf{pesos} (weights) y $b \in \R$ el \textbf{sesgo} (bias). \pause 

A continuación, se aplica una función de activación $f$ y se entrega un output $h = f(u)$. \pause 
\begin{figure}[H]
  \centering
  \includegraphics[scale=.3]{../img/cap5_activaciones}
  \caption{Algunos ejemplos de funciones de activación}
\end{figure}

\end{frame}

\begin{frame}{Redes Neuronales - El perceptrón}

Un conjunto de perceptrones conectados unos con otros en secuencia forman arquitecturas capaces de resolver problemas más complejos. \pause  

\begin{columns}

  \begin{column}{0.7\textwidth}

  \begin{figure}[H]
    \centering
    \includegraphics[scale=.3]{../img/cap7_xor}
    \caption{Red Neuronal para computar XOR}
  \end{figure}

  \end{column}

  \begin{column}{0.3\textwidth}
  
  XOR operator

  \scalebox{1.3}{
  \begin{tabular}{| c  c | c |}
    \hline
    $x_1$ & $x_2$ & $y$ \\ \hline
    0 & 0 & 0 \\
    1 & 0 & 1 \\
    0 & 1 & 1 \\ 
    1 & 1 & 0 \\ \hline
  \end{tabular}
  }

  

  \end{column}

\end{columns} \pause

En su forma matricial 
$$
(h_1 , h_2) = \text{Relu} \left ( \begin{pmatrix}
x_1 & x_2  
\end{pmatrix} \begin{pmatrix}
1 &1 \\ 
 1 & 1
\end{pmatrix} + \begin{pmatrix}
0 & -1 
\end{pmatrix}\right) \hspace{0.5cm} h = \text{Relu}(xW + b)
$$
\pause
$$
y = \begin{pmatrix}
h_1 & h_2 
\end{pmatrix}\begin{pmatrix}
1 \\ 
-2 
\end{pmatrix} + (0) \hspace{0.5cm} y = hU + c
$$
Donde $U$ y $c$ corresponden al peso y bias de la última capa respectivamente. 
\textbf{¿Cómo construir el resto de operadores lógicos?}
\end{frame}


%Quitar de comentarios apenas se agregue alguna referencia 
%\bibliography{../capitulos/referencias} %Bibliografía
%\bibliographystyle{apacite}
\end{document} 