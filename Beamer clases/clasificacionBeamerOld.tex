\documentclass[9pt]{beamer}
\include{config/commands}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{ \usetheme{Madrid}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{beaver} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
}

% \usepackage[T1]{fontenc}
\input{preambuloBeamer} %Este es el mismo utilizado en el tex de clases salvo modificaciones

\title[Aprendizaje de Máquinas]{ Clasificación\\
\textit{Aprendizaje de Máquinas}}
\author[]{Felipe Tobar}
\institute{CMM - Universidad de Chile}
\date{\today}
%-------------------------------------------
% Inicio del documento, no tocar la config. de portada
%-------------------------------------------
\begin{document}
% Portada
\begin{frame}
  \titlepage
\end{frame}
% Tabla de contenidos
\begin{frame}{Contenido}
  \tableofcontents
  
\end{frame}
% Portada de seccion
\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par
  \end{beamercolorbox}
  \vfill
  \end{frame}}

%tcolorbox
%-------------------------------------------
% Contenido
%-------------------------------------------
% Nueva sección, o capitulo, tiene diapo de portada propio, basta con ponerla fuera del entorno frame

\section{Problema de Clasificación}
\begin{frame}{Introducción - Problema de Clasificación}
El problema de clasificación dice relación con la identificación del conjunto, categoría o \emph{clase} a la cual pertenece un elemento en base a sus \emph{características}. \vspace{0.5cm} \pause

\begin{columns}

  \begin{column}{0.5\textwidth}
    En el contexto del aprendizaje supervisado, puede ser visto como un problema de regresión. \vspace{0.5cm} \pause

    En efecto, basta suponer que $y$ variable de salida (o variable dependiente) es \emph{categórica} y usualmente denotada por $\{ 0 , 1 \}$ en el caso binario o para el caso multiclase $\{ 1 \dots K \}$ \vspace{0.5cm} \pause

    Entre los métodos de clasificación existentes, trabajaremos en 

    \begin{enumerate}
      \item \emph{k} vecinos más cercanos \pause
      \item Regresión Logística \pause
      \item Support Vector Machines (SVM) \pause

    \end{enumerate}


  \end{column}

  \begin{column}{0.5\textwidth}
    \centering
    \includegraphics[width=5cm]{../img/cap2_dosclases.pdf}\\
    \captionof{figure}{Ejemplo del problema  de clasificación binaria, donde la clase $\cC_1$ está presentada en azul y la clase $\cC_2$ en rojo.}
    \label{fig:puntos_2d}
  \end{column}


\end{columns}


\end{frame}

\section{Clasificación Lineal}

\begin{frame}{Clasificación Lineal - Binaria}

Consideremos el caso binario K=2 clases, proponemos un modelo lineal para relacionar la variable independiente con su clase, es decir, $y(x) = a^Tx+b$ tal que  $x \in \cC_1$ si $y(x) \geq 0$, en caso contrario,  $x \in \cC_2$. \pause

Para encontrar los parámetros $a,b$ óptimos, sean $x_1$ y $x_2$ en la región de decisión $y(x)=0$ \pause  
\begin{align*}
  0 &= y(x_1) - y(x_2) \nonumber\\ 
    &= a^\top x_1 + b - a^\top x_2 - b \nonumber\\ 
    &= a^\top (x_1-x_2).
\end{align*}
\pause
Además, observemos que para cualquier $x$ en la región de decisión se tiene que 
\begin{equation*}
  \norm{\text{proy}_a(x)} = \norm{x}cos(\theta) = \norm{x} \frac{a^\top x}{\norm{a}\cdot\norm{x}} = -\frac{b}{\norm{a}}
\end{equation*}

Con lo que tenemos una intepretación geométrica de ambos parámetros. 

\end{frame}

\begin{frame}{Clasificación Lineal - Binaria}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{../img/cap3_proy.pdf}
    \caption{Proyección de un punto sobre la región de decisión. }
\end{figure}


\end{frame}

\begin{frame}{Clasificación Lineal - Binaria}

También es posible interpretar $y(x)$ como una distancia con signo entre un $x\in\R^M$ cualquiera y la superficie de decisión. \pause

Para ver esto, consideremos $x\in\R^M$ y descompongámoslo en dos componentes: la primera denotada por $x_{\bot}$, la cual es la proyección ortogonal de $x$ en el hiperplano de  decisión, y la segunda que es perpendicular al hiperplano (y consecuentemente paralela al vector $a$) denotada por $r\frac{a}{\left \| a \right \|}$, donde $r$ denota la distancia (positiva o negativa) entre $x$ y el  hiperplano de  decisión. Expresamos entonces  \pause

\begin{equation*}
  x = x_{\bot}+r\frac{a}{\left \| a \right \|},
\end{equation*}

y observamos que \pause

\begin{equation*}
  y(x) 
  = a^\top x+b 
  =a^\top  \left( x_{\bot} + r\frac{a}{\left \| a \right \|} \right) +b 
  = \underbrace{a^\top x_{\bot} +b }_{=0} +   r\frac{a^\top  a}{\left \| a \right \|}
  = r||a||.
\end{equation*}

\pause 

Luego $r = \frac{y(x)}{\norm{a}}$ y como $r$ es una medida con signo, $y(x)$ también lo es. 


\end{frame}

\begin{frame}{Clasificación Lineal - Múltiples clases}

El caso de múltiples clases ($K>2$) puede ser enfrentado mediante una extensión del caso binario, algunas de ellas 

\begin{enumerate}

  \item \textbf{One versus rest: } Construcción de $K-1$ clasificadores binarios que discrimina una clase $\cC_k$ del resto \pause


  \item \textbf{One versus one: } Construccion de $K(K-1)/2$ clasificadores binarios que discriminan entre cada par posible de clases \pause

\end{enumerate}

\textbf{¿Qué problema presentan estos métodos?} \pause Busquemos otra forma \pause 

\vspace{0.5cm}

Una alternativa más robusta para resolver el problema de clasificación multiclase es construir un clasificador para $K$ clases que contiene $K$ funciones lineales de la forma 
\begin{equation*}
  y_k(x) = a_k^\top x + b_k, \quad k=1,\ldots,K.
\end{equation*}
\pause 
Donde $x$ es asignado a la clase $\mathcal{C}_k$ si y solo si $y_k(x) > y_j(x), \forall j\neq k$, es decir: 

\begin{equation*}
  \mathcal{C}(x) = \underset{k}{\argmax}\hspace{1mm} y_j(x).
\end{equation*}

\pause 
\textbf{¿Qué ventajas posee este método?}

\end{frame}

\section{Ajuste mediante mínimos cuadrados}

\begin{frame}{Ajuste mediante mínimos cuadrados}

Ya hemos planteado el modelo y analizado el rol  y significado de cada uno de sus parámetros; ahora queda por estudiar cómo determinar dichos parámetros $a$ y $b$, dado un conjunto de datos $\datos$.
\newline \pause 
Consideremos el  punto $x\in\R^M$ con clase $c\in\{\mathcal{C}_k\}_{k=1}^K$. Usaremos la \emph{codificación} $t \in\{0,1\}^K$ para representar la pertenencia de $x$ a su respectiva clase. Es decir, \pause
\begin{equation*}
  c = \cC_j \Leftrightarrow [t]_j=1 \wedge [t]_i=0, \quad \forall i\neq j.
\end{equation*}
Este tipo de codificación  es conocida como \emph{one-hot  encoding.} \textbf{¿Por qué la usamos?} \pause \newline
Asumiendo entonces un modelo lineal para cada clase $\mathcal{C}_k$, se tiene que
\begin{equation*}
  y_k(x) = a_k^\top x + b_k = \tilde{\theta}_k^\top\tx, \quad \text{donde } \tx = \left( \begin{matrix} x\\ 1 \end{matrix}\right)\in\R^{M+1} ,\quad
  \tilde{\theta}_k = \left( \begin{matrix} a_k\\ b_k \end{matrix}\right)\in\R^{M+1}
\end{equation*}
\newline \pause
Lo anterior se puede unir en un único sistema matricial:

\begin{equation*}
\tilde{\Theta} = \left(\theta_1,\cdots,\theta_K\right)\in\R^{(M+1)\times K} \implies  y(x) = \left( \begin{matrix}y_1(x) \\\vdots \\ y_K(x) \end{matrix}\right) = \tilde{\Theta}^\top \tilde{x}.
  \end{equation*}




\end{frame}

\begin{frame}{Ajuste mediante mínimos cuadrados}
Con la notación establecida, ahora podemos enfocarnos en el entrenamiento del modelo.  Para esto consideremos un conjunto de entrenamiento $\{(x_n,t_n)\}_{n=1}^N$. El enfoque de entrenamiento será el correspondiente a mínimos cuadrados asociado al error de asignación:

\begin{equation*}
  J = \sum_{i=1}^N \norm{t_i - \tilde{\Theta}^\top\tx_i}_2^2
\end{equation*}  \pause

Por otra parte, definiendo las siguientes matrices:

\begin{equation*}
  T = \left(\begin{matrix}
    t_1^\top\\
    \vdots\\
    t_N^\top
  \end{matrix}\right)\in\R^{N\times K},\qquad
  \tX = \left(\begin{matrix}
    t_1^\top\\
    \vdots\\
    t_N^\top
  \end{matrix}\right)\in\R^{N\times (M+1)}
\end{equation*}

\pause

se tiene el siguiente resultado:

\end{frame}

\begin{frame}{Ajuste mediante mínimos cuadrados}

\begin{lemma}
  Bajo la notación anterior, $J=\operatorfont{Tr} \left((\tX\tilde{\Theta}-T)^\top (\tX\tilde{\Theta}-T)\right)$ y su mínimo es alcanzado en: 
  \begin{equation*}
    \tilde{\Theta} = (\tilde{X}^\top\tilde{X})^{-1}\tilde{X}^\top T
  \end{equation*}
  donde $\operatorfont{Tr}$ corresponde al operador traza: $A\in\R^{n\times n}\mapsto \operatorfont{Tr}(A):=\sum\limits_{i=1}^n a_{ii}$.
\end{lemma}


\end{frame}

\begin{frame}{Ajuste mediante mínimos cuadrados}

\begin{proof}
\begin{align*}
  J &= \sum_{i=1}^N \norm{t_i - \tilde{\Theta}^\top\tx_i}_2^2 = \sum_{i=1}^N \norm{\left(T - \tX\tilde{\Theta}\right)_{i\cdot}}_2^2 = \sum_{i=1}^N \sum_{j=1}^K \left(T - \tX\tilde{\Theta}\right)_{ij} \left(T - \tX\tilde{\Theta}\right)_{ij}\\
  &= \sum_{i=1}^N \sum_{j=1}^K \left(T - \tX\tilde{\Theta}\right)^\top_{ji} \left(T - \tX\tilde{\Theta}\right)_{ij} =  \sum_{j=1}^K \left[\left(T - \tX\tilde{\Theta}\right)^\top \left(T - \tX\tilde{\Theta}\right)\right]_{jj}\\
  &=\operatorfont{Tr} \left((\tX\tilde{\Theta}-T)^\top (\tX\tilde{\Theta}-T)\right).
\end{align*}
  
Por otra parte:
\begin{equation*}
  \frac{\partial J}{\partial\tilde{\Theta}} = 2(\tX\tilde{\Theta}-T)^\top\tX=0 \iff \tilde{\Theta}^\top\tX^\top\tX - T^\top\tX = 0 
\end{equation*}
\begin{equation*}
\iff \tilde{\Theta}^\top = T^\top\tX(\tX^\top\tX)^{-1}\iff \tilde{\Theta} = (\tX^\top\tX)^{-1}\tX^\top T
\end{equation*}
  
Y dado que $J$ es estrictamente convexo, su mínimo se alcanza en su único punto crítico.  

\end{proof}


\end{frame}

\begin{frame}{Ajuste mediante mínimos cuadrados}
Problemáticas conceptuales de este enfoque: 
\begin{enumerate}
  \item Sensibilidad a presencia de puntos aislados (outliers) \pause
  \item Intervención ''manual'' de las etiquetas \pause
\end{enumerate}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../img/cap2_dosclases_clasificador.pdf}\\
  \caption{Ejemplo ilustrativo sobre cómo los puntos lejanos de una clase pueden afectar incorrectamente los resultados.}
  \label{fig:clasif_mse}
\end{figure}


\end{frame}


\section{El perceptrón}

\begin{frame}{El perceptrón - Introducción}
Las nociones básicas que hemos visto hasta ahora para lidiar con el problema de clasificación tienen dos problemas conceptuales. \pause
\begin{enumerate}
  \item Falta de una métrica correcta \pause
  \item No existe una \emph{función de verosimilitud} apropiada \pause
\end{enumerate}
La incorporación de esta función que conecta el modelo lineal con la clase, resulta en un \emph{modelo lineal generalizado}, es decir, una modelo lineal conectado a una  función no-lineal que llamaremos \emph{función de enlace}.\\ \pause
Sin embargo, el desafío más importante en esta construcción es que el modelo resultante ya no es lineal, ni en la entrada ni en los parámetros , pues una verosimilitud (función de enlace) lineal nunca nos llevará de un espacio de inputs (hemos asumido $\R^M$) al espacio de categorías $\{\cC_1,\cC_2,\ldots,\cC_k\}$, es decir, necesitamos una no-linealidad ``después'' de la parte lineal \\ \pause

Una forma de resolver estas problemáticas es mediante el uso del \emph{Perceptrón} \cite{rosenblatt_1958}, un modelo de clasificación binario que tuvo mucha importancia en el área de reconocimiento de patrones.

\end{frame}

\begin{frame}{El perceptrón}

El Perceptrón consiste en una función no lineal fija usada para transformar $x$ en un vector de características\footnote{En este caso consideramos no linealidad antes y después de la parte lineal, sin embargo, considerar la entrada como $x$ o  como $\phi(x)$ es  equivalente en base a lo  visto en los modelos lineales en los parámetros. } $\phi(x)\in\R^D$, que luego es usado para generar un modelo lineal \emph{generalizado} con función de enlace no lineal $f(\cdot)$ de la siguiente forma:
\begin{align*}
  y(x) &= f(\theta^\top\phi(x))\\
  f(u) &= \left\{\begin{matrix}
  +1,\quad u\geq 0\\
  -1,\quad u<0
  \end{matrix}\right.
\end{align*} \pause
El Perceptrón entonces asigna $x$ a la clase $\mathcal{C}_1$ si $y(x)=+1$ y asignará $x$ a la clase $\mathcal{C}_2$ cuando $y(x)=-1$. Notemos que  para  el caso que $\phi$ es lineal, este es el mismo clasificador presentado en la Sección de clasificación lineal, pero en este caso el criterio para asignar la clase es \textbf{parte del modelo}. \pause

Usando el hecho que las etiquetas están representadas por la  codificación $t\in\{1,-1\}$, la condición de asignación puede ser cubiertas por la expresión:
\begin{equation*}
  \theta^\top\phi(x_n)t_n > 0,\quad \forall (x_n,t_n) \in \datos.
\end{equation*}

\end{frame}

\begin{frame}{El perceptrón}
Podemos entonces satisfacer esta restricción mediante el ``criterio del perceptrón'', el cual se basa en examinar  los elementos de $\datos$ que fueron clasificados incorrectamente. Este criterio asocia a los puntos clasificados correctamente error 0 y a los puntos mal clasificados error $-\theta^\top\phi(x)t>0$. De esta forma, si denotamos $\mathcal{M}$ el conjunto de puntos mal clasificados, se debe minimizar la siguiente función objetivo: \pause 

\begin{equation*}
  J_\text{P}(\theta,x) = \E\left(-\theta^\top\phi(x)t(x)\mathds{1}_{\theta^\top\phi(x)t(x)\leq 0} \right) 
\end{equation*} \pause
\begin{equation*}
\approx -\sum_{(x_i,t_i)\in \mathcal{D}}\theta^\top\phi(x_i)t_i \mathds{1}_{\theta^\top\phi(x_i)t_i\leq 0} = -\sum_{(x_i,t_i)\in \mathcal{M}}\theta^\top\phi(x_i)t_i
\end{equation*} 

Para el problema de minimización del funcional del perceptrón, se puede utilizar el método del gradiente estocástico (ver anexo). 

\end{frame}

\begin{frame}{El perceptrón}

En este caso, el algoritmo iterativo tiene la siguiente estructura: \pause
\begin{align*}
  \theta^{\tau+1} &= \theta^\tau - \eta_\tau \nabla_\theta J_\text{P}(\theta^\tau,x_i)\nonumber\\
  &= \theta^\tau + \eta_\tau \phi(x_i)t_i.\label{eq:percetron_rule}
\end{align*}\pause
Es importante notar que al actualizar el vector $\theta$, el conjunto de puntos mal clasificados $\mathcal{M}$ va a cambiar, pues (esperamos que) en cada iteración los elementos del conjunto de puntos mal clasificados vaya disminuyendo.\\

Por lo tanto, el algoritmo de entrenamiento para el perceptrón es el siguiente: \pause

\begin{itemize}
  \item[i)] se recorre el conjunto de puntos de entrenamiento $\{x_i\}_{i=1}^N$, \pause
  \item[ii)] si el punto $x_i$ fue clasificado correctamente el vector de pesos de mantiene igual \pause
  \item[iii)] si $x_i$ fue clasificado incorrectamente, el vector $\theta^\tau$ es actualizado según la ecuación anterior con $\eta=1$ mediante
  \begin{equation*}
   \theta^{\tau+1} = \theta^\tau + \phi(x_i)t_i.
  \end{equation*} 
\end{itemize}
\pause
Es decir, el parámetro $\theta$ está paso a paso modificado en la dirección de las características $\phi(x_i)$ con multiplicador $\pm1$ en base a la clase verdadera de $x_i$ hasta  que todos los puntos de $\datos$ están bien clasificados.


\end{frame}
\section{Clasificación probabilística: modelo generativo}

\begin{frame}{Modelo generativo}
Los modelos que hemos revisado hasta este punto son del tipo \emph{discriminativo}, es decir, modelan directamente la función $f:x\mapsto c$. Con una interpretación probabilística, esto es equivalente a modelar la probabilidad condicional $\mathbb{P}(\mathcal{C}_k|x)$, es decir, dado que conozco el input (o características de) $x$, cuál es la distribución de probabilidad sobre las clases. Sin embargo, hemos considerado métodos determinísticos, que solo asignan probabilidad 1 a una sola clase. \pause
\vspace{0.5cm}

Un paradigma alternativo es considerar es un enfoque \emph{generativo}, en el cual modelamos dos  objetos: en primer lugar la ``probabilidad condicional de clase'' la cual representa cómo distribuyen los valores de los inputs $x$ cuando la  clase es, por  ejemplo, $\cC_k$, denotada por $\mathbb{P}(x|\mathcal{C}_k)$. En segundo lugar las ``probabilidades de clase'', o el prior sobre clases, denotada $\mathbb{P}(\mathcal{C}_k)$. Luego, podemos calcular la densidad posterior sobre las clases dado un input $x$ usando el Teorema de Bayes de acuerdo a 
\begin{equation*}
  \mathbb{P}(\mathcal{C}_k|x) = \frac{\mathbb{P}(x|\mathcal{C}_k)\mathbb{P}(\mathcal{C}_k)}{\mathbb{P}(x)}.
\end{equation*}

\end{frame}

\begin{frame}{Modelo generativo}
Para el caso de 2 clases, se tiene el siguiente desarrollo: \pause 
\begin{align*}
  \mathbb{P}(\mathcal{C}_1|x) 
  &= \frac{\mathbb{P}(x|\mathcal{C}_1)\mathbb{P}(\mathcal{C}_1)}{\mathbb{P}(x)}\nonumber\\
  &= \frac{\mathbb{P}(x|\mathcal{C}_1)\mathbb{P}(\mathcal{C}_1)}{\mathbb{P}(x|\mathcal{C}_1)\mathbb{P}(\mathcal{C}_1)+\mathbb{P}(x|\mathcal{C}_2)\mathbb{P}(\mathcal{C}_2)}\nonumber\\
  &=\frac{1}{1+\frac{\mathbb{P}(x|\mathcal{C}_2)\mathbb{P}(\mathcal{C}_2)}{\mathbb{P}(x|\mathcal{C}_1)\mathbb{P}(\mathcal{C}_1)}}\nonumber\\
  &=\frac{1}{1+\exp(-r)} = \sigma(r).\label{eq:logistic1}
\end{align*} \pause 
Donde hemos introducido la notación $r = r(x) =\ln\left(\frac{\mathbb{P}(x|\mathcal{C}_1)\mathbb{P}(\mathcal{C}_1)}{\mathbb{P}(x|\mathcal{C}_2)\mathbb{P}(\mathcal{C}_2)}\right)$  y la  función logística definida mediante $\sigma(r) = \frac{1}{1+e^{-r}}$, la cual  tiene propiedades que serán útiles en el entrenamiento, en particular: \pause
\begin{align*}
  \text{reflejo: }\sigma(-r)&=1-\sigma(r)\\
  \text{derivada: }\frac{d}{dr}\sigma(r)&=\sigma(r)(1-\sigma(r))\\
  \text{inversa: }r(\sigma)&=\ln\left(\frac{\sigma}{1-\sigma}\right).
\end{align*}


\end{frame}

\begin{frame}{Modelo generativo}
Si bien la expresión de la distribución condicional en la ecuación anterior parece una presentación antojadiza para hacer aparecer la  función logística (sigmoide), pues $r=r(x)$ puede ser cualquier cosa. Sin embargo, veremos que existe una elección particular de las distribuciones condicionales de clase que lleva a un $r$ que es efectivamente lineal en $x$. En general, nos  referiremos a este clasificador como \textbf{regresión logística} en dicho caso, es decir, cuando $r(x) = a^\top x  + b$. \pause

Podemos ahora considerar el caso de múltiples clases $\{\cC_1,\ldots,\cC_K\}$, donde un desarrollo similar al anterior resulta en:  
\begin{equation*}
  \mathbb{P}(\mathcal{C}_i | x) = \frac{\mathbb{P}(x | \mathcal{C}_i)\mathbb{P}(\mathcal{C}_i)}{\sum_{j}\mathbb{P}(x | \mathcal{C}_j)\mathbb{P}(\mathcal{C}_j)} = \frac{\exp(s_i)}{\sum_{j}\exp(s_j)},\label{eq:softmax1}  
\end{equation*}
\pause
donde hemos denotado $s_i = \log\left(\mathbb{P}(x | \mathcal{C}_i)\mathbb{P}(\mathcal{C}_i)\right)$. La función que aparece al lado derecho de la ecuación se conoce como \emph{exponencial normalizada} o \emph{softmax}, y corresponde a una generalización de la función logística a múltiples clases. \\Además, esta función tiene la propiedad de ser una aproximación suave de la función máximo y convertir cualquier vector $s=[s_1,\ldots,s_k]$ en una distribución de probabilidad, donde podemos hablar de ``la probabilidad de ser clase $\cC_k$''.


\end{frame}

\section{Regresión Logística}
\begin{frame}{Regresión Logística}
Analizaremos ahora  los supuestos sobre el modelo generativo (i.e., las  probabilidades de clase y condicionales) para encontrar un $r$ que resulte en la bien conocida regresión logística. \pause 
Consideraremos el caso binario donde las densidades condicionales de clase son Gaussianas multivariadas, dadas por 
\begin{equation*}
  p(x|\mathcal{C}_k) \sim \mathcal{N} (\mu_k,\Sigma) = \frac{1}{(2\pi)^\frac{D}{2}|\Sigma|^\frac{1}{2}}\exp(-\frac{1}{2}(x-\mu_k)^\top \Sigma^{-1}(x-\mu_k))\quad k\in\{1,2\}.
\end{equation*} \pause 

Donde $u_k\in\R^M$ corresponde al centroide de la clase $\mathcal{C}_k$ y $\Sigma\in\R^{M\times M}$ simétrica y definida positiva, corresponde a la matriz de covarianza de las clases (misma matriz para todas las clases). Para este caso, se tiene que para la ecuación: $r = r(x) =\ln\left(\frac{\mathbb{P}(x|\mathcal{C}_1)\mathbb{P}(\mathcal{C}_1)}{\mathbb{P}(x|\mathcal{C}_2)\mathbb{P}(\mathcal{C}_2)}\right)$ 


\end{frame}

\begin{frame}{Regresión Logística}
\begin{align*}
r &= \ln\left(\frac{\mathbb{P}(x|\mathcal{C}_1)\mathbb{P}(\mathcal{C}_1)}{\mathbb{P}(x|\mathcal{C}_2)\mathbb{P}(\mathcal{C}_2)}\right) = \ln\left(\frac{\exp(-\frac{1}{2}(x-\mu_1)^\top \Sigma^{-1}(x-\mu_1))\mathbb{P}(\mathcal{C}_1)}{\exp(-\frac{1}{2}(x-\mu_2)^\top \Sigma^{-1}(x-\mu_2))\mathbb{P}(\mathcal{C}_2)}\right)\\
&= -\frac{1}{2}(x-\mu_1)^\top \Sigma^{-1}(x-\mu_1) +\frac{1}{2}(x-\mu_2)^\top \Sigma^{-1}(x-\mu_2) + \ln\left(\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\right)\\
&= \frac{1}{2}\left(x^\top\Sigma^{-1}(\mu_1-\mu_2) + (\mu_1-\mu_2)^\top\Sigma^{-1}x - \mu_1\Sigma^{-1}\mu_1 + \mu_2\Sigma^{-1}\mu_2 \right) + \ln\left(\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\right)\\
&= (\mu_1-\mu_2)^\top\Sigma^{-1}x + \frac{1}{2}\left(\mu_2\Sigma^{-1}\mu_2 - \mu_1\Sigma^{-1}\mu_1 \right)+ \ln\left(\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\right)\\
&= a^\top x+b
\end{align*} \pause

donde hemos usado la notación
\begin{align*}
a &= \Sigma^{-1}(\mu_1-\mu_2)\\
b &= \frac{1}{2}(\mu_2^\top \Sigma^{-1}\mu_2-\mu_1^\top \Sigma^{-1}\mu_1)
+\ln\left(\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\right). 
\end{align*}

\end{frame}

\begin{frame}{Regresión Logística}

Lo anterior nos entrega la regresión logística (lineal) para el  caso binario, donde al incorporar la expresión anterior en la ecuación logística obtenemos
\begin{equation*}
  p(\mathcal{C}_k|x) = \sigma(a^\top x+b) = \frac{1}{1 + \exp{\left(-a^\top x-b\right)}}.
\end{equation*} \pause
Ahora que hemos definido el modelo para nuestro problema de clasificación, aflora naturalmente la siguiente pregunta: ¿Cómo ajustar los parámetros de las condicionales a la clase y priors respectivamente? Para esto, reiteremos que los parámetros del modelos serán los de la probabilidad de clase $p(\cC_k)$ y de la probabilidades condicionales de clase $p(x|\cC_k)$. Respectivamente: \pause

\begin{itemize}
  \item Probabilidad de clase:
  \begin{equation*}
    p(\mathcal{C}_1)=\pi,\quad  p(\mathcal{C}_2)=1-\pi,
   \end{equation*}  es decir,  un parámetro $\pi$ (por determinar). \pause
  \item Probabilidad condicional de clase:
  \begin{equation*}
    p(x|\mathcal{C}_k) = \mathcal{N}(\mu_k,  \Sigma); k=1,2
  \end{equation*} 
  es decir, parámetros $ \mu_1\in\R^M,\mu_2\in\R^M,\Sigma\in\R^M\times\R^M$ (por determinar) o, equivalentemente, $M + M + M(M+1)/2=M(M+5)/2$ parámetros escalares (considerando que $\Sigma$ es simétrica). 
\end{itemize}
\pause
Denotaremos todos los parámetros mediante el parámetro agregado $\theta =\{\pi,\mu_1,\mu_2,\Sigma \}$.
\end{frame}
\begin{frame}{Regresión Logística}
Realizaremos el entrenamiento del modelo mediante el método de máxima verosimilitud. \pause
Consideremos la codificación donde la observación $(x_i,t_i)$ corresponde a clase $\cC_1$ con $t_i = 1$ y a clase $\cC_2$ con  $t=0$, podemos expresar la verosimilitud con una observación mediante:
\begin{equation*}
  L_i(\theta) = p(x_i, t_i|\theta) =  p(x_i,\cC_1|\theta)^{t_i}p(x_i,\cC_0|\theta)^{1-t_i}. 
\end{equation*} \pause
Para un conjunto de $\datos$ de la forma  
\begin{equation*}
X=\left(\begin{matrix}
    x_1^\top\\ \vdots\\ x_N^\top
  \end{matrix}\right)\in\R^{N\times M},\qquad
  T=\left(\begin{matrix}
    t_1\\ \vdots\\ t_N
  \end{matrix}\right)\in\{0,1\}^N \text{ es decir, codificación $0-1$.}
\end{equation*} \pause

podemos escribir la verosimilitud mediante $L(\theta) = p(X,T|\theta) $, luego:
\begin{align*}
  L(\theta) &= \prod_{i=1}^{N}p(x_i,t_i|\theta)
  = \prod_{i=1}^{N}p(x_i,\mathcal{C}_1|\theta)^{t_i}p(x_i,\mathcal{C}_0|\theta)^{1-t_i}\\
  &=\prod_{i=1}^{N}\left(p(x_i|\mathcal{C}_1,\theta)p(\mathcal{C}_1|\theta)\right)^{t_i}\left(p(x_i|\mathcal{C}_0,\theta)p(\mathcal{C}_0|\theta)\right)^{1-t_i}\\
  &= \prod_{i=1}^{N}(\pi\mathcal{N}(x_i|\mu_1,\Sigma))^{t_i}
  ((1-\pi)\mathcal{N}(x_i|\mu_2,\Sigma))^{1-t_i}.
\end{align*}


\end{frame}

\begin{frame}{Regresión Logística}
Nuestro interés se encuentra en la log-verosimilitud 
\begin{equation*}
  l(\theta) := \log L(\theta) 
\end{equation*} 
\begin{equation*}
= \sum_{i=1}^{N}\left(t_i(\log(\pi)+\log(\mathcal{N}(x_i|\mu_1,\Sigma)))+(1-t_i)(\log(1-\pi)+\log(\mathcal{N}(x_i|\mu_2,\Sigma)))\right)
\end{equation*} \pause
Aplicando condiciones de primer orden

\begin{itemize}
  
\item \textbf{1)} Con respecto a $\pi$:
  
  \begin{align}
  \frac{\partial\log(L)}{\partial\pi} &= \sum_{i=1}^N \frac{t_i}{\pi}-\frac{1-t_i}{1-\pi}=0\nonumber\\
  \Rightarrow \quad & (1-\pi)\sum_{i=1}^Nt_i = \pi\sum_{i=1}^N(1-t_i)\nonumber\\
  \Rightarrow \quad & \sum_{i=1}^Nt_i=\pi N \quad\Rightarrow\quad \pi = \frac{\sum_{i=1}^Nt_i}{N} = \frac{N_1}{N_1+N_2} \label{eq:log_reg_pi}
  \end{align}
  
  Donde $N_i:=\text{Card}(x:x\in\mathcal{C}_i)$. Por lo tanto, el EMV de $pi$ colapsa a la regla de Laplace.

\end{itemize}
\end{frame}

\begin{frame}{Regresión Logística}
\begin{itemize}
\item \textbf{2)} Con respecto a $\mu_1$:
  
  \begin{align*}
  \frac{\partial\log(L)}{\partial\mu_1} &= \sum_{i=1}^N t_i
  \frac{\partial}{\partial \mu_1}(-\frac{1}{2}(x_i-\mu_1)^\top \Sigma^{-1}(x_i-\mu_1))\nonumber\\
  &= \sum_{i=1}^N t_i(\Sigma^{-1}(x_i-\mu_1)) =
  \Sigma^{-1}\sum_{i=1}^N t_i(x_i-\mu_1) = 0\nonumber\\
  \Rightarrow\quad & \sum_{i=1}^Nt_ix_i= \mu_1\sum_{i=1}^N t_i
  \quad\Rightarrow\quad \mu_1  = \frac{1}{N_1}\sum_{i=1}^Nt_ix_i = \frac{1}{N_1}\sum_{x_i\in \mathcal{C}_1}x_i \label{eq:log_reg_mu1}
  \end{align*}
  
  De forma análoga:
  \begin{equation*}
  \mu_2 = \frac{1}{N_2}\sum_{x_i\in \mathcal{C}_2}x_i \label{eq:log_reg_mu2}
  \end{equation*} \pause

  Queda la siguiente pregunta, \textbf{¿Cuál es el estimador MV de $\Sigma$?}
  
\end{itemize}

\end{frame}

\section{Regresión Logística v/s modelo generativo}
\begin{frame}{Regresión Logística v/s modelo generativo}
Recordemos que los supuestos tomados sobre el modelo generativo para el problema de clasificación resultaron en:
\begin{equation*}
p(\mathcal{C}_1|x) = \sigma(w^\top x) = \frac{1}{1+e^{-w^\top x}}
\end{equation*}
donde por claridad de notación hemos elegido la representación lineal $(w^\top x)$ y no afín $(a^\top x + b)$. \pause
En el caso anterior se ha entrenado el modelo generativo completo, es decir, $\pi, \mu_1,\mu_2, \Sigma$, lo cual tiene la ventaja de tener solución en forma cerrada, sin embargo, puede ser innecesario cuando solo necesitamos conocer el peso $w$ en la ecuación anterior. \pause
Calculemos la verosimilitud de la regresión logística con datos $\datos=\{(x_i,t_i)\}_{i=1}^N$, para hacer la notación más compacta denotamos $\sigma_i = \sigma(w^\top x_i)$. Entonces:
\begin{equation*}
p((t_i)_{i=1}^N|(x_i)_{i=1}^N,w) = \prod_{i=1}^{N}p(t_i|x_i,w)= \prod_{i=1}^{N} p(\mathcal{C}_1|x_i)^{t_i} p(\mathcal{C}_2|x_i)^{1-t_i} =  \prod_{i=1}^{N}\sigma_i^{t_i}(1-\sigma_i)^{1-t_i} 
\end{equation*}
\end{frame}
\begin{frame}{Regresión Logística v/s modelo generativo}

Con lo que la log-verosimilitud está dada por
\begin{equation*}
  l(w) = \sum_{i=1}^N t_i\log(\sigma_i) + (1-t_i)\log(1-\sigma_i).
\end{equation*} \pause 
Notemos que este  problema de optimización no exhibe una solución en forma cerrada, por lo que podemos resolverlo mediante gradiente, para lo cual es necesario calcular el gradiente de $l(w)$ respecto a $w$:
\begin{align*}
\nabla_w l(w) = \sum_{i=1}^N (t_i-\sigma_i)x_i,
\end{align*} \pause
lo cual nos da una regla de  ajuste $\theta \mapsto \theta - \eta \sum_{i=1}^N (\sigma_i-t_i)x_i$, o bien 
\begin{equation*}
  \theta \mapsto \theta + \eta(t_i-\sigma_i)x_i, \label{eq:reg_log_theta_update}
\end{equation*}
si tomamos los  datos de ``a uno'' (método del gradiente estocástico).


\end{frame}

\begin{frame}{Regresión Logística v/s modelo generativo}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{../img/cap3_logistica.pdf}\\
  \caption{En gris la frontera de decisión: una nueva entrada $x_\star$ será asignada a la clase $\cC_1$ si $p(\cC_1|x_\star)>\frac{1}{2}$, en caso contrario, será asignada a $\cC_2$. Se observa que al entrenar con más muestras, $\norm{w}$ crece por lo que el parámetro se sobreajusta a los datos y el clasificador converge a una función indicatriz.}
\end{figure}


\end{frame}

\section{Support Vector Machines}

\bibliography{../capitulos/referencias} %Bibliografía
\bibliographystyle{apacite}
\end{document}